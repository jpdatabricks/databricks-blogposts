{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd2e42b-db43-40f7-b703-8e96fd069227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup and Configuration for Demonstrating Real-Time Streaming Credit Card Fraud Feature Engineering Pipeline\n",
    "\n",
    "This notebook handles the initial setup and configuration for the Real-Time streaming feature engineering pipeline that publishes features to Databricks Lakebase PostgreSQL table.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 17.3+ (with Spark 4.0+ for transformWithStateInPandas)\n",
    "- Ensure the cluster is configured \n",
    "  - to support [Real-Time Streaming](https://docs.databricks.com/aws/en/structured-streaming/real-time#cluster-configuration)\n",
    "  - to have enough task slots/cores [Cluster size requirements](https://docs.databricks.com/aws/en/structured-streaming/real-time#cluster-size-requirements)\n",
    "- Databricks Python SDK 0.65.0 or above installed on the cluster\n",
    "- dbldatagen library installed on the cluster\n",
    "- Access to an existing Lakebase PostgreSQL instance\n",
    "\n",
    "## Setup Tasks\n",
    "1. **Import Required Libraries**: Import required library dependencies\n",
    "2. **Configuration**: Set up Lakebase PostgreSQL connection\n",
    "3. **Database Setup**: Create the unified `transaction_features` table\n",
    "4. **Validation**: Test connection and verify table creation\n",
    "\n",
    "## What Gets Created\n",
    "- **transaction_features table**: Stores both stateless and stateful fraud detection features\n",
    "\n",
    "## Post-Setup\n",
    "After running this notebook, proceed with:\n",
    "- 1. Run **01_generate_streaming_data** notebook to generate synthetic streaming credit card transaction data\n",
    "- 2. Run **02_streaming_fraud_detection_pipeline** notebook to generate streaming fraud detection features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9defeb21-688b-4b2f-8297-3fc27171edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8343e878-00e4-496d-bcc5-be6bfe029cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    }
   ],
   "source": [
    "#Validate if databricks-sdk > 0.65.0 is installed to support Lakebase SDK\n",
    "%pip show databricks-sdk | grep -oP '(?<=Version: )\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3005072-7b7f-4cb4-be98-73a658d856d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbldatagen version: 0.4.0post1\n"
     ]
    }
   ],
   "source": [
    "#Validate if dbldatagen is installed for kafka data generation\n",
    "import dbldatagen as dg \n",
    "print(\"dbldatagen version:\", dg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d9cde-4efb-46fb-bef1-50eae970123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6712667302074722>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Import Lakebase client\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlakebase_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LakebaseClient\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Config\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#initialize Config\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Workspace/Users/jay.palaniappan@databricks.com/databricks-blogposts/2025-10-realtime-streaming-feature-enginnering-with-lakebase/utils/lakebase_client.py'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "FileNotFoundError",
        "evalue": "[Errno 2] No such file or directory: '/Workspace/Users/jay.palaniappan@databricks.com/databricks-blogposts/2025-10-realtime-streaming-feature-enginnering-with-lakebase/utils/lakebase_client.py'"
       },
       "metadata": {
        "errorSummary": "FileNotFoundError: [Errno 2] No such file or directory: '/Workspace/Users/jay.palaniappan@databricks.com/databricks-blogposts/2025-10-realtime-streaming-feature-enginnering-with-lakebase/utils/lakebase_client.py'\n[Trace ID: 00-54551ffe4a2345f6947ff5f48e41fdb9-8e87328169d90ce5-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6712667302074722>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Import Lakebase client\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlakebase_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LakebaseClient\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Config\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#initialize Config\u001B[39;00m\n",
        "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Workspace/Users/jay.palaniappan@databricks.com/databricks-blogposts/2025-10-realtime-streaming-feature-enginnering-with-lakebase/utils/lakebase_client.py'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Lakebase client\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "from utils.config import Config\n",
    "\n",
    "#initialize Config\n",
    "config = Config()\n",
    "\n",
    "print(\"Connecting to Lakebase PostgreSQL...\\n\")\n",
    "\n",
    "# Initialize Lakebase client\n",
    "lakebase = LakebaseClient(**config.lakebase_config)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Lakebase connection...\")\n",
    "if lakebase.test_connection():\n",
    "    print(\"Successfully connected to Lakebase PostgreSQL!\")    \n",
    "else:\n",
    "    print(\"Failed to connect to Lakebase\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"  1. Lakebase instance is provisioned\")\n",
    "    print(\"  2. Instance name is correct\")\n",
    "    print(\"  3. Database name is correct\")\n",
    "    raise Exception(\"Lakebase connection failed\")\n",
    "\n",
    "# Create unified feature table\n",
    "print(\"\\nCreating unified feature table in Lakebase...\")\n",
    "print(\"  Table: transaction_features\")\n",
    "print(\"  Includes: stateless + stateful fraud detection features\")\n",
    "\n",
    "lakebase.create_feature_table(\"transaction_features\")\n",
    "\n",
    "print(\"Table created successfully!\")\n",
    "\n",
    "# Verify table exists\n",
    "print(\"\\nVerifying table...\")\n",
    "try:\n",
    "    stats_txn = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"  transaction_features: {stats_txn['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table exists but is empty (just created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKEBASE POSTGRESQL SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 01_generate_streaming_data notebook to generate synthetic streaming credit card transaction data\")\n",
    "print(\"  2. Run 02_streaming_fraud_detection_pipeline notebook to generate streaming fraud detection features\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
