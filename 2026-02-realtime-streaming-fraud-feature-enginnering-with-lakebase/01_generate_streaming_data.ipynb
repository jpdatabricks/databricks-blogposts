{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c318ab-0659-452b-a1c0-9a9701a441d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Synthetic Data\n",
    "Generate  synthetic streaming credit card transaction data for feature engineering and publish it to a kafka topic. \n",
    "\n",
    "### Prerequisites\n",
    "- Kafka topic to write and read streaming data\n",
    "  - Configure Kafka topic information in **utils/config.py** prior to running this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f55e757-87f6-49ee-a744-90925d831089",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Spark Configs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "dbutils.library.restartPython()\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\")\n",
    "spark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5925a17c-4c62-4400-a114-393d04803780",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dbldatagen._version:Version : VersionInfo(major='0', minor='4', patch='0', release='post', build='1')\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import logging\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Import utility modules\n",
    "from utils.data_generator import TransactionDataGenerator\n",
    "from utils.config import Config\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af261522-f8a3-446d-95c3-8ab010e4074c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Generate Streaming Transaction Data\n",
    "\n",
    "Create a streaming source that continuously generates synthetic transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a42abb-4b98-4730-b5c9-e8388343372a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data_generator:Creating synthetic transaction source with dbldatagen...\nINFO:utils.data_generator:   Rows: 1000\nINFO:utils.data_generator:   Users: 10000, Merchants: 1000\nINFO:utils.data_generator:Synthetic source created successfully\n"
     ]
    }
   ],
   "source": [
    "#Get Config\n",
    "config = Config()\n",
    "\n",
    "# Generate streaming transaction data\n",
    "data_gen = TransactionDataGenerator(spark)\n",
    "df_transactions = data_gen.generate_transaction_data(\n",
    "    num_users=config.data_gen_config[\"num_users\"],              # unique users\n",
    "    num_merchants=config.data_gen_config[\"num_merchants\"],      # unique merchants\n",
    "    rows_per_second=config.data_gen_config[\"rows_per_second\"]     # transactions per second\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab4ce8b-5aaf-4c9d-acb3-43417d972274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Write Stream Data to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5bb6a0-b78f-488d-bc54-f1febbbeb77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[STREAMING] Kafka producer started successfully\n  Query ID: fb7aaf64-7725-478d-9f7f-22be9dc24167\n  Query Name: None\n  Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n  Kafka Topic: fraud_feature_eng_example\n  Checkpoint: /Volumes/main/fraud_feature_eng_demo/default/checkpoints//transaction-data-generator-checkpoint\n  Serialization: Protobuf (Confluent Schema Registry)\n\n"
     ]
    }
   ],
   "source": [
    "#Get Kafka Config\n",
    "kafka_credentials_secrets = config.kafka_config[\"kafka_credentials_secrets\"]\n",
    "scope = kafka_credentials_secrets[\"scope\"]\n",
    "\n",
    "# Retrieve secrets from Databricks secret scope. \n",
    "# Note: Daatabricks secrets should be stored prior to using them and it's not covered in this example\n",
    "# You can find more information about Databricks secrets here: https://docs.databricks.com/aws/en/security/secrets/#secrets-overview \n",
    "KAFKA_USERNAME = dbutils.secrets.get(scope = scope, key = kafka_credentials_secrets[\"username\"])\n",
    "KAFKA_SECRET = dbutils.secrets.get(scope = scope, key = kafka_credentials_secrets[\"secret\"])\n",
    "KAFKA_SERVER = dbutils.secrets.get(scope = scope, key = kafka_credentials_secrets[\"server\"])\n",
    "KAFKA_TOPIC = config.kafka_config[\"kafka_topic\"]\n",
    "\n",
    "#Data Generator Configuration\n",
    "CHECKPOINT_BASE_PATH = config.kafka_config[\"checkpoint_base_path\"]\n",
    "CHECKPOINT_LOCATION = f\"{CHECKPOINT_BASE_PATH}/transaction-data-generator-checkpoint\"\n",
    "#Since we generate data using synthentic dbldatagen tool, we need to delete old checkpoint directory everytime we run the notebook to avoid any conflicts\n",
    "dbutils.fs.rm(CHECKPOINT_LOCATION, True) \n",
    "\n",
    "#Convert row to JSON String\n",
    "json_df = df_transactions.select(to_json(struct(*[col(c) for c in df_transactions.columns])).alias(\"value\"), col(\"transaction_id\").alias(\"key\"))\n",
    "\n",
    "kafkaWriter = (\n",
    "    json_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{KAFKA_USERNAME}' password='{KAFKA_SECRET}';\")\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"topic\", KAFKA_TOPIC)\n",
    "    .option(\"failOnDataLoss\", \"true\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_LOCATION)\n",
    ")\n",
    "\n",
    "#Start Kafka Producer\n",
    "kafkaQuery = kafkaWriter.start()\n",
    "\n",
    "print(f\"\"\"\n",
    "[STREAMING] Kafka producer started successfully\n",
    "  Query ID: {kafkaQuery.id}\n",
    "  Query Name: {kafkaQuery.name}\n",
    "  Status: {kafkaQuery.status}\n",
    "  Kafka Topic: {KAFKA_TOPIC}\n",
    "  Checkpoint: {CHECKPOINT_LOCATION}\n",
    "  Serialization: Protobuf (Confluent Schema Registry)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551b6d01-d800-43b8-9608-7f52a3d63c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop Kafka Writer\n",
    "# if kafkaQuery.isActive:\n",
    "#     kafkaQuery.stop()\n",
    "#     logger.info(\"Streaming query stopped\")\n",
    "\n",
    "# logger.info(\"\\nPipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_generate_streaming_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
