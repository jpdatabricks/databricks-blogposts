{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de73176-34e7-4145-8bf6-fc373b28fa8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Feature Engineering for Fraud Detection\n",
    "\n",
    "This notebook demonstrates an end-to-end [Real-Time streaming](https://www.databricks.com/blog/introducing-real-time-mode-apache-sparktm-structured-streaming) feature engineering pipeline that demontrates stateless and stateful feature engineering of credit card transaction data and publishing the features to Lakebase PostgresSQL table with subsecond latency.\n",
    "\n",
    "## Features\n",
    "\n",
    "**Stateless Features:**\n",
    "- Time-based: hour, day, business hours, cyclical encodings\n",
    "- Amount-based: log, sqrt, categories, z-scores\n",
    "- Merchant: risk scores based on category\n",
    "- Location: risk indicators, region classification\n",
    "- Device: device type detection\n",
    "- Network: IP classification\n",
    "\n",
    "**Stateful Features:**\n",
    "- Velocity: transaction counts in time windows (10 min, 1 hour)\n",
    "- IP tracking: IP change detection and counts\n",
    "- Location anomalies: impossible travel detection (velocity > 800 km/h)\n",
    "- Amount anomalies: z-score calculation vs user history\n",
    "- Fraud scoring: composite 0-100 score with prediction flag\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Streaming Source (rate)\n",
    "    ↓\n",
    "Generate Transactions (synthetic data)\n",
    "    ↓\n",
    "Apply Stateless Features (AdvancedFeatureEngineering)\n",
    "    ↓\n",
    "Apply Stateful Fraud Detection (transformWithState)\n",
    "    ↓\n",
    "Write to Lakebase PostgreSQL in Realtime Mode (foreach)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Lakebase PostgreSQL instance provisioned\n",
    "    - Configure Lakebase information in utils/config.py  \n",
    "- Ensure the Databricks Cluster is configured \n",
    "  - with Databricks Runtime 17.3+\n",
    "  - to support [Realtime Streaming](https://docs.databricks.com/aws/en/structured-streaming/real-time#cluster-configuration)\n",
    "  - to have enough task slots/cores [Cluster size requirements](https://docs.databricks.com/aws/en/structured-streaming/real-time#cluster-size-requirements) \n",
    "- Kafka topic to write and read streaming data \n",
    "  - Configure Kafka topic information in utils/config.py  \n",
    "- Run `00_setup.ipynb` to create `transaction_features` table\n",
    "- Run `01_generate_streaming_data.ipynb` to generate and write transaction data to a kafka stream \n",
    "\n",
    "## Output\n",
    "\n",
    "All features (stateless + stateful) are written to:\n",
    "- **Table**: `transaction_features`\n",
    "- **Write latency**: 400ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f55e757-87f6-49ee-a744-90925d831089",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Spark Configs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#Retsart Python env to clear config and code caches. \n",
    "dbutils.library.restartPython()\n",
    "\n",
    "#Setup Spark streaming configs \n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\")\n",
    "spark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "\n",
    "#Adjust Shuffle partitions based on your need. \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5925a17c-4c62-4400-a114-393d04803780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dbldatagen._version:Version : VersionInfo(major='0', minor='4', patch='0', release='post', build='1')\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Import utility modules\n",
    "from utils.config import Config\n",
    "from utils.data_generator import TransactionDataGenerator\n",
    "from utils.feature_engineering import (\n",
    "    AdvancedFeatureEngineering, \n",
    "    FraudDetectionFeaturesProcessor,\n",
    "    get_fraud_detection_output_schema\n",
    ")\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea342fd-087f-4418-91dc-55751a4da71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Configure Lakebase connection and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fabf4d6-fea9-435c-b92c-2bce2217bae5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup and Initialize Lakebase Config"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Lakebase connection test successful\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Lakebase PostgreSQL\n\nVerifying transaction_features table...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Table stats: 1,400,707 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Table exists with 1,400,707 rows\n"
     ]
    }
   ],
   "source": [
    "#Initialize Config\n",
    "config = Config()\n",
    "\n",
    "# Initialize components\n",
    "data_gen = TransactionDataGenerator(spark)\n",
    "feature_engineer = AdvancedFeatureEngineering(spark)\n",
    "lakebase = LakebaseClient(**config.lakebase_config)\n",
    "\n",
    "# Test Lakebase connection\n",
    "if lakebase.test_connection():\n",
    "    print(\"Connected to Lakebase PostgreSQL\")\n",
    "else:\n",
    "    raise Exception(\"Failed to connect to Lakebase\")\n",
    "\n",
    "# Verify transaction_features table exists\n",
    "print(\"\\nVerifying transaction_features table...\")\n",
    "try:\n",
    "    stats = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"  Table exists with {stats['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table not found. Please run 00_setup.ipynb first!\")\n",
    "    raise Exception(\"transaction_features table does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859d6274-372c-42f0-8aaa-6d14c9ae1cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Read Data from Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5514be69-e7cc-4ce2-8d4f-e78b74593273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve Kafka credentials from Databricks secrets\n",
    "kafka_credentials_secrets = config.kafka_config[\"kafka_credentials_secrets\"]\n",
    "scope = kafka_credentials_secrets[\"scope\"]\n",
    "\n",
    "# Retrieve secrets from Databricks secret scope. \n",
    "# Note: Daatabricks secrets should be stored prior to using them and it's not covered in this example\n",
    "# You can find more information about Databricks secrets here: https://docs.databricks.com/aws/en/security/secrets/#secrets-overview \n",
    "KAFKA_USERNAME = dbutils.secrets.get(scope = scope, key = kafka_credentials_secrets[\"username\"])\n",
    "KAFKA_SECRET = dbutils.secrets.get(scope = scope, key = kafka_credentials_secrets[\"secret\"])\n",
    "KAFKA_SERVER = dbutils.secrets.get(scope = scope, key = kafka_credentials_secrets[\"server\"])\n",
    "KAFKA_TOPIC = config.kafka_config[\"kafka_topic\"]\n",
    "\n",
    "# Define the schema for the Kafka value column\n",
    "value_schema = \"STRUCT<transaction_id: STRING, user_id: STRING, merchant_id: STRING, amount: DOUBLE, currency: STRING, merchant_category: STRING, payment_method: STRING, ip_address: STRING, device_id: STRING, latitude: DOUBLE, longitude: DOUBLE, card_type: STRING, timestamp: TIMESTAMP>\"\n",
    "\n",
    "# Read streaming data from Kafka topic with SASL_SSL authentication\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{KAFKA_USERNAME}' password='{KAFKA_SECRET}';\")\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")  \n",
    "    .load()\n",
    ") \\\n",
    ".select(\n",
    "    from_json(col(\"value\").cast(\"string\"), \n",
    "              value_schema).alias(\"value\"), \n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\")) \\\n",
    ".selectExpr(\"value.*\", \"kafka_timestamp\") \\\n",
    "  .drop(\"timestamp\") \\\n",
    "  .withColumnRenamed(\"kafka_timestamp\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228c0478-8d9e-494c-9f06-2d59f20ce5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Apply Stateless Features\n",
    "\n",
    "Apply time-based, amount-based, merchant, location, device, and network features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f35d8c-696a-458a-9a82-17c350209007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.feature_engineering:Applying streaming-compatible feature engineering...\nINFO:utils.feature_engineering:Creating time-based features...\nINFO:utils.feature_engineering:Creating amount-based features...\nINFO:utils.feature_engineering:Creating merchant features (streaming-only)...\nINFO:utils.feature_engineering:Skipping stateless location features (optimized out)\nINFO:utils.feature_engineering:Skipping device features (optimized out)\nINFO:utils.feature_engineering:Creating network features (streaming-only)...\nINFO:utils.feature_engineering:Streaming feature engineering completed!\n"
     ]
    }
   ],
   "source": [
    "df_with_stateless_features = feature_engineer.apply_all_features(kafka_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccb1c16-224a-4efc-be22-d98df03be063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Apply Stateful Features\n",
    "\n",
    "Use `transformWithStateInPandas` to maintain per-user state and detect fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f0fef-d09f-4c9a-8222-d45cd47ef79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply stateful fraud detection using transformWithState\n",
    "df_with_fraud_features = df_with_stateless_features \\\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .transformWithState(\n",
    "        statefulProcessor=FraudDetectionFeaturesProcessor(),\n",
    "        outputStructType=get_fraud_detection_output_schema(),\n",
    "        outputMode=\"Update\",\n",
    "        timeMode=\"processingTime\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4005cb7d-5a33-4066-9ed6-664bac2f7787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Write to Lakebase\n",
    "\n",
    "Stream all features to Lakebase for real-time serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33378362-5a0c-4c15-a106-e571c56c0d8f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clear Checkpoint Location"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_BASE_PATH = config.kafka_config[\"checkpoint_base_path\"]\n",
    "FRAUD_PIPELINE_CHECKPOINT_LOCATION = f\"{CHECKPOINT_BASE_PATH}/fraud-pipeline-checkpoint\"\n",
    "dbutils.fs.rm(FRAUD_PIPELINE_CHECKPOINT_LOCATION, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149557d7-1590-4592-82e3-0f9bcb628195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    }
   ],
   "source": [
    "#Get Schema from dataframe\n",
    "table_schema = df_with_fraud_features.schema\n",
    "\n",
    "#Initialize lakebase writer\n",
    "lakebase_writer  = lakebase.get_foreach_writer(column_names=table_schema.names, batch_size=5)\n",
    "\n",
    "#Start streaming query\n",
    "query = df_with_fraud_features \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreach(lakebase_writer) \\\n",
    "    .option(\"checkpointLocation\", FRAUD_PIPELINE_CHECKPOINT_LOCATION) \\\n",
    "    .trigger(realTime=\"5 minutes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4007f7d-660c-417c-8864-334a2d16680d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Stop Streaming Query\n",
    "\n",
    "Stop the streaming pipeline when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551b6d01-d800-43b8-9608-7f52a3d63c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Stop streaming query\n",
    "# if query.isActive:\n",
    "#     query.stop()\n",
    "#     print(\"Streaming query stopped\")\n",
    "\n",
    "# print(\"\\nPipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_streaming_fraud_detection_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
