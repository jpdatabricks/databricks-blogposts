{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8cd2e42b-db43-40f7-b703-8e96fd069227",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Setup and Configuration for Streaming Feature Engineering Pipeline\n",
        "\n",
        "This notebook handles the initial setup and configuration for the streaming feature engineering pipeline with Lakebase.  \n",
        "\n",
        "## Prerequisites\n",
        "- Databricks Runtime 17.3+ \n",
        "- Install Databricks Python SDK 0.65.0 or above on the cluster to support lakebase APIs\n",
        "- Access to an existing Lakebase Postgres Database\n",
        "\n",
        "\n",
        "## Setup Tasks\n",
        "2. **Configuration**: Set up LakeBase connection\n",
        "3. **Database Setup**: Create the feature table to store the features\n",
        "4. **Validation**: Test all components and connections\n",
        "\n",
        "## Post-Setup\n",
        "After running this notebook, you can proceed with:\n",
        "- `01_streaming_features.ipynb` - Streaming feature engineering pipeline to walkthrough real-time feature engineering and publish the features to Lakebase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9defeb21-688b-4b2f-8297-3fc27171edd7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "# Note: 'spark' session is already available in Databricks\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import DeltaTable\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f96d9cde-4efb-46fb-bef1-50eae970123b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import Lakebase client\n",
        "from utils.lakebase_client import LakebaseClient\n",
        "\n",
        "# Get OAuth token for authentication\n",
        "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
        "\n",
        "# OR use secrets (recommended for production)\n",
        "# token = dbutils.secrets.get(scope=\"lakebase\", key=\"token\")\n",
        "# host = dbutils.secrets.get(scope=\"lakebase\", key=\"host\")\n",
        "\n",
        "# Lakebase connection configuration\n",
        "LAKEBASE_CONFIG = {\n",
        "    \"instance_name\": \"neha-lakebase-demo\",\n",
        "    \"database\": \"databricks_postgres\"\n",
        "}\n",
        "\n",
        "print(\" Connecting to Lakebase PostgreSQL...\\n\")\n",
        "\n",
        "# Initialize Lakebase client\n",
        "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
        "\n",
        "# Test connection\n",
        "print(\" Testing Lakebase connection...\")\n",
        "if lakebase.test_connection():\n",
        "    print(\" Successfully connected to Lakebase PostgreSQL!\")    \n",
        "else:\n",
        "    print(\" Failed to connect to Lakebase\")\n",
        "    print(\"   Please check:\")\n",
        "    print(\"   1. Lakebase instance is provisioned\")\n",
        "    print(\"   2. Host is correct\")\n",
        "    print(\"   3. OAuth token is valid\")\n",
        "    raise Exception(\"Lakebase connection failed\")\n",
        "\n",
        "# Create feature table\n",
        "print(\"\\n Creating unified feature tables in Lakebase...\")\n",
        "print(\"   \u2022 transaction_features (for stateless features)\")\n",
        "print(\"   \u2022 fraud_features (for stateful fraud detection)\")\n",
        "lakebase.create_feature_table(\"transaction_features\")\n",
        "lakebase.create_feature_table(\"fraud_features\")\n",
        "print(\" Tables created successfully!\")\n",
        "\n",
        "# Verify table exists\n",
        "try:\n",
        "    print(f\"   Total rows: {stats['total_rows']:,}\")\n",
        "    print(f\"   Unique users: {stats['unique_users']:,}\")\n",
        "    print(f\"   Unique merchants: {stats['unique_merchants']:,}\")\n",
        "except Exception as e:\n",
        "    print(\"   Table exists but is empty (just created)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" LAKEBASE POSTGRESQL SETUP COMPLETE\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "00_setup",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}