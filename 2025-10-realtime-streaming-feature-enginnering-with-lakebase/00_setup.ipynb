{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd2e42b-db43-40f7-b703-8e96fd069227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup and Configuration for Streaming Feature Engineering Pipeline\n",
    "\n",
    "This notebook handles the initial setup and configuration for the streaming feature engineering pipeline with Lakebase.  \n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 17.3+ \n",
    "- Install Databricks Python SDK 0.65.0 or above on the cluster to support lakebase APIs\n",
    "- Access to an existing Lakebase Postgres Database\n",
    "\n",
    "\n",
    "## Setup Tasks\n",
    "2. **Configuration**: Set up LakeBase connection\n",
    "3. **Database Setup**: Create the feature table to store the features\n",
    "4. **Validation**: Test all components and connections\n",
    "\n",
    "## Post-Setup\n",
    "After running this notebook, you can proceed with:\n",
    "- `01_streaming_features.ipynb` - Streaming feature engineering pipeline to walkthrough real-time feature engineering and publish the features to Lakebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9defeb21-688b-4b2f-8297-3fc27171edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# Note: 'spark' session is already available in Databricks\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d9cde-4efb-46fb-bef1-50eae970123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Lakebase PostgreSQL...\n\nTesting Lakebase connection...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Lakebase connection test successful\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Lakebase PostgreSQL!\n\nCreating unified feature tables in Lakebase...\n  • transaction_features (for stateless features)\n  • fraud_features (for stateful fraud detection)\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Created unified feature table: transaction_features (~70+ columns)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Created unified feature table: fraud_features (~70+ columns)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully!\n\nVerifying tables...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Table stats: 0 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Table stats: 0 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_features: 0 rows\n  fraud_features: 0 rows\n\n============================================================\nLAKEBASE POSTGRESQL SETUP COMPLETE\n============================================================\n\nNext steps:\n  Run streaming_fraud_detection_pipeline.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import Lakebase client\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Get OAuth token for authentication\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# OR use secrets (recommended for production)\n",
    "# token = dbutils.secrets.get(scope=\"lakebase\", key=\"token\")\n",
    "# host = dbutils.secrets.get(scope=\"lakebase\", key=\"host\")\n",
    "\n",
    "# Lakebase connection configuration\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"rtm-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\"\n",
    "}\n",
    "\n",
    "print(\"Connecting to Lakebase PostgreSQL...\\n\")\n",
    "\n",
    "# Initialize Lakebase client\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Lakebase connection...\")\n",
    "if lakebase.test_connection():\n",
    "    print(\"Successfully connected to Lakebase PostgreSQL!\")    \n",
    "else:\n",
    "    print(\"Failed to connect to Lakebase\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"  1. Lakebase instance is provisioned\")\n",
    "    print(\"  2. Host is correct\")\n",
    "    print(\"  3. OAuth token is valid\")\n",
    "    raise Exception(\"Lakebase connection failed\")\n",
    "\n",
    "# Create feature tables\n",
    "print(\"\\nCreating unified feature tables in Lakebase...\")\n",
    "print(\"  • transaction_features (for stateless features)\")\n",
    "print(\"  • fraud_features (for stateful fraud detection)\")\n",
    "\n",
    "lakebase.create_feature_table(\"transaction_features\")\n",
    "lakebase.create_feature_table(\"fraud_features\")\n",
    "\n",
    "print(\"Tables created successfully!\")\n",
    "\n",
    "# Verify tables exist\n",
    "print(\"\\nVerifying tables...\")\n",
    "try:\n",
    "    stats_txn = lakebase.get_table_stats(\"transaction_features\")\n",
    "    stats_fraud = lakebase.get_table_stats(\"fraud_features\")\n",
    "    print(f\"  transaction_features: {stats_txn['total_rows']:,} rows\")\n",
    "    print(f\"  fraud_features: {stats_fraud['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Tables exist but are empty (just created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKEBASE POSTGRESQL SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  Run streaming_fraud_detection_pipeline.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
