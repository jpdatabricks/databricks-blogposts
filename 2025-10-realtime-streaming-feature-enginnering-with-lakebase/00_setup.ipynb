{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd2e42b-db43-40f7-b703-8e96fd069227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup and Configuration for Streaming Feature Engineering Pipeline\n",
    "\n",
    "This notebook handles the initial setup and configuration for the streaming feature engineering pipeline with Databricks Lakebase PostgreSQL.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 17.3+ (with Spark 4.0+ for transformWithStateInPandas)\n",
    "- Databricks Python SDK 0.65.0 or above installed on the cluster\n",
    "- Access to an existing Lakebase PostgreSQL instance\n",
    "\n",
    "## Setup Tasks\n",
    "1. **Import Libraries**: Import required dependencies\n",
    "2. **Configuration**: Set up Lakebase PostgreSQL connection\n",
    "3. **Database Setup**: Create the unified `transaction_features` table (~70+ columns)\n",
    "4. **Validation**: Test connection and verify table creation\n",
    "\n",
    "## What Gets Created\n",
    "- **transaction_features table**: Stores both stateless and stateful fraud detection features\n",
    "\n",
    "## Post-Setup\n",
    "After running this notebook, proceed with:\n",
    "- `01_streaming_fraud_detection_pipeline.ipynb` - End-to-end streaming fraud detection pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9defeb21-688b-4b2f-8297-3fc27171edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d9cde-4efb-46fb-bef1-50eae970123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Lakebase PostgreSQL...\n",
      "\n",
      "Testing Lakebase connection...\n",
      "0.68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:utils.lakebase_client:Lakebase connection test successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Lakebase PostgreSQL!\n",
      "\n",
      "Creating unified feature tables in Lakebase...\n",
      "  â€¢ transaction_features (for features)\n",
      "0.68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:utils.lakebase_client:Created unified feature table: transaction_features (~70+ columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully!\n",
      "\n",
      "Verifying tables...\n",
      "0.68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:utils.lakebase_client:Table stats: 0 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_features: 0 rows\n",
      "\n",
      "============================================================\n",
      "LAKEBASE POSTGRESQL SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "  Run 01_streaming_fraud_detection_pipeline.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import Lakebase client\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Get OAuth token for authentication (Databricks handles this automatically)\n",
    "# Note: In production, use secrets for credential management\n",
    "# token = dbutils.secrets.get(scope=\"lakebase\", key=\"token\")\n",
    "\n",
    "# Lakebase connection configuration\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"rtm-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\"\n",
    "}\n",
    "\n",
    "print(\"Connecting to Lakebase PostgreSQL...\\n\")\n",
    "\n",
    "# Initialize Lakebase client\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Lakebase connection...\")\n",
    "if lakebase.test_connection():\n",
    "    print(\"Successfully connected to Lakebase PostgreSQL!\")    \n",
    "else:\n",
    "    print(\"Failed to connect to Lakebase\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"  1. Lakebase instance is provisioned\")\n",
    "    print(\"  2. Instance name is correct\")\n",
    "    print(\"  3. Database name is correct\")\n",
    "    raise Exception(\"Lakebase connection failed\")\n",
    "\n",
    "# Create unified feature table\n",
    "print(\"\\nCreating unified feature table in Lakebase...\")\n",
    "print(\"  Table: transaction_features (~70+ columns)\")\n",
    "print(\"  Includes: stateless + stateful fraud detection features\")\n",
    "\n",
    "lakebase.create_feature_table(\"transaction_features\")\n",
    "\n",
    "print(\"Table created successfully!\")\n",
    "\n",
    "# Verify table exists\n",
    "print(\"\\nVerifying table...\")\n",
    "try:\n",
    "    stats_txn = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"  transaction_features: {stats_txn['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table exists but is empty (just created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKEBASE POSTGRESQL SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 01_streaming_fraud_detection_pipeline.ipynb\")\n",
    "print(\"  2. Features will be written to: transaction_features table\")\n",
    "print(\"  3. Query latency: <10ms for real-time serving\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
