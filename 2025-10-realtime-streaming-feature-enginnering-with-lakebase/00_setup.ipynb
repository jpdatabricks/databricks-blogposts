{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd2e42b-db43-40f7-b703-8e96fd069227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup and Configuration for Streaming Feature Engineering Pipeline\n",
    "\n",
    "This notebook handles the initial setup and configuration for the streaming feature engineering pipeline with Databricks Lakebase PostgreSQL.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 17.3+ (with Spark 4.0+ for transformWithStateInPandas)\n",
    "- Ensure the cluster is configured \n",
    "  - to support [Realtime Streaming](https://docs.databricks.com/aws/en/structured-streaming/real-time#cluster-configuration)\n",
    "  - to have enough task slots/cores [Cluster size requirements](https://docs.databricks.com/aws/en/structured-streaming/real-time#cluster-size-requirements)\n",
    "- Databricks Python SDK 0.65.0 or above installed on the cluster\n",
    "- dbldatagen library installed on the cluster\n",
    "- Access to an existing Lakebase PostgreSQL instance\n",
    "\n",
    "## Setup Tasks\n",
    "1. **Import Required Libraries**: Import required library dependencies\n",
    "2. **Configuration**: Set up Lakebase PostgreSQL connection\n",
    "3. **Database Setup**: Create the unified `transaction_features` table\n",
    "4. **Validation**: Test connection and verify table creation\n",
    "\n",
    "## What Gets Created\n",
    "- **transaction_features table**: Stores both stateless and stateful fraud detection features\n",
    "\n",
    "## Post-Setup\n",
    "After running this notebook, proceed with:\n",
    "- `01_streaming_fraud_detection_pipeline.ipynb` - End-to-end streaming fraud detection pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9defeb21-688b-4b2f-8297-3fc27171edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8343e878-00e4-496d-bcc5-be6bfe029cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    }
   ],
   "source": [
    "#Validate if databricks-sdk > 0.65.0 is installed to support Lakebase SDK\n",
    "%pip show databricks-sdk | grep -oP '(?<=Version: )\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3005072-7b7f-4cb4-be98-73a658d856d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dbldatagen._version:Version : VersionInfo(major='0', minor='4', patch='0', release='post', build='1')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbldatagen version: 0.4.0post1\n"
     ]
    }
   ],
   "source": [
    "#Validate if dbldatagen is installed for kafka data generation\n",
    "import dbldatagen as dg \n",
    "print(\"dbldatagen version:\", dg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d9cde-4efb-46fb-bef1-50eae970123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Lakebase PostgreSQL...\n\nTesting Lakebase connection...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Lakebase connection test successful\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Lakebase PostgreSQL!\n\nCreating unified feature table in Lakebase...\n  Table: transaction_features\n  Includes: stateless + stateful fraud detection features\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Created optimized feature table: transaction_features\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n\nVerifying table...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Table stats: 1,400,707 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_features: 1,400,707 rows\n\n============================================================\nLAKEBASE POSTGRESQL SETUP COMPLETE\n============================================================\n\nNext steps:\n  1. Run 01_generate_streaming_data notebook to generate synthetic streaming credit card transaction data\n  2. Run 02_streaming_fraud_detection_pipeline notebook to generate streaming fraud detection features\n"
     ]
    }
   ],
   "source": [
    "# Import Lakebase client\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "from utils.config import Config\n",
    "\n",
    "#initialize Config\n",
    "config = Config()\n",
    "\n",
    "print(\"Connecting to Lakebase PostgreSQL...\\n\")\n",
    "\n",
    "# Initialize Lakebase client\n",
    "lakebase = LakebaseClient(**config.lakebase_config)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Lakebase connection...\")\n",
    "if lakebase.test_connection():\n",
    "    print(\"Successfully connected to Lakebase PostgreSQL!\")    \n",
    "else:\n",
    "    print(\"Failed to connect to Lakebase\")\n",
    "    print(\"  Please check:\")\n",
    "    print(\"  1. Lakebase instance is provisioned\")\n",
    "    print(\"  2. Instance name is correct\")\n",
    "    print(\"  3. Database name is correct\")\n",
    "    raise Exception(\"Lakebase connection failed\")\n",
    "\n",
    "# Create unified feature table\n",
    "print(\"\\nCreating unified feature table in Lakebase...\")\n",
    "print(\"  Table: transaction_features\")\n",
    "print(\"  Includes: stateless + stateful fraud detection features\")\n",
    "\n",
    "lakebase.create_feature_table(\"transaction_features\")\n",
    "\n",
    "print(\"Table created successfully!\")\n",
    "\n",
    "# Verify table exists\n",
    "print(\"\\nVerifying table...\")\n",
    "try:\n",
    "    stats_txn = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"  transaction_features: {stats_txn['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table exists but is empty (just created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAKEBASE POSTGRESQL SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Run 01_generate_streaming_data notebook to generate synthetic streaming credit card transaction data\")\n",
    "print(\"  2. Run 02_streaming_fraud_detection_pipeline notebook to generate streaming fraud detection features\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
