{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd2e42b-db43-40f7-b703-8e96fd069227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup and Configuration for Streaming Feature Engineering Pipeline\n",
    "\n",
    "This notebook handles the initial setup and configuration for the streaming feature engineering pipeline with Lakebase. It installs required packages, configures the environment, and validates the setup.\n",
    "\n",
    "## Setup Tasks\n",
    "1. **Package Installation**: Install required Python packages\n",
    "2. **Environment Configuration**: Set up Spark configurations for streaming and Delta Lake\n",
    "3. **Database Setup**: Create necessary databases and feature tables\n",
    "4. **Validation**: Test all components and connections\n",
    "5. **Sample Data**: Create initial sample datasets\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Runtime 17.3+ \n",
    "- Install Databricks Python SDK 0.65.0 or above on the cluster to support lakebase APIs\n",
    "- Access to Lakebase Postgres Database\n",
    "\n",
    "## Post-Setup\n",
    "After running this notebook, you can proceed with:\n",
    "- `01_fraud_detection_streaming.ipynb` - Streaming feature engineering pipeline to walkthrough real-time feature engineering and publish the features to Lakebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9defeb21-688b-4b2f-8297-3fc27171edd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# Note: 'spark' session is already available in Databricks\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96d9cde-4efb-46fb-bef1-50eae970123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import Lakebase client\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Get OAuth token for authentication\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# OR use secrets (recommended for production)\n",
    "# token = dbutils.secrets.get(scope=\"lakebase\", key=\"token\")\n",
    "# host = dbutils.secrets.get(scope=\"lakebase\", key=\"host\")\n",
    "\n",
    "# Lakebase connection configuration\n",
    "# TODO: Update with your actual Lakebase host\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"neha-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\",    \n",
    "}\n",
    "\n",
    "print(\" Connecting to Lakebase PostgreSQL...\\n\")\n",
    "\n",
    "# Initialize Lakebase client\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Test connection\n",
    "print(\" Testing Lakebase connection...\")\n",
    "if lakebase.test_connection():\n",
    "    print(\" Successfully connected to Lakebase PostgreSQL!\")    \n",
    "else:\n",
    "    print(\" Failed to connect to Lakebase\")\n",
    "    print(\"   Please check:\")\n",
    "    print(\"   1. Lakebase instance is provisioned\")\n",
    "    print(\"   2. Host is correct\")\n",
    "    print(\"   3. OAuth token is valid\")\n",
    "    raise Exception(\"Lakebase connection failed\")\n",
    "\n",
    "# Create feature table\n",
    "print(\"\\n Creating transaction_features table in Lakebase...\")\n",
    "lakebase.create_feature_table(\"transaction_features\")\n",
    "print(\" Table created successfully!\")\n",
    "\n",
    "# Verify table exists\n",
    "print(\"\\n Verifying table...\")\n",
    "try:\n",
    "    stats = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"   Total rows: {stats['total_rows']:,}\")\n",
    "    print(f\"   Unique users: {stats['unique_users']:,}\")\n",
    "    print(f\"   Unique merchants: {stats['unique_merchants']:,}\")\n",
    "except Exception as e:\n",
    "    print(\"   Table exists but is empty (just created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" LAKEBASE POSTGRESQL SETUP COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}