{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Configuration for Streaming Feature Engineering Pipeline\n",
        "\n",
        "This notebook handles the initial setup and configuration for the streaming feature engineering pipeline with Lakebase. It installs required packages, configures the environment, and validates the setup.\n",
        "\n",
        "## Setup Tasks\n",
        "1. **Package Installation**: Install required Python packages\n",
        "2. **Environment Configuration**: Set up Spark configurations for streaming and Delta Lake\n",
        "3. **Database Setup**: Create necessary databases and feature tables\n",
        "4. **Validation**: Test all components and connections\n",
        "5. **Sample Data**: Create initial sample datasets\n",
        "\n",
        "## Prerequisites\n",
        "- Databricks Runtime 13.0+ with ML\n",
        "- Access to Delta Lake / Lakebase storage\n",
        "- Optional: Kafka/Event Hub for streaming sources\n",
        "\n",
        "## Post-Setup\n",
        "After running this notebook, you can proceed with:\n",
        "- `01_fraud_detection_streaming.ipynb` - Streaming feature engineering pipeline\n",
        "- `02_feature_engineering.ipynb` - Feature engineering examples\n",
        "- `04_data_generator.ipynb` - Data generation for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAGIC %md\n",
        "# MAGIC ## Package Installation\n",
        "# MAGIC\n",
        "# MAGIC **Pre-installed in Databricks Runtime 13.0+:**\n",
        "# MAGIC - \u2705 Apache Spark\n",
        "# MAGIC - \u2705 Delta Lake\n",
        "# MAGIC - \u2705 MLflow\n",
        "# MAGIC - \u2705 Pandas, NumPy, Scikit-learn\n",
        "# MAGIC - \u2705 Matplotlib, Seaborn, Plotly\n",
        "# MAGIC\n",
        "# MAGIC **Install only if needed:**\n",
        "\n",
        "# Install optional packages for your data source\n",
        "# Uncomment the line for your data source:\n",
        "\n",
        "# For Kafka:\n",
        "# %pip install kafka-python\n",
        "\n",
        "# For Azure Event Hub:\n",
        "# %pip install azure-eventhub\n",
        "\n",
        "# For configuration management:\n",
        "# %pip install pyyaml\n",
        "\n",
        "# Restart Python to ensure packages are loaded (only if you installed something)\n",
        "# dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAGIC %md\n",
        "# MAGIC ## Environment Configuration\n",
        "\n",
        "# Import required libraries\n",
        "# Note: 'spark' session is already available in Databricks\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import DeltaTable\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure Spark for optimal streaming performance\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.streaming.stateStore.maintenanceInterval\", \"600s\")\n",
        "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"\ud83d\udd27 Environment Configuration:\")\n",
        "print(f\"  Spark Version: {spark.version}\")\n",
        "print(f\"  Databricks Runtime: {spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion', 'Unknown')}\")\n",
        "print(f\"  Delta Lake Enabled: {spark.conf.get('spark.databricks.delta.optimizeWrite.enabled')}\")\n",
        "print(\"  \u2705 Streaming optimizations applied\")\n",
        "print(\"  \u2705 Delta Lake optimizations enabled\")\n",
        "print(\"  \u2705 Stateful streaming configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAGIC %md\n",
        "# MAGIC ## Lakebase PostgreSQL Setup\n",
        "\n",
        "# Install psycopg2 for PostgreSQL connection\n",
        "%pip install psycopg2-binary\n",
        "\n",
        "# Import Lakebase client\n",
        "from lakebase_client import LakebaseClient\n",
        "\n",
        "# Get OAuth token for authentication\n",
        "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
        "\n",
        "# OR use secrets (recommended for production)\n",
        "# token = dbutils.secrets.get(scope=\"lakebase\", key=\"token\")\n",
        "# host = dbutils.secrets.get(scope=\"lakebase\", key=\"host\")\n",
        "\n",
        "# Lakebase connection configuration\n",
        "# TODO: Update with your actual Lakebase host\n",
        "LAKEBASE_CONFIG = {\n",
        "    \"host\": \"your-workspace.cloud.databricks.com\",\n",
        "    \"port\": 5432,\n",
        "    \"database\": \"feature_store\",\n",
        "    \"user\": \"token\",\n",
        "    \"password\": token\n",
        "}\n",
        "\n",
        "print(\"\ud83d\udcca Connecting to Lakebase PostgreSQL...\\n\")\n",
        "\n",
        "# Initialize Lakebase client\n",
        "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
        "\n",
        "# Test connection\n",
        "print(\"\ud83d\udd0d Testing Lakebase connection...\")\n",
        "if lakebase.test_connection():\n",
        "    print(\"\u2705 Successfully connected to Lakebase PostgreSQL!\")\n",
        "    print(f\"   Host: {LAKEBASE_CONFIG['host']}\")\n",
        "    print(f\"   Port: {LAKEBASE_CONFIG['port']}\")\n",
        "    print(f\"   Database: {LAKEBASE_CONFIG['database']}\")\n",
        "else:\n",
        "    print(\"\u274c Failed to connect to Lakebase\")\n",
        "    print(\"   Please check:\")\n",
        "    print(\"   1. Lakebase instance is provisioned\")\n",
        "    print(\"   2. Host is correct\")\n",
        "    print(\"   3. OAuth token is valid\")\n",
        "    raise Exception(\"Lakebase connection failed\")\n",
        "\n",
        "# Create feature table\n",
        "print(\"\\n\ud83d\udccb Creating transaction_features table in Lakebase...\")\n",
        "lakebase.create_feature_table(\"transaction_features\")\n",
        "print(\"\u2705 Table created successfully!\")\n",
        "\n",
        "# Verify table exists\n",
        "print(\"\\n\ud83d\udcca Verifying table...\")\n",
        "try:\n",
        "    stats = lakebase.get_table_stats(\"transaction_features\")\n",
        "    print(f\"   Total rows: {stats['total_rows']:,}\")\n",
        "    print(f\"   Unique users: {stats['unique_users']:,}\")\n",
        "    print(f\"   Unique merchants: {stats['unique_merchants']:,}\")\n",
        "except Exception as e:\n",
        "    print(\"   Table exists but is empty (just created)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 LAKEBASE POSTGRESQL SETUP COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n\ud83d\udca1 Lakebase Info:\")\n",
        "print(\"   Type: PostgreSQL-compatible OLTP database\")\n",
        "print(\"   Port: 5432\")\n",
        "print(\"   Features: ACID transactions, <10ms query latency\")\n",
        "print(\"   Use case: Real-time feature serving\")\n",
        "print(\"\\n\ud83d\udcdd Table Schema:\")\n",
        "print(\"   transaction_features:\")\n",
        "print(\"     - transaction_id (PRIMARY KEY)\")\n",
        "print(\"     - timestamp\")\n",
        "print(\"     - user_id, merchant_id, amount\")\n",
        "print(\"     - Time-based features (hour, day_of_week, etc.)\")\n",
        "print(\"     - Amount features (amount_log, amount_sqrt)\")\n",
        "print(\"     - Velocity features (user_txn_count_1h, etc.)\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAGIC %md\n",
        "# MAGIC ## Setup Validation and Testing\n",
        "\n",
        "# Test basic functionality\n",
        "print(\"\ud83e\uddea Validating setup...\\n\")\n",
        "\n",
        "# 1. Test Spark functionality\n",
        "test_df = spark.range(10).toDF(\"id\")\n",
        "test_count = test_df.count()\n",
        "print(f\"\u2705 Spark test: Created DataFrame with {test_count} records\")\n",
        "\n",
        "# 2. Test Delta Lake functionality\n",
        "test_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta_test\")\n",
        "delta_test_df = spark.read.format(\"delta\").load(\"/tmp/delta_test\")\n",
        "print(f\"\u2705 Delta Lake test: Read {delta_test_df.count()} records from Delta table\")\n",
        "\n",
        "# 3. Test streaming capability\n",
        "try:\n",
        "    streaming_df = spark.readStream.format(\"rate\").load()\n",
        "    print(\"\u2705 Streaming test: Successfully created streaming DataFrame\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Streaming test warning: {e}\")\n",
        "\n",
        "# 4. Test feature engineering functions\n",
        "test_data = [(1, 100.0, \"2025-10-03 14:30:00\")]\n",
        "test_schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"amount\", DoubleType()),\n",
        "    StructField(\"timestamp\", StringType())\n",
        "])\n",
        "test_feature_df = spark.createDataFrame(test_data, test_schema)\n",
        "test_feature_df = test_feature_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
        "test_feature_df = test_feature_df \\\n",
        "    .withColumn(\"amount_log\", log1p(\"amount\")) \\\n",
        "    .withColumn(\"hour\", hour(\"timestamp\")) \\\n",
        "    .withColumn(\"day_of_week\", dayofweek(\"timestamp\")) \\\n",
        "    .withColumn(\"is_weekend\", when(dayofweek(\"timestamp\").isin([1, 7]), 1).otherwise(0))\n",
        "print(\"\u2705 Feature engineering test: Successfully applied transformations\")\n",
        "print(f\"   Generated features: {', '.join([c for c in test_feature_df.columns if c not in ['id', 'amount', 'timestamp']])}\")\n",
        "\n",
        "# 5. Test window functions (for velocity features)\n",
        "try:\n",
        "    window_spec = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").cast(\"long\")).rangeBetween(-3600, 0)\n",
        "    test_window_df = test_feature_df.withColumn(\"count_1h\", count(\"*\").over(window_spec))\n",
        "    print(\"\u2705 Window functions test: Successfully applied windowed aggregations\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Window functions test warning: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83c\udf89 SETUP VALIDATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u2705 All components validated successfully!\")\n",
        "print(\"\u2705 Ready to run streaming feature engineering pipeline!\")\n",
        "print(\"\\n\ud83d\udcdd Next steps:\")\n",
        "print(\"1. Lakebase PostgreSQL is ready at port 5432\")\n",
        "print(\"2. Generate streaming data: data_generator.py\")\n",
        "print(\"3. Run streaming demo: 01_streaming_lakebase_demo.ipynb\")\n",
        "print(\"4. Features will be written to Lakebase PostgreSQL (<100ms latency)\")\n",
        "print(\"\\n\ud83d\udca1 Tip: Check the console output in the streaming notebook for real-time feature monitoring\")\n",
        ""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}