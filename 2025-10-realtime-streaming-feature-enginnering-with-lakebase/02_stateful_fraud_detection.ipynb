{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stateful Fraud Detection with transformWithStateInPandas\n",
        "\n",
        "This notebook demonstrates **advanced streaming fraud detection** using PySpark's `transformWithStateInPandas` API.\n",
        "\n",
        "## What is transformWithStateInPandas?\n",
        "\n",
        "`transformWithStateInPandas` is the **next-generation** stateful streaming operator in Apache Spark 4.0+, replacing the older `applyInPandasWithState` API. According to the [official Spark documentation](https://spark.apache.org/docs/latest/streaming/structured-streaming-transform-with-state.html), it provides:\n",
        "\n",
        "- **Object-oriented design**: Define stateful logic using `StatefulProcessor` classes\n",
        "- **State variable types**: `ValueState`, `ListState`, `MapState` for optimized operations\n",
        "- **Automatic TTL eviction**: Built-in Time-To-Live for state cleanup\n",
        "- **Timer management**: Register, list, and delete timers for time-based processing\n",
        "- **State schema evolution**: Add/remove state variables across query runs\n",
        "- **Checkpointed timers**: Fault-tolerant timer persistence\n",
        "\n",
        "## Fraud Detection Features\n",
        "\n",
        "This notebook calculates real-time fraud indicators based on:\n",
        "\n",
        "1. **Transaction Velocity**: Count of transactions in time windows\n",
        "2. **IP Address Changes**: Frequency of IP changes per user\n",
        "3. **Location Anomalies**: Geographic distance from previous transaction\n",
        "4. **Amount Patterns**: Statistical anomalies in transaction amounts\n",
        "5. **Time-based Patterns**: Unusual transaction timing\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Streaming Source (Rate/Kafka)\n",
        "    ↓\n",
        "Feature Generation (TransactionDataGenerator)\n",
        "    ↓\n",
        "Group by Key (user_id)\n",
        "    ↓\n",
        "transformWithStateInPandas (StatefulProcessor)\n",
        "  • Maintain transaction history per user (ValueState + ListState)\n",
        "  • Calculate velocity features\n",
        "  • Detect location anomalies\n",
        "  • Track IP changes\n",
        "  • Compute fraud scores\n",
        "    ↓\n",
        "Write to Lakebase PostgreSQL (foreachBatch)\n",
        "    ↓\n",
        "Real-time Feature Serving (<10ms latency)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Run `00_setup.ipynb` first to provision Lakebase PostgreSQL\n",
        "- Databricks Runtime with Spark 4.0+ (for transformWithStateInPandas support)\n",
        "- Lakebase PostgreSQL instance configured and accessible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.streaming import StatefulProcessor, StatefulProcessorHandle\n",
        "from pyspark.sql.streaming.state import ValueState, ListState, TTLConfig\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Iterator\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utility modules\n",
        "from utils.data_generator import TransactionDataGenerator\n",
        "from utils.lakebase_client import LakebaseClient\n",
        "\n",
        "# Initialize data generator\n",
        "data_gen = TransactionDataGenerator(spark)\n",
        "\n",
        "print(\"Utility modules loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Lakebase connection\n",
        "LAKEBASE_CONFIG = {\n",
        "    \"instance_name\": \"neha-lakebase-demo\",\n",
        "    \"database\": \"databricks_postgres\"\n",
        "}\n",
        "\n",
        "# Initialize Lakebase client\n",
        "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
        "\n",
        "# Test connection\n",
        "if lakebase.test_connection():\n",
        "    print(\"Connected to Lakebase PostgreSQL\")\n",
        "else:\n",
        "    raise Exception(\"Failed to connect to Lakebase\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Streaming Transaction Data\n",
        "\n",
        "Generate synthetic transaction data with fraud indicators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate streaming transaction data\n",
        "df_transactions = data_gen.generate_transaction_data(\n",
        "    rows_per_second=10,\n",
        "    num_users=100,\n",
        "    fraud_ratio=0.1\n",
        ")\n",
        "\n",
        "print(\"Schema of streaming transactions:\")\n",
        "df_transactions.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Output Schema\n",
        "\n",
        "Define the output schema for fraud features. State is managed internally by the `StatefulProcessor` using `ValueState` and `ListState`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define output schema - fraud features per transaction\n",
        "output_schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), False),\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"amount\", DoubleType(), False),\n",
        "    StructField(\"merchant_id\", StringType(), False),\n",
        "    StructField(\"ip_address\", StringType(), False),\n",
        "    StructField(\"latitude\", DoubleType(), False),\n",
        "    StructField(\"longitude\", DoubleType(), False),\n",
        "    \n",
        "    # Fraud detection features\n",
        "    StructField(\"user_transaction_count\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_hour\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_10min\", IntegerType(), False),\n",
        "    StructField(\"ip_changed\", IntegerType(), False),\n",
        "    StructField(\"ip_change_count_total\", IntegerType(), False),\n",
        "    StructField(\"distance_from_last_km\", DoubleType(), True),\n",
        "    StructField(\"velocity_kmh\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_avg_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_max_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_zscore\", DoubleType(), True),\n",
        "    StructField(\"seconds_since_last_transaction\", DoubleType(), True),\n",
        "    StructField(\"is_rapid_transaction\", IntegerType(), False),\n",
        "    StructField(\"is_impossible_travel\", IntegerType(), False),\n",
        "    StructField(\"is_amount_anomaly\", IntegerType(), False),\n",
        "    StructField(\"fraud_score\", DoubleType(), False),\n",
        "    StructField(\"is_fraud_prediction\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "print(\"Output schema defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implement FraudDetectorProcessor (StatefulProcessor)\n",
        "\n",
        "Create a `StatefulProcessor` class that implements the fraud detection logic. This is the object-oriented approach introduced in Spark 4.0+ for `transformWithStateInPandas`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate distance between two geographic points in kilometers.\n",
        "    \"\"\"\n",
        "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
        "        return None\n",
        "    \n",
        "    R = 6371.0  # Earth radius in kilometers\n",
        "    \n",
        "    # Convert to radians\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    \n",
        "    # Haversine formula\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    \n",
        "    return R * c\n",
        "\n",
        "print(\"Distance calculation helper function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FraudDetectorProcessor(StatefulProcessor):\n",
        "    \"\"\"\n",
        "    StatefulProcessor for fraud detection using transformWithStateInPandas.\n",
        "    \n",
        "    This processor maintains a single consolidated state per user to detect fraud patterns including:\n",
        "    - Transaction velocity (time windows)\n",
        "    - IP address changes\n",
        "    - Geographic anomalies (impossible travel)\n",
        "    - Amount-based anomalies\n",
        "    \"\"\"\n",
        "    \n",
        "    def init(self, handle: StatefulProcessorHandle) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the stateful processor with a single consolidated state variable.\n",
        "        \n",
        "        Consolidated state includes:\n",
        "        - Transaction count\n",
        "        - Last transaction details (timestamp, IP, location)\n",
        "        - IP change count\n",
        "        - Amount statistics (total, avg, max)\n",
        "        - Recent transaction times (up to 50)\n",
        "        - Recent transaction amounts (up to 50)\n",
        "        \"\"\"\n",
        "        self.handle = handle\n",
        "        \n",
        "        # Define comprehensive state schema - consolidates ALL state into one object\n",
        "        state_schema = StructType([\n",
        "            # Transaction count\n",
        "            StructField(\"transaction_count\", IntegerType(), False),\n",
        "            \n",
        "            # Last transaction details\n",
        "            StructField(\"last_timestamp\", TimestampType(), True),\n",
        "            StructField(\"last_ip_address\", StringType(), True),\n",
        "            StructField(\"last_latitude\", DoubleType(), True),\n",
        "            StructField(\"last_longitude\", DoubleType(), True),\n",
        "            \n",
        "            # IP change tracking\n",
        "            StructField(\"ip_change_count\", IntegerType(), False),\n",
        "            \n",
        "            # Amount statistics\n",
        "            StructField(\"total_amount\", DoubleType(), False),\n",
        "            StructField(\"avg_amount\", DoubleType(), False),\n",
        "            StructField(\"max_amount\", DoubleType(), False),\n",
        "            \n",
        "            # Recent transaction history (bounded to 50 each)\n",
        "            StructField(\"recent_timestamps\", ArrayType(TimestampType()), False),\n",
        "            StructField(\"recent_amounts\", ArrayType(DoubleType()), False)\n",
        "        ])\n",
        "        \n",
        "        # Initialize SINGLE consolidated state variable with TTL (1 hour of inactivity)\n",
        "        ttl_config = TTLConfig(ttl_duration=timedelta(hours=1))\n",
        "        \n",
        "        self.user_state = handle.getValueState(\n",
        "            \"user_fraud_state\",  # Single state variable name\n",
        "            state_schema,\n",
        "            ttl_config\n",
        "        )\n",
        "    \n",
        "    def handleInputRows(self, key, rows, timer_values) -> Iterator[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Process input rows for a given user and emit fraud features.\n",
        "        \n",
        "        Args:\n",
        "            key: user_id\n",
        "            rows: Iterator of Pandas DataFrames containing transactions for this user\n",
        "            timer_values: Timer values (not used in this implementation)\n",
        "        \n",
        "        Yields:\n",
        "            pd.DataFrame: Enriched transactions with fraud features\n",
        "        \"\"\"\n",
        "        user_id = key\n",
        "        \n",
        "        # Process each micro-batch\n",
        "        for pdf in rows:\n",
        "            if pdf.empty:\n",
        "                continue\n",
        "            \n",
        "            # Sort by timestamp\n",
        "            pdf = pdf.sort_values('timestamp')\n",
        "            \n",
        "            # Retrieve existing state (single consolidated object)\n",
        "            if self.user_state.exists():\n",
        "                state = self.user_state.get()\n",
        "                prev_count = state[0]  # transaction_count\n",
        "                prev_last_time = state[1]  # last_timestamp\n",
        "                prev_ip = state[2]  # last_ip_address\n",
        "                prev_lat = state[3]  # last_latitude\n",
        "                prev_lon = state[4]  # last_longitude\n",
        "                prev_ip_changes = state[5]  # ip_change_count\n",
        "                prev_total_amount = state[6]  # total_amount\n",
        "                prev_avg_amount = state[7]  # avg_amount\n",
        "                prev_max_amount = state[8]  # max_amount\n",
        "                prev_times = list(state[9]) if state[9] else []  # recent_timestamps\n",
        "                prev_amounts = list(state[10]) if state[10] else []  # recent_amounts\n",
        "            else:\n",
        "                # Initialize state for new user\n",
        "                prev_count = 0\n",
        "                prev_last_time = None\n",
        "                prev_ip = None\n",
        "                prev_lat = None\n",
        "                prev_lon = None\n",
        "                prev_ip_changes = 0\n",
        "                prev_total_amount = 0.0\n",
        "                prev_avg_amount = 0.0\n",
        "                prev_max_amount = 0.0\n",
        "                prev_times = []\n",
        "                prev_amounts = []\n",
        "            \n",
        "            # Process each transaction\n",
        "            results = []\n",
        "            \n",
        "            for idx, row in pdf.iterrows():\n",
        "                current_time = row['timestamp']\n",
        "                current_ip = row['ip_address']\n",
        "                current_lat = row['latitude']\n",
        "                current_lon = row['longitude']\n",
        "                current_amount = row['amount']\n",
        "                \n",
        "                # Update transaction count\n",
        "                prev_count += 1\n",
        "                \n",
        "                # Calculate time-based features\n",
        "                if prev_last_time is not None:\n",
        "                    time_diff = (current_time - prev_last_time).total_seconds()\n",
        "                else:\n",
        "                    time_diff = None\n",
        "                \n",
        "                # IP change detection\n",
        "                ip_changed = 0\n",
        "                if prev_ip is not None and current_ip != prev_ip:\n",
        "                    ip_changed = 1\n",
        "                    prev_ip_changes += 1\n",
        "                \n",
        "                # Geographic distance calculation\n",
        "                distance_km = None\n",
        "                velocity_kmh = None\n",
        "                if prev_lat is not None and prev_lon is not None:\n",
        "                    distance_km = calculate_haversine_distance(\n",
        "                        prev_lat, prev_lon, current_lat, current_lon\n",
        "                    )\n",
        "                    if distance_km is not None and time_diff is not None and time_diff > 0:\n",
        "                        velocity_kmh = (distance_km / time_diff) * 3600\n",
        "                \n",
        "                # Amount-based features\n",
        "                prev_total_amount += current_amount\n",
        "                prev_avg_amount = prev_total_amount / prev_count\n",
        "                prev_max_amount = max(prev_max_amount, current_amount)\n",
        "                \n",
        "                amount_vs_avg_ratio = current_amount / prev_avg_amount if prev_avg_amount > 0 else 1.0\n",
        "                amount_vs_max_ratio = current_amount / prev_max_amount if prev_max_amount > 0 else 1.0\n",
        "                \n",
        "                # Z-score calculation\n",
        "                amount_zscore = None\n",
        "                if len(prev_amounts) >= 3:\n",
        "                    amounts_std = np.std(prev_amounts)\n",
        "                    if amounts_std > 0:\n",
        "                        amount_zscore = (current_amount - prev_avg_amount) / amounts_std\n",
        "                \n",
        "                # Update recent transactions (bounded to 50)\n",
        "                prev_times.append(current_time)\n",
        "                prev_amounts.append(current_amount)\n",
        "                if len(prev_times) > 50:\n",
        "                    prev_times = prev_times[-50:]\n",
        "                    prev_amounts = prev_amounts[-50:]\n",
        "                \n",
        "                # Count transactions in time windows\n",
        "                one_hour_ago = current_time - timedelta(hours=1)\n",
        "                ten_min_ago = current_time - timedelta(minutes=10)\n",
        "                \n",
        "                trans_last_hour = sum(1 for t in prev_times if t >= one_hour_ago)\n",
        "                trans_last_10min = sum(1 for t in prev_times if t >= ten_min_ago)\n",
        "                \n",
        "                # Fraud indicators\n",
        "                is_rapid = 1 if trans_last_10min >= 5 else 0\n",
        "                is_impossible_travel = 1 if velocity_kmh is not None and velocity_kmh > 800 else 0\n",
        "                is_amount_anomaly = 1 if amount_zscore is not None and abs(amount_zscore) > 3 else 0\n",
        "                \n",
        "                # Calculate fraud score (0-100)\n",
        "                fraud_score = 0.0\n",
        "                if is_rapid:\n",
        "                    fraud_score += 20\n",
        "                if is_impossible_travel:\n",
        "                    fraud_score += 30\n",
        "                if is_amount_anomaly:\n",
        "                    fraud_score += 25\n",
        "                if prev_ip_changes >= 5:\n",
        "                    fraud_score += 15\n",
        "                if trans_last_hour >= 10:\n",
        "                    fraud_score += 10\n",
        "                fraud_score = min(fraud_score, 100.0)\n",
        "                \n",
        "                # Fraud prediction\n",
        "                is_fraud_pred = 1 if fraud_score >= 50 else 0\n",
        "                \n",
        "                # Append result\n",
        "                results.append({\n",
        "                    'transaction_id': row['transaction_id'],\n",
        "                    'user_id': user_id,\n",
        "                    'timestamp': current_time,\n",
        "                    'amount': current_amount,\n",
        "                    'merchant_id': row['merchant_id'],\n",
        "                    'ip_address': current_ip,\n",
        "                    'latitude': current_lat,\n",
        "                    'longitude': current_lon,\n",
        "                    'user_transaction_count': prev_count,\n",
        "                    'transactions_last_hour': trans_last_hour,\n",
        "                    'transactions_last_10min': trans_last_10min,\n",
        "                    'ip_changed': ip_changed,\n",
        "                    'ip_change_count_total': prev_ip_changes,\n",
        "                    'distance_from_last_km': distance_km,\n",
        "                    'velocity_kmh': velocity_kmh,\n",
        "                    'amount_vs_user_avg_ratio': amount_vs_avg_ratio,\n",
        "                    'amount_vs_user_max_ratio': amount_vs_max_ratio,\n",
        "                    'amount_zscore': amount_zscore,\n",
        "                    'seconds_since_last_transaction': time_diff,\n",
        "                    'is_rapid_transaction': is_rapid,\n",
        "                    'is_impossible_travel': is_impossible_travel,\n",
        "                    'is_amount_anomaly': is_amount_anomaly,\n",
        "                    'fraud_score': fraud_score,\n",
        "                    'is_fraud_prediction': is_fraud_pred\n",
        "                })\n",
        "                \n",
        "                # Update state for next transaction\n",
        "                prev_last_time = current_time\n",
        "                prev_ip = current_ip\n",
        "                prev_lat = current_lat\n",
        "                prev_lon = current_lon\n",
        "            \n",
        "            # Update SINGLE consolidated state object (atomic update)\n",
        "            self.user_state.update((\n",
        "                prev_count,           # transaction_count\n",
        "                prev_last_time,       # last_timestamp\n",
        "                prev_ip,              # last_ip_address\n",
        "                prev_lat,             # last_latitude\n",
        "                prev_lon,             # last_longitude\n",
        "                prev_ip_changes,      # ip_change_count\n",
        "                prev_total_amount,    # total_amount\n",
        "                prev_avg_amount,      # avg_amount\n",
        "                prev_max_amount,      # max_amount\n",
        "                prev_times,           # recent_timestamps\n",
        "                prev_amounts          # recent_amounts\n",
        "            ))\n",
        "            \n",
        "            # Yield results\n",
        "            if results:\n",
        "                yield pd.DataFrame(results)\n",
        "    \n",
        "    def handleExpiredTimer(self, key, timer_values, expired_timer_info) -> Iterator[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Handle expired timers (not used in this implementation).\n",
        "        \"\"\"\n",
        "        # No timer logic in this basic implementation\n",
        "        yield pd.DataFrame()\n",
        "    \n",
        "    def close(self) -> None:\n",
        "        \"\"\"\n",
        "        Perform cleanup operations (none needed).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "print(\"FraudDetectorProcessor class defined with CONSOLIDATED state\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply transformWithStateInPandas\n",
        "\n",
        "Apply the `FraudDetectorProcessor` to the streaming data using `transformWithStateInPandas`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply stateful fraud detection using transformWithStateInPandas\n",
        "df_with_fraud_features = df_transactions \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .transformWithStateInPandas(\n",
        "        statefulProcessor=FraudDetectorProcessor(),\n",
        "        outputStructType=output_schema,\n",
        "        outputMode=\"Append\",\n",
        "        timeMode=\"None\"\n",
        "    )\n",
        "\n",
        "print(\"Stateful processing configured with transformWithStateInPandas\")\n",
        "print(\"\\nOutput schema:\")\n",
        "df_with_fraud_features.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Unified Feature Table in Lakebase\n",
        "\n",
        "Create a comprehensive PostgreSQL table that stores **ALL features** in one place:\n",
        "\n",
        "### Table Design: `fraud_features`\n",
        "\n",
        "This unified table combines:\n",
        "\n",
        "1. **Stateless Transaction Features** (from `FeatureEngineer` class):\n",
        "   - Time-based features (cyclical encodings, business hours, etc.)\n",
        "   - Amount-based features (log, sqrt, categories, etc.)\n",
        "   - Merchant risk scores\n",
        "   - Device and network features\n",
        "   \n",
        "2. **Stateful Fraud Detection Features** (from `FraudDetectorProcessor`):\n",
        "   - Transaction velocity (counts in time windows)\n",
        "   - IP change tracking\n",
        "   - Geographic anomalies (impossible travel)\n",
        "   - Amount anomalies (z-scores, ratios)\n",
        "   - Composite fraud scores and predictions\n",
        "\n",
        "### Why One Table?\n",
        "\n",
        "✅ **Unified feature store** - All features for ML in one query  \n",
        "✅ **Simplified architecture** - Single table vs joining multiple tables  \n",
        "✅ **Better performance** - No joins needed for model inference  \n",
        "✅ **Easier to maintain** - One schema to manage\n",
        "\n",
        "**Note:** If you want to add stateless features, you can apply `FeatureEngineer` transformations before or after the `FraudDetectorProcessor`. For this demo, we'll create the table structure ready to accept both types of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified fraud features table using LakebaseClient method\n",
        "print(\"Creating unified fraud_features table in Lakebase...\")\n",
        "lakebase.create_fraud_features_table(\"fraud_features\")\n",
        "print(\"Table created successfully!\")\n",
        "print(\"\\nThis unified table includes:\")\n",
        "print(\"  • Stateless transaction features (~40 columns)\")\n",
        "print(\"  • Stateful fraud detection features (~25 columns)\")\n",
        "print(\"  • Processing metadata (5 columns)\")\n",
        "print(f\"  • Total: ~70+ columns ready for ML\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Write Fraud Features to Lakebase\n",
        "\n",
        "Stream fraud features to Lakebase PostgreSQL for real-time serving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define foreachBatch function to write to Lakebase\n",
        "def write_to_lakebase(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Write each micro-batch to Lakebase PostgreSQL.\n",
        "    \"\"\"\n",
        "    if batch_df.isEmpty():\n",
        "        return\n",
        "    \n",
        "    logger.info(f\"Processing batch {batch_id} with {batch_df.count()} rows\")\n",
        "    \n",
        "    # Write to Lakebase using client\n",
        "    lakebase.write_streaming_batch(batch_df, \"fraud_features\")\n",
        "    \n",
        "    logger.info(f\"Batch {batch_id} written to Lakebase\")\n",
        "\n",
        "\n",
        "# Start streaming query to Lakebase\n",
        "query_lakebase = df_with_fraud_features \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .foreachBatch(write_to_lakebase) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/fraud_detection_checkpoint\") \\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"Streaming to Lakebase PostgreSQL...\")\n",
        "print(f\"Query ID: {query_lakebase.id}\")\n",
        "print(f\"Status: {query_lakebase.status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Monitor and Query Fraud Features\n",
        "\n",
        "Query fraud features from Lakebase for real-time insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for data to process\n",
        "import time\n",
        "print(\"Waiting 30 seconds for data to process...\")\n",
        "time.sleep(30)\n",
        "\n",
        "# Query top users by fraud score\n",
        "query_results = \"\"\"\n",
        "SELECT \n",
        "    user_id,\n",
        "    COUNT(*) as total_transactions,\n",
        "    SUM(is_fraud_prediction) as predicted_frauds,\n",
        "    AVG(fraud_score) as avg_fraud_score,\n",
        "    MAX(fraud_score) as max_fraud_score,\n",
        "    SUM(is_rapid_transaction) as rapid_transactions,\n",
        "    SUM(is_impossible_travel) as impossible_travels,\n",
        "    SUM(is_amount_anomaly) as amount_anomalies\n",
        "FROM fraud_features\n",
        "GROUP BY user_id\n",
        "ORDER BY predicted_frauds DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    result_df = pd.read_sql(query_results, conn)\n",
        "\n",
        "print(\"\\nTop 10 Users by Predicted Fraud Count:\")\n",
        "display(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query high-risk transactions\n",
        "high_risk_query = \"\"\"\n",
        "SELECT \n",
        "    transaction_id,\n",
        "    user_id,\n",
        "    timestamp,\n",
        "    amount,\n",
        "    fraud_score,\n",
        "    is_rapid_transaction,\n",
        "    is_impossible_travel,\n",
        "    is_amount_anomaly,\n",
        "    transactions_last_10min,\n",
        "    velocity_kmh\n",
        "FROM fraud_features\n",
        "WHERE fraud_score >= 50\n",
        "ORDER BY fraud_score DESC, timestamp DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    high_risk_df = pd.read_sql(high_risk_query, conn)\n",
        "\n",
        "print(\"\\nHigh-Risk Transactions (fraud_score >= 50):\")\n",
        "display(high_risk_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-time feature serving example - Get features for specific user\n",
        "def get_user_fraud_features(user_id: str):\n",
        "    \"\"\"\n",
        "    Get real-time fraud features for a user from Lakebase PostgreSQL.\n",
        "    Query latency: <10ms\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        transaction_id,\n",
        "        timestamp,\n",
        "        amount,\n",
        "        user_transaction_count,\n",
        "        transactions_last_hour,\n",
        "        transactions_last_10min,\n",
        "        fraud_score,\n",
        "        is_fraud_prediction\n",
        "    FROM fraud_features\n",
        "    WHERE user_id = %s\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    \n",
        "    with lakebase.get_connection() as conn:\n",
        "        df = pd.read_sql(query, conn, params=(user_id,))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example: Get features for a user\n",
        "sample_user = \"user_001\"\n",
        "user_features = get_user_fraud_features(sample_user)\n",
        "\n",
        "print(f\"\\nRecent transactions for {sample_user}:\")\n",
        "display(user_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Stop Streaming Queries\n",
        "\n",
        "When done, stop the streaming query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop streaming query\n",
        "if query_lakebase.isActive:\n",
        "    query_lakebase.stop()\n",
        "    print(\"Streaming query stopped\")\n",
        "\n",
        "print(\"\\nAll streaming queries stopped successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated advanced streaming fraud detection using `transformWithStateInPandas` with a **consolidated state object** - the next-generation stateful streaming API in Apache Spark 4.0+.\n",
        "\n",
        "### Key Capabilities\n",
        "\n",
        "1. **Object-Oriented Stateful Processing**: Using `StatefulProcessor` class with `transformWithStateInPandas`\n",
        "2. **Consolidated State Management**: Single `ValueState` object containing all user state (atomic updates)\n",
        "3. **Automatic TTL Eviction**: Built-in 1-hour TTL for state cleanup\n",
        "4. **Fraud Features**:\n",
        "   - Transaction velocity (counts in time windows)\n",
        "   - IP address change tracking\n",
        "   - Geographic anomalies (impossible travel detection)\n",
        "   - Amount-based anomalies (z-score, ratios)\n",
        "   - Composite fraud scores (0-100)\n",
        "5. **Real-time Serving**: Writing features to Lakebase PostgreSQL for <10ms query latency\n",
        "6. **Production Patterns**: Proper watermarking and checkpointing\n",
        "\n",
        "### Key Advantages of transformWithStateInPandas\n",
        "\n",
        "According to the [Apache Spark documentation](https://spark.apache.org/docs/latest/streaming/structured-streaming-transform-with-state.html), `transformWithStateInPandas` provides:\n",
        "\n",
        "- **Object-oriented design**: Define stateful logic using `StatefulProcessor` classes (vs function-based `applyInPandasWithState`)\n",
        "- **State variable types**: `ValueState`, `ListState`, `MapState` optimized for different operations\n",
        "- **Automatic TTL eviction**: Built-in Time-To-Live for state cleanup (1 hour in this example)\n",
        "- **Timer management**: Register, list, and delete timers for time-based processing\n",
        "- **State schema evolution**: Add/remove state variables across query runs\n",
        "- **Checkpointed timers**: Fault-tolerant timer persistence\n",
        "- **Next-generation API**: Replaces older `applyInPandasWithState` in Spark 4.0+\n",
        "\n",
        "### Consolidated State Architecture\n",
        "\n",
        "Instead of managing 6 separate state variables, this implementation uses a **single consolidated `ValueState`** for:\n",
        "\n",
        "```\n",
        "user_fraud_state (ValueState) - SINGLE STATE OBJECT\n",
        "  ├── transaction_count (int)\n",
        "  ├── last_timestamp (timestamp)\n",
        "  ├── last_ip_address (string)\n",
        "  ├── last_latitude (double)\n",
        "  ├── last_longitude (double)\n",
        "  ├── ip_change_count (int)\n",
        "  ├── total_amount (double)\n",
        "  ├── avg_amount (double)\n",
        "  ├── max_amount (double)\n",
        "  ├── recent_timestamps (array<timestamp>)  # Bounded to 50\n",
        "  └── recent_amounts (array<double>)        # Bounded to 50\n",
        "```\n",
        "\n",
        "**Benefits of Consolidated State:**\n",
        "- ✅ **Atomic updates**: All state updated in a single operation\n",
        "- ✅ **Simplified code**: Single state variable vs 6 separate ones\n",
        "- ✅ **Better performance**: Single read/write vs multiple operations\n",
        "- ✅ **Easier to reason about**: State is cohesive and self-contained\n",
        "- ✅ **Schema evolution**: Easier to add/modify fields in one place\n",
        "\n",
        "### StatefulProcessor Implementation\n",
        "\n",
        "```\n",
        "FraudDetectorProcessor (StatefulProcessor)\n",
        "  ├── init() - Initialize single consolidated state\n",
        "  │   └── user_fraud_state (ValueState with 11 fields + arrays)\n",
        "  │\n",
        "  ├── handleInputRows() - Process transactions\n",
        "  │   ├── state = self.user_state.get() (single read)\n",
        "  │   ├── Calculate fraud features\n",
        "  │   └── self.user_state.update((...)) (single atomic write)\n",
        "  │\n",
        "  ├── handleExpiredTimer() - Handle timers (not used here)\n",
        "  └── close() - Cleanup operations (none needed)\n",
        "```\n",
        "\n",
        "### Fraud Detection Logic\n",
        "\n",
        "**Fraud Score Calculation (0-100 points):**\n",
        "- Rapid transactions (5+ in 10 min): +20 points\n",
        "- Impossible travel (>800 km/h): +30 points  \n",
        "- Amount anomaly (z-score > 3): +25 points\n",
        "- Frequent IP changes (5+ total): +15 points\n",
        "- High velocity (10+ in 1 hour): +10 points\n",
        "\n",
        "**Fraud Prediction:** Score >= 50 triggers fraud flag\n",
        "\n",
        "### Real-time Architecture\n",
        "\n",
        "```\n",
        "Streaming Transactions\n",
        "    ↓\n",
        "transformWithStateInPandas (grouped by user_id)\n",
        "  • StatefulProcessor with SINGLE consolidated ValueState\n",
        "  • Automatic TTL (1 hour)\n",
        "  • Bounded arrays (last 50 transactions)\n",
        "  • Atomic state updates\n",
        "    ↓\n",
        "foreachBatch\n",
        "    ↓\n",
        "Lakebase PostgreSQL (fraud_features table)\n",
        "    ↓\n",
        "Real-time Queries (<10ms latency)\n",
        "```\n",
        "\n",
        "### Comparison: Multiple vs Consolidated State\n",
        "\n",
        "**OLD (6 separate state variables):**\n",
        "```python\n",
        "# Multiple reads\n",
        "if self.transaction_count.exists():\n",
        "    prev_count = self.transaction_count.get()[0]\n",
        "if self.last_transaction.exists():\n",
        "    last_txn = self.last_transaction.get()\n",
        "    prev_last_time = last_txn[0]\n",
        "# ... 4 more state variable reads\n",
        "\n",
        "# Multiple writes\n",
        "self.transaction_count.update((prev_count,))\n",
        "self.last_transaction.update((prev_last_time, prev_ip, prev_lat, prev_lon))\n",
        "self.ip_change_count.update((prev_ip_changes,))\n",
        "# ... 3 more state variable writes\n",
        "```\n",
        "\n",
        "**NEW (1 consolidated state object):**\n",
        "```python\n",
        "# Single read\n",
        "if self.user_state.exists():\n",
        "    state = self.user_state.get()\n",
        "    prev_count = state[0]\n",
        "    prev_last_time = state[1]\n",
        "    prev_ip = state[2]\n",
        "    # ... all fields in one read\n",
        "\n",
        "# Single atomic write\n",
        "self.user_state.update((\n",
        "    prev_count, prev_last_time, prev_ip, prev_lat, prev_lon,\n",
        "    prev_ip_changes, prev_total_amount, prev_avg_amount, \n",
        "    prev_max_amount, prev_times, prev_amounts\n",
        "))\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Integrate with ML models for enhanced fraud scoring\n",
        "- Add timer-based processing using `handleExpiredTimer()`\n",
        "- Implement state schema evolution for adding new fields\n",
        "- Connect to downstream systems (dashboards, notification services)\n",
        "- Tune TTL duration and processing trigger intervals\n",
        "- Add more sophisticated fraud detection rules (device fingerprinting, network analysis)\n",
        "- Implement A/B testing for fraud detection thresholds\n",
        "- Explore `MapState` for multi-level hierarchical state\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
