{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stateful Fraud Detection with applyInPandasWithState\n",
        "\n",
        "This notebook demonstrates **advanced streaming fraud detection** using PySpark's `applyInPandasWithState` API.\n",
        "\n",
        "## What is applyInPandasWithState?\n",
        "\n",
        "`applyInPandasWithState` is a powerful Structured Streaming API that enables:\n",
        "- **Stateful processing**: Maintain state across micro-batches per key (e.g., user_id, IP address)\n",
        "- **Pandas UDFs**: Process data using familiar Pandas operations\n",
        "- **Complex logic**: Implement sophisticated fraud detection rules with historical context\n",
        "- **Bounded state**: Automatic state cleanup with timeout management\n",
        "\n",
        "## Fraud Detection Features\n",
        "\n",
        "This notebook calculates real-time fraud indicators based on:\n",
        "\n",
        "1. **Transaction Velocity**: Count of transactions in time window\n",
        "2. **IP Address Changes**: Frequency of IP changes per user\n",
        "3. **Location Anomalies**: Geographic distance from previous transaction\n",
        "4. **Amount Patterns**: Statistical anomalies in transaction amounts\n",
        "5. **Time-based Patterns**: Unusual transaction timing\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Streaming Source (Rate/Kafka)\n",
        "    \u2193\n",
        "Feature Generation (TransactionDataGenerator)\n",
        "    \u2193\n",
        "Group by Key (user_id)\n",
        "    \u2193\n",
        "applyInPandasWithState\n",
        "  \u2022 Maintain transaction history per user\n",
        "  \u2022 Calculate velocity features\n",
        "  \u2022 Detect location anomalies\n",
        "  \u2022 Track IP changes\n",
        "  \u2022 Compute fraud scores\n",
        "    \u2193\n",
        "Write to Lakebase PostgreSQL (foreachBatch)\n",
        "    \u2193\n",
        "Real-time Feature Serving (<10ms latency)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Run `00_setup.ipynb` first to provision Lakebase PostgreSQL\n",
        "- Databricks Runtime 13.0+ (for applyInPandasWithState support)\n",
        "- Lakebase PostgreSQL instance configured and accessible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Iterator\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utility modules\n",
        "from utils.data_generator import TransactionDataGenerator\n",
        "from utils.lakebase_client import LakebaseClient\n",
        "from utils.feature_engineering import FraudDetectionFeaturesProcessor\n",
        "\n",
        "# Initialize data generator\n",
        "data_gen = TransactionDataGenerator(spark)\n",
        "\n",
        "print(\"Utility modules loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Lakebase connection\n",
        "LAKEBASE_CONFIG = {\n",
        "    \"instance_name\": \"neha-lakebase-demo\",\n",
        "    \"database\": \"databricks_postgres\"\n",
        "}\n",
        "\n",
        "# Initialize Lakebase client\n",
        "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
        "\n",
        "# Test connection\n",
        "if lakebase.test_connection():\n",
        "    print(\"Connected to Lakebase PostgreSQL\")\n",
        "else:\n",
        "    raise Exception(\"Failed to connect to Lakebase\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Streaming Transaction Data\n",
        "\n",
        "Generate synthetic transaction data with fraud indicators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate streaming transaction data\n",
        "df_transactions = data_gen.generate_transaction_data(\n",
        "    rows_per_second=10,\n",
        "    num_users=100,\n",
        "    fraud_ratio=0.1\n",
        ")\n",
        "\n",
        "print(\"Schema of streaming transactions:\")\n",
        "df_transactions.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Output Schema\n",
        "\n",
        "Define the output schema for fraud features. State is managed internally by the `StatefulProcessor` using `ValueState` and `ListState`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define output schema - fraud features per transaction\n",
        "output_schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), False),\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"amount\", DoubleType(), False),\n",
        "    StructField(\"merchant_id\", StringType(), False),\n",
        "    StructField(\"ip_address\", StringType(), False),\n",
        "    StructField(\"latitude\", DoubleType(), False),\n",
        "    StructField(\"longitude\", DoubleType(), False),\n",
        "    \n",
        "    # Fraud detection features\n",
        "    StructField(\"user_transaction_count\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_hour\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_10min\", IntegerType(), False),\n",
        "    StructField(\"ip_changed\", IntegerType(), False),\n",
        "    StructField(\"ip_change_count_total\", IntegerType(), False),\n",
        "    StructField(\"distance_from_last_km\", DoubleType(), True),\n",
        "    StructField(\"velocity_kmh\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_avg_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_max_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_zscore\", DoubleType(), True),\n",
        "    StructField(\"seconds_since_last_transaction\", DoubleType(), True),\n",
        "    StructField(\"is_rapid_transaction\", IntegerType(), False),\n",
        "    StructField(\"is_impossible_travel\", IntegerType(), False),\n",
        "    StructField(\"is_amount_anomaly\", IntegerType(), False),\n",
        "    StructField(\"fraud_score\", DoubleType(), False),\n",
        "    StructField(\"is_fraud_prediction\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "print(\"Output schema defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implement FraudDetectionFeaturesProcessor (StatefulProcessor)\n",
        "\n",
        "Create a `StatefulProcessor` class that implements the fraud detection logic. This is the object-oriented approach introduced in Spark 4.0+ for `transformWithStateInPandas`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply transformWithStateInPandas\n",
        "\n",
        "Apply the `FraudDetectionFeaturesProcessor` to the streaming data using `transformWithStateInPandas`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply stateful fraud detection using transformWithStateInPandas\n",
        "df_with_fraud_features = df_transactions \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .transformWithStateInPandas(\n",
        "        statefulProcessor=FraudDetectionFeaturesProcessor(),\n",
        "        outputStructType=output_schema,\n",
        "        outputMode=\"Append\",\n",
        "        timeMode=\"None\"\n",
        "    )\n",
        "\n",
        "print(\"Stateful processing configured with transformWithStateInPandas\")\n",
        "print(\"\\nOutput schema:\")\n",
        "df_with_fraud_features.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Unified Feature Table\n",
        "\n",
        "Verify that the unified feature table was created during setup.\n",
        "\n",
        "### Table Design: `fraud_features`\n",
        "\n",
        "This unified table (created in `00_setup.ipynb`) combines:\n",
        "\n",
        "1. **Stateless Transaction Features** (from `AdvancedFeatureEngineering` class):\n",
        "   - Time-based features (cyclical encodings, business hours, etc.)\n",
        "   - Amount-based features (log, sqrt, categories, etc.)\n",
        "   - Merchant risk scores\n",
        "   - Device and network features\n",
        "   \n",
        "2. **Stateful Fraud Detection Features** (from `FraudDetectionFeaturesProcessor`):\n",
        "   - Transaction velocity (counts in time windows)\n",
        "   - IP change tracking\n",
        "   - Geographic anomalies (impossible travel)\n",
        "   - Amount anomalies (z-scores, ratios)\n",
        "   - Composite fraud scores and predictions\n",
        "\n",
        "### Why One Table?\n",
        "\n",
        "\u2705 **Unified feature store** - All features for ML in one query  \n",
        "\u2705 **Simplified architecture** - Single table vs joining multiple tables  \n",
        "\u2705 **Better performance** - No joins needed for model inference  \n",
        "\u2705 **Easier to maintain** - One schema to manage\n",
        "\n",
        "**Note:** The table should already exist from running `00_setup.ipynb`. If not, it will be created automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that the unified feature table exists (created in 00_setup.ipynb)\n",
        "print(\"Verifying fraud_features table...\")\n",
        "try:\n",
        "    stats = lakebase.get_table_stats(\"fraud_features\")\n",
        "    print(f\"   Table exists with {stats['total_rows']:,} rows\")\n",
        "    print(\"   Table is ready for fraud feature writes\")\n",
        "except Exception as e:\n",
        "    print(\"   Table not found. Creating it now...\")\n",
        "    lakebase.create_feature_table(\"fraud_features\")\n",
        "    print(\"   Table created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Write Fraud Features to Lakebase\n",
        "\n",
        "Stream fraud features to Lakebase PostgreSQL for real-time serving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define foreachBatch function to write to Lakebase\n",
        "def write_to_lakebase(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Write each micro-batch to Lakebase PostgreSQL.\n",
        "    \"\"\"\n",
        "    if batch_df.isEmpty():\n",
        "        return\n",
        "    \n",
        "    logger.info(f\"Processing batch {batch_id} with {batch_df.count()} rows\")\n",
        "    \n",
        "    # Write to Lakebase using client\n",
        "    lakebase.write_streaming_batch(batch_df, \"fraud_features\")\n",
        "    \n",
        "    logger.info(f\"Batch {batch_id} written to Lakebase\")\n",
        "\n",
        "\n",
        "# Start streaming query to Lakebase\n",
        "query_lakebase = df_with_fraud_features \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .foreachBatch(write_to_lakebase) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/fraud_detection_checkpoint\") \\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"Streaming to Lakebase PostgreSQL...\")\n",
        "print(f\"Query ID: {query_lakebase.id}\")\n",
        "print(f\"Status: {query_lakebase.status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Monitor and Query Fraud Features\n",
        "\n",
        "Query fraud features from Lakebase for real-time insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for data to process\n",
        "import time\n",
        "print(\"Waiting 30 seconds for data to process...\")\n",
        "time.sleep(30)\n",
        "\n",
        "# Query top users by fraud score\n",
        "query_results = \"\"\"\n",
        "SELECT \n",
        "    user_id,\n",
        "    COUNT(*) as total_transactions,\n",
        "    SUM(is_fraud_prediction) as predicted_frauds,\n",
        "    AVG(fraud_score) as avg_fraud_score,\n",
        "    MAX(fraud_score) as max_fraud_score,\n",
        "    SUM(is_rapid_transaction) as rapid_transactions,\n",
        "    SUM(is_impossible_travel) as impossible_travels,\n",
        "    SUM(is_amount_anomaly) as amount_anomalies\n",
        "FROM fraud_features\n",
        "GROUP BY user_id\n",
        "ORDER BY predicted_frauds DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    result_df = pd.read_sql(query_results, conn)\n",
        "\n",
        "print(\"\\nTop 10 Users by Predicted Fraud Count:\")\n",
        "display(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query high-risk transactions\n",
        "high_risk_query = \"\"\"\n",
        "SELECT \n",
        "    transaction_id,\n",
        "    user_id,\n",
        "    timestamp,\n",
        "    amount,\n",
        "    fraud_score,\n",
        "    is_rapid_transaction,\n",
        "    is_impossible_travel,\n",
        "    is_amount_anomaly,\n",
        "    transactions_last_10min,\n",
        "    velocity_kmh\n",
        "FROM fraud_features\n",
        "WHERE fraud_score >= 50\n",
        "ORDER BY fraud_score DESC, timestamp DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    high_risk_df = pd.read_sql(high_risk_query, conn)\n",
        "\n",
        "print(\"\\nHigh-Risk Transactions (fraud_score >= 50):\")\n",
        "display(high_risk_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-time feature serving example - Get features for specific user\n",
        "def get_user_fraud_features(user_id: str):\n",
        "    \"\"\"\n",
        "    Get real-time fraud features for a user from Lakebase PostgreSQL.\n",
        "    Query latency: <10ms\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        transaction_id,\n",
        "        timestamp,\n",
        "        amount,\n",
        "        user_transaction_count,\n",
        "        transactions_last_hour,\n",
        "        transactions_last_10min,\n",
        "        fraud_score,\n",
        "        is_fraud_prediction\n",
        "    FROM fraud_features\n",
        "    WHERE user_id = %s\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    \n",
        "    with lakebase.get_connection() as conn:\n",
        "        df = pd.read_sql(query, conn, params=(user_id,))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example: Get features for a user\n",
        "sample_user = \"user_001\"\n",
        "user_features = get_user_fraud_features(sample_user)\n",
        "\n",
        "print(f\"\\nRecent transactions for {sample_user}:\")\n",
        "display(user_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Stop Streaming Queries\n",
        "\n",
        "When done, stop the streaming query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop streaming query\n",
        "if query_lakebase.isActive:\n",
        "    query_lakebase.stop()\n",
        "    print(\"Streaming query stopped\")\n",
        "\n",
        "print(\"\\nAll streaming queries stopped successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated advanced streaming fraud detection using `transformWithStateInPandas` with a **consolidated state object** - the next-generation stateful streaming API in Apache Spark 4.0+.\n",
        "\n",
        "### Key Capabilities\n",
        "\n",
        "1. **Object-Oriented Stateful Processing**: Using `StatefulProcessor` class with `transformWithStateInPandas`\n",
        "2. **Consolidated State Management**: Single `ValueState` object containing all user state (atomic updates)\n",
        "3. **Automatic TTL Eviction**: Built-in 1-hour TTL for state cleanup\n",
        "4. **Fraud Features**:\n",
        "   - Transaction velocity (counts in time windows)\n",
        "   - IP address change tracking\n",
        "   - Geographic anomalies (impossible travel detection)\n",
        "   - Amount-based anomalies (z-score, ratios)\n",
        "   - Composite fraud scores (0-100)\n",
        "5. **Real-time Serving**: Writing features to Lakebase PostgreSQL for <10ms query latency\n",
        "6. **Production Patterns**: Proper watermarking and checkpointing\n",
        "\n",
        "### Key Advantages of transformWithStateInPandas\n",
        "\n",
        "According to the [Apache Spark documentation](https://spark.apache.org/docs/latest/streaming/structured-streaming-transform-with-state.html), `transformWithStateInPandas` provides:\n",
        "\n",
        "- **Object-oriented design**: Define stateful logic using `StatefulProcessor` classes (vs function-based `applyInPandasWithState`)\n",
        "- **State variable types**: `ValueState`, `ListState`, `MapState` optimized for different operations\n",
        "- **Automatic TTL eviction**: Built-in Time-To-Live for state cleanup (1 hour in this example)\n",
        "- **Timer management**: Register, list, and delete timers for time-based processing\n",
        "- **State schema evolution**: Add/remove state variables across query runs\n",
        "- **Checkpointed timers**: Fault-tolerant timer persistence\n",
        "- **Next-generation API**: Replaces older `applyInPandasWithState` in Spark 4.0+\n",
        "\n",
        "### Consolidated State Architecture\n",
        "\n",
        "Instead of managing 6 separate state variables, this implementation uses a **single consolidated `ValueState`** for:\n",
        "\n",
        "```\n",
        "user_fraud_state (ValueState) - SINGLE STATE OBJECT\n",
        "  \u251c\u2500\u2500 transaction_count (int)\n",
        "  \u251c\u2500\u2500 last_timestamp (timestamp)\n",
        "  \u251c\u2500\u2500 last_ip_address (string)\n",
        "  \u251c\u2500\u2500 last_latitude (double)\n",
        "  \u251c\u2500\u2500 last_longitude (double)\n",
        "  \u251c\u2500\u2500 ip_change_count (int)\n",
        "  \u251c\u2500\u2500 total_amount (double)\n",
        "  \u251c\u2500\u2500 avg_amount (double)\n",
        "  \u251c\u2500\u2500 max_amount (double)\n",
        "  \u251c\u2500\u2500 recent_timestamps (array<timestamp>)  # Bounded to 50\n",
        "  \u2514\u2500\u2500 recent_amounts (array<double>)        # Bounded to 50\n",
        "```\n",
        "\n",
        "**Benefits of Consolidated State:**\n",
        "- \u2705 **Atomic updates**: All state updated in a single operation\n",
        "- \u2705 **Simplified code**: Single state variable vs 6 separate ones\n",
        "- \u2705 **Better performance**: Single read/write vs multiple operations\n",
        "- \u2705 **Easier to reason about**: State is cohesive and self-contained\n",
        "- \u2705 **Schema evolution**: Easier to add/modify fields in one place\n",
        "\n",
        "### StatefulProcessor Implementation\n",
        "\n",
        "```\n",
        "FraudDetectionFeaturesProcessor (StatefulProcessor)\n",
        "  \u251c\u2500\u2500 init() - Initialize single consolidated state\n",
        "  \u2502   \u2514\u2500\u2500 user_fraud_state (ValueState with 11 fields + arrays)\n",
        "  \u2502\n",
        "  \u251c\u2500\u2500 handleInputRows() - Process transactions\n",
        "  \u2502   \u251c\u2500\u2500 state = self.user_state.get() (single read)\n",
        "  \u2502   \u251c\u2500\u2500 Calculate fraud features\n",
        "  \u2502   \u2514\u2500\u2500 self.user_state.update((...)) (single atomic write)\n",
        "  \u2502\n",
        "  \u251c\u2500\u2500 handleExpiredTimer() - Handle timers (not used here)\n",
        "  \u2514\u2500\u2500 close() - Cleanup operations (none needed)\n",
        "```\n",
        "\n",
        "### Fraud Detection Logic\n",
        "\n",
        "**Fraud Score Calculation (0-100 points):**\n",
        "- Rapid transactions (5+ in 10 min): +20 points\n",
        "- Impossible travel (>800 km/h): +30 points  \n",
        "- Amount anomaly (z-score > 3): +25 points\n",
        "- Frequent IP changes (5+ total): +15 points\n",
        "- High velocity (10+ in 1 hour): +10 points\n",
        "\n",
        "**Fraud Prediction:** Score >= 50 triggers fraud flag\n",
        "\n",
        "### Real-time Architecture\n",
        "\n",
        "```\n",
        "Streaming Transactions\n",
        "    \u2193\n",
        "transformWithStateInPandas (grouped by user_id)\n",
        "  \u2022 StatefulProcessor with SINGLE consolidated ValueState\n",
        "  \u2022 Automatic TTL (1 hour)\n",
        "  \u2022 Bounded arrays (last 50 transactions)\n",
        "  \u2022 Atomic state updates\n",
        "    \u2193\n",
        "foreachBatch\n",
        "    \u2193\n",
        "Lakebase PostgreSQL (fraud_features table)\n",
        "    \u2193\n",
        "Real-time Queries (<10ms latency)\n",
        "```\n",
        "\n",
        "### Comparison: Multiple vs Consolidated State\n",
        "\n",
        "**OLD (6 separate state variables):**\n",
        "```python\n",
        "# Multiple reads\n",
        "if self.transaction_count.exists():\n",
        "    prev_count = self.transaction_count.get()[0]\n",
        "if self.last_transaction.exists():\n",
        "    last_txn = self.last_transaction.get()\n",
        "    prev_last_time = last_txn[0]\n",
        "# ... 4 more state variable reads\n",
        "\n",
        "# Multiple writes\n",
        "self.transaction_count.update((prev_count,))\n",
        "self.last_transaction.update((prev_last_time, prev_ip, prev_lat, prev_lon))\n",
        "self.ip_change_count.update((prev_ip_changes,))\n",
        "# ... 3 more state variable writes\n",
        "```\n",
        "\n",
        "**NEW (1 consolidated state object):**\n",
        "```python\n",
        "# Single read\n",
        "if self.user_state.exists():\n",
        "    state = self.user_state.get()\n",
        "    prev_count = state[0]\n",
        "    prev_last_time = state[1]\n",
        "    prev_ip = state[2]\n",
        "    # ... all fields in one read\n",
        "\n",
        "# Single atomic write\n",
        "self.user_state.update((\n",
        "    prev_count, prev_last_time, prev_ip, prev_lat, prev_lon,\n",
        "    prev_ip_changes, prev_total_amount, prev_avg_amount, \n",
        "    prev_max_amount, prev_times, prev_amounts\n",
        "))\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Integrate with ML models for enhanced fraud scoring\n",
        "- Add timer-based processing using `handleExpiredTimer()`\n",
        "- Implement state schema evolution for adding new fields\n",
        "- Connect to downstream systems (dashboards, notification services)\n",
        "- Tune TTL duration and processing trigger intervals\n",
        "- Add more sophisticated fraud detection rules (device fingerprinting, network analysis)\n",
        "- Implement A/B testing for fraud detection thresholds\n",
        "- Explore `MapState` for multi-level hierarchical state\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}