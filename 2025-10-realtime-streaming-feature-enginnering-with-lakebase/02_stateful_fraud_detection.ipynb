{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stateful Fraud Detection with applyInPandasWithState\n",
        "\n",
        "This notebook demonstrates **advanced streaming fraud detection** using PySpark's `applyInPandasWithState` API.\n",
        "\n",
        "## What is applyInPandasWithState?\n",
        "\n",
        "`applyInPandasWithState` is a powerful Structured Streaming API that enables:\n",
        "- **Stateful processing**: Maintain state across micro-batches per key (e.g., user_id, IP address)\n",
        "- **Pandas UDFs**: Process data using familiar Pandas operations\n",
        "- **Complex logic**: Implement sophisticated fraud detection rules with historical context\n",
        "- **Bounded state**: Automatic state cleanup with timeout management\n",
        "\n",
        "## Fraud Detection Features\n",
        "\n",
        "This notebook calculates real-time fraud indicators based on:\n",
        "\n",
        "1. **Transaction Velocity**: Count of transactions in time window\n",
        "2. **IP Address Changes**: Frequency of IP changes per user\n",
        "3. **Location Anomalies**: Geographic distance from previous transaction\n",
        "4. **Amount Patterns**: Statistical anomalies in transaction amounts\n",
        "5. **Time-based Patterns**: Unusual transaction timing\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Streaming Source (Rate/Kafka)\n",
        "    ↓\n",
        "Feature Generation (TransactionDataGenerator)\n",
        "    ↓\n",
        "Group by Key (user_id)\n",
        "    ↓\n",
        "applyInPandasWithState\n",
        "  • Maintain transaction history per user\n",
        "  • Calculate velocity features\n",
        "  • Detect location anomalies\n",
        "  • Track IP changes\n",
        "  • Compute fraud scores\n",
        "    ↓\n",
        "Write to Lakebase PostgreSQL (foreachBatch)\n",
        "    ↓\n",
        "Real-time Feature Serving (<10ms latency)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Run `00_setup.ipynb` first to provision Lakebase PostgreSQL\n",
        "- Databricks Runtime 13.0+ (for applyInPandasWithState support)\n",
        "- Lakebase PostgreSQL instance configured and accessible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.streaming.state import GroupStateTimeout\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Iterator, Tuple\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utility modules\n",
        "from utils.data_generator import TransactionDataGenerator\n",
        "from utils.lakebase_client import LakebaseClient\n",
        "\n",
        "# Initialize data generator\n",
        "data_gen = TransactionDataGenerator(spark)\n",
        "\n",
        "print(\"Utility modules loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Lakebase connection\n",
        "LAKEBASE_CONFIG = {\n",
        "    \"instance_name\": \"neha-lakebase-demo\",\n",
        "    \"database\": \"databricks_postgres\"\n",
        "}\n",
        "\n",
        "# Initialize Lakebase client\n",
        "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
        "\n",
        "# Test connection\n",
        "if lakebase.test_connection():\n",
        "    print(\"Connected to Lakebase PostgreSQL\")\n",
        "else:\n",
        "    raise Exception(\"Failed to connect to Lakebase\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Streaming Transaction Data\n",
        "\n",
        "Generate synthetic transaction data with fraud indicators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate streaming transaction data\n",
        "df_transactions = data_gen.generate_transaction_data(\n",
        "    rows_per_second=10,\n",
        "    num_users=100,\n",
        "    fraud_ratio=0.1\n",
        ")\n",
        "\n",
        "print(\"Schema of streaming transactions:\")\n",
        "df_transactions.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define State and Output Schemas\n",
        "\n",
        "Define the structure for maintaining state across streaming batches and the output schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define state schema - what we track for each user\n",
        "state_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"transaction_count\", IntegerType(), False),\n",
        "    StructField(\"last_transaction_time\", TimestampType(), False),\n",
        "    StructField(\"last_ip_address\", StringType(), True),\n",
        "    StructField(\"last_latitude\", DoubleType(), True),\n",
        "    StructField(\"last_longitude\", DoubleType(), True),\n",
        "    StructField(\"ip_change_count\", IntegerType(), False),\n",
        "    StructField(\"total_amount\", DoubleType(), False),\n",
        "    StructField(\"avg_amount\", DoubleType(), False),\n",
        "    StructField(\"max_amount\", DoubleType(), False),\n",
        "    StructField(\"transaction_times\", ArrayType(TimestampType()), False),\n",
        "    StructField(\"recent_amounts\", ArrayType(DoubleType()), False)\n",
        "])\n",
        "\n",
        "# Define output schema - fraud features per transaction\n",
        "output_schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), False),\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"amount\", DoubleType(), False),\n",
        "    StructField(\"merchant_id\", StringType(), False),\n",
        "    StructField(\"ip_address\", StringType(), False),\n",
        "    StructField(\"latitude\", DoubleType(), False),\n",
        "    StructField(\"longitude\", DoubleType(), False),\n",
        "    \n",
        "    # Fraud detection features\n",
        "    StructField(\"user_transaction_count\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_hour\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_10min\", IntegerType(), False),\n",
        "    StructField(\"ip_changed\", IntegerType(), False),\n",
        "    StructField(\"ip_change_count_total\", IntegerType(), False),\n",
        "    StructField(\"distance_from_last_km\", DoubleType(), True),\n",
        "    StructField(\"velocity_kmh\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_avg_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_max_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_zscore\", DoubleType(), True),\n",
        "    StructField(\"seconds_since_last_transaction\", DoubleType(), True),\n",
        "    StructField(\"is_rapid_transaction\", IntegerType(), False),\n",
        "    StructField(\"is_impossible_travel\", IntegerType(), False),\n",
        "    StructField(\"is_amount_anomaly\", IntegerType(), False),\n",
        "    StructField(\"fraud_score\", DoubleType(), False),\n",
        "    StructField(\"is_fraud_prediction\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "print(\"State and output schemas defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implement Stateful Fraud Detection Function\n",
        "\n",
        "This function processes each user's transactions with maintained state across micro-batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate distance between two geographic points in kilometers.\n",
        "    \"\"\"\n",
        "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
        "        return None\n",
        "    \n",
        "    R = 6371.0  # Earth radius in kilometers\n",
        "    \n",
        "    # Convert to radians\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    \n",
        "    # Haversine formula\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    \n",
        "    return R * c\n",
        "\n",
        "print(\"Distance calculation helper function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_fraud(\n",
        "    key: Tuple[str],\n",
        "    pdf_iter: Iterator[pd.DataFrame],\n",
        "    state: GroupState\n",
        ") -> Iterator[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Stateful fraud detection function using applyInPandasWithState.\n",
        "    \n",
        "    Args:\n",
        "        key: Tuple containing the grouping key (user_id)\n",
        "        pdf_iter: Iterator of Pandas DataFrames for this key\n",
        "        state: GroupState object to maintain state across batches\n",
        "    \n",
        "    Yields:\n",
        "        pd.DataFrame: Enriched transactions with fraud features\n",
        "    \"\"\"\n",
        "    user_id = key[0]\n",
        "    \n",
        "    # Process each micro-batch for this user\n",
        "    for pdf in pdf_iter:\n",
        "        if pdf.empty:\n",
        "            continue\n",
        "        \n",
        "        # Sort by timestamp\n",
        "        pdf = pdf.sort_values('timestamp')\n",
        "        \n",
        "        # Initialize or retrieve state\n",
        "        if state.exists:\n",
        "            state_row = state.get\n",
        "            prev_count = state_row['transaction_count']\n",
        "            prev_last_time = state_row['last_transaction_time']\n",
        "            prev_ip = state_row['last_ip_address']\n",
        "            prev_lat = state_row['last_latitude']\n",
        "            prev_lon = state_row['last_longitude']\n",
        "            prev_ip_changes = state_row['ip_change_count']\n",
        "            prev_total_amount = state_row['total_amount']\n",
        "            prev_avg_amount = state_row['avg_amount']\n",
        "            prev_max_amount = state_row['max_amount']\n",
        "            prev_times = state_row['transaction_times']\n",
        "            prev_amounts = state_row['recent_amounts']\n",
        "        else:\n",
        "            prev_count = 0\n",
        "            prev_last_time = None\n",
        "            prev_ip = None\n",
        "            prev_lat = None\n",
        "            prev_lon = None\n",
        "            prev_ip_changes = 0\n",
        "            prev_total_amount = 0.0\n",
        "            prev_avg_amount = 0.0\n",
        "            prev_max_amount = 0.0\n",
        "            prev_times = []\n",
        "            prev_amounts = []\n",
        "        \n",
        "        # Initialize output columns\n",
        "        results = []\n",
        "        \n",
        "        # Process each transaction\n",
        "        for idx, row in pdf.iterrows():\n",
        "            current_time = row['timestamp']\n",
        "            current_ip = row['ip_address']\n",
        "            current_lat = row['latitude']\n",
        "            current_lon = row['longitude']\n",
        "            current_amount = row['amount']\n",
        "            \n",
        "            # Update transaction count\n",
        "            prev_count += 1\n",
        "            \n",
        "            # Calculate time-based features\n",
        "            if prev_last_time is not None:\n",
        "                time_diff = (current_time - prev_last_time).total_seconds()\n",
        "            else:\n",
        "                time_diff = None\n",
        "            \n",
        "            # IP change detection\n",
        "            ip_changed = 0\n",
        "            if prev_ip is not None and current_ip != prev_ip:\n",
        "                ip_changed = 1\n",
        "                prev_ip_changes += 1\n",
        "            \n",
        "            # Geographic distance calculation\n",
        "            distance_km = None\n",
        "            velocity_kmh = None\n",
        "            if prev_lat is not None and prev_lon is not None:\n",
        "                distance_km = calculate_haversine_distance(\n",
        "                    prev_lat, prev_lon, current_lat, current_lon\n",
        "                )\n",
        "                if distance_km is not None and time_diff is not None and time_diff > 0:\n",
        "                    velocity_kmh = (distance_km / time_diff) * 3600  # km/h\n",
        "            \n",
        "            # Amount-based features\n",
        "            prev_total_amount += current_amount\n",
        "            prev_avg_amount = prev_total_amount / prev_count\n",
        "            prev_max_amount = max(prev_max_amount, current_amount)\n",
        "            \n",
        "            amount_vs_avg_ratio = current_amount / prev_avg_amount if prev_avg_amount > 0 else 1.0\n",
        "            amount_vs_max_ratio = current_amount / prev_max_amount if prev_max_amount > 0 else 1.0\n",
        "            \n",
        "            # Z-score calculation for amount\n",
        "            amount_zscore = None\n",
        "            if len(prev_amounts) >= 3:\n",
        "                amounts_std = np.std(prev_amounts)\n",
        "                if amounts_std > 0:\n",
        "                    amount_zscore = (current_amount - prev_avg_amount) / amounts_std\n",
        "            \n",
        "            # Update recent transactions list (keep last 50)\n",
        "            prev_times.append(current_time)\n",
        "            prev_amounts.append(current_amount)\n",
        "            if len(prev_times) > 50:\n",
        "                prev_times = prev_times[-50:]\n",
        "                prev_amounts = prev_amounts[-50:]\n",
        "            \n",
        "            # Count transactions in time windows\n",
        "            one_hour_ago = current_time - timedelta(hours=1)\n",
        "            ten_min_ago = current_time - timedelta(minutes=10)\n",
        "            \n",
        "            trans_last_hour = sum(1 for t in prev_times if t >= one_hour_ago)\n",
        "            trans_last_10min = sum(1 for t in prev_times if t >= ten_min_ago)\n",
        "            \n",
        "            # Fraud indicators\n",
        "            is_rapid = 1 if trans_last_10min >= 5 else 0\n",
        "            is_impossible_travel = 1 if velocity_kmh is not None and velocity_kmh > 800 else 0\n",
        "            is_amount_anomaly = 1 if amount_zscore is not None and abs(amount_zscore) > 3 else 0\n",
        "            \n",
        "            # Calculate fraud score (0-100)\n",
        "            fraud_score = 0.0\n",
        "            if is_rapid:\n",
        "                fraud_score += 20\n",
        "            if is_impossible_travel:\n",
        "                fraud_score += 30\n",
        "            if is_amount_anomaly:\n",
        "                fraud_score += 25\n",
        "            if prev_ip_changes >= 5:\n",
        "                fraud_score += 15\n",
        "            if trans_last_hour >= 10:\n",
        "                fraud_score += 10\n",
        "            fraud_score = min(fraud_score, 100.0)\n",
        "            \n",
        "            # Fraud prediction (threshold at 50)\n",
        "            is_fraud_pred = 1 if fraud_score >= 50 else 0\n",
        "            \n",
        "            # Append result\n",
        "            results.append({\n",
        "                'transaction_id': row['transaction_id'],\n",
        "                'user_id': user_id,\n",
        "                'timestamp': current_time,\n",
        "                'amount': current_amount,\n",
        "                'merchant_id': row['merchant_id'],\n",
        "                'ip_address': current_ip,\n",
        "                'latitude': current_lat,\n",
        "                'longitude': current_lon,\n",
        "                'user_transaction_count': prev_count,\n",
        "                'transactions_last_hour': trans_last_hour,\n",
        "                'transactions_last_10min': trans_last_10min,\n",
        "                'ip_changed': ip_changed,\n",
        "                'ip_change_count_total': prev_ip_changes,\n",
        "                'distance_from_last_km': distance_km,\n",
        "                'velocity_kmh': velocity_kmh,\n",
        "                'amount_vs_user_avg_ratio': amount_vs_avg_ratio,\n",
        "                'amount_vs_user_max_ratio': amount_vs_max_ratio,\n",
        "                'amount_zscore': amount_zscore,\n",
        "                'seconds_since_last_transaction': time_diff,\n",
        "                'is_rapid_transaction': is_rapid,\n",
        "                'is_impossible_travel': is_impossible_travel,\n",
        "                'is_amount_anomaly': is_amount_anomaly,\n",
        "                'fraud_score': fraud_score,\n",
        "                'is_fraud_prediction': is_fraud_pred\n",
        "            })\n",
        "            \n",
        "            # Update state for next transaction\n",
        "            prev_last_time = current_time\n",
        "            prev_ip = current_ip\n",
        "            prev_lat = current_lat\n",
        "            prev_lon = current_lon\n",
        "        \n",
        "        # Update state\n",
        "        new_state = pd.Series({\n",
        "            'user_id': user_id,\n",
        "            'transaction_count': prev_count,\n",
        "            'last_transaction_time': prev_last_time,\n",
        "            'last_ip_address': prev_ip,\n",
        "            'last_latitude': prev_lat,\n",
        "            'last_longitude': prev_lon,\n",
        "            'ip_change_count': prev_ip_changes,\n",
        "            'total_amount': prev_total_amount,\n",
        "            'avg_amount': prev_avg_amount,\n",
        "            'max_amount': prev_max_amount,\n",
        "            'transaction_times': prev_times,\n",
        "            'recent_amounts': prev_amounts\n",
        "        })\n",
        "        state.update(new_state)\n",
        "        state.setTimeoutDuration(\"1 hour\")\n",
        "        \n",
        "        # Yield results\n",
        "        if results:\n",
        "            yield pd.DataFrame(results)\n",
        "\n",
        "print(\"Fraud detection function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply Stateful Processing\n",
        "\n",
        "Apply the fraud detection function to the streaming data using `applyInPandasWithState`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply stateful fraud detection\n",
        "df_with_fraud_features = df_transactions \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .applyInPandasWithState(\n",
        "        detect_fraud,\n",
        "        output_schema,\n",
        "        state_schema,\n",
        "        \"append\",\n",
        "        GroupStateTimeout.ProcessingTimeTimeout\n",
        "    )\n",
        "\n",
        "print(\"Stateful processing configured\")\n",
        "print(\"\\nOutput schema:\")\n",
        "df_with_fraud_features.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Fraud Features Table in Lakebase\n",
        "\n",
        "Create the PostgreSQL table to store fraud detection features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fraud features table in Lakebase\n",
        "create_table_sql = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS fraud_features (\n",
        "    transaction_id VARCHAR(100) PRIMARY KEY,\n",
        "    user_id VARCHAR(100) NOT NULL,\n",
        "    timestamp TIMESTAMP NOT NULL,\n",
        "    amount DOUBLE PRECISION NOT NULL,\n",
        "    merchant_id VARCHAR(100),\n",
        "    ip_address VARCHAR(50),\n",
        "    latitude DOUBLE PRECISION,\n",
        "    longitude DOUBLE PRECISION,\n",
        "    \n",
        "    -- Velocity features\n",
        "    user_transaction_count INTEGER,\n",
        "    transactions_last_hour INTEGER,\n",
        "    transactions_last_10min INTEGER,\n",
        "    \n",
        "    -- IP features\n",
        "    ip_changed INTEGER,\n",
        "    ip_change_count_total INTEGER,\n",
        "    \n",
        "    -- Location features\n",
        "    distance_from_last_km DOUBLE PRECISION,\n",
        "    velocity_kmh DOUBLE PRECISION,\n",
        "    \n",
        "    -- Amount features\n",
        "    amount_vs_user_avg_ratio DOUBLE PRECISION,\n",
        "    amount_vs_user_max_ratio DOUBLE PRECISION,\n",
        "    amount_zscore DOUBLE PRECISION,\n",
        "    \n",
        "    -- Time features\n",
        "    seconds_since_last_transaction DOUBLE PRECISION,\n",
        "    \n",
        "    -- Fraud indicators\n",
        "    is_rapid_transaction INTEGER,\n",
        "    is_impossible_travel INTEGER,\n",
        "    is_amount_anomaly INTEGER,\n",
        "    fraud_score DOUBLE PRECISION,\n",
        "    is_fraud_prediction INTEGER,\n",
        "    \n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\n",
        "-- Indexes for fast queries\n",
        "CREATE INDEX IF NOT EXISTS idx_fraud_user_id ON fraud_features(user_id);\n",
        "CREATE INDEX IF NOT EXISTS idx_fraud_timestamp ON fraud_features(timestamp DESC);\n",
        "CREATE INDEX IF NOT EXISTS idx_fraud_score ON fraud_features(fraud_score DESC);\n",
        "CREATE INDEX IF NOT EXISTS idx_fraud_prediction ON fraud_features(is_fraud_prediction);\n",
        "\"\"\"\n",
        "\n",
        "# Execute table creation\n",
        "with lakebase.get_connection() as conn:\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(create_table_sql)\n",
        "    conn.commit()\n",
        "\n",
        "print(\"fraud_features table created in Lakebase PostgreSQL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Write Fraud Features to Lakebase\n",
        "\n",
        "Stream fraud features to Lakebase PostgreSQL for real-time serving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define foreachBatch function to write to Lakebase\n",
        "def write_to_lakebase(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Write each micro-batch to Lakebase PostgreSQL.\n",
        "    \"\"\"\n",
        "    if batch_df.isEmpty():\n",
        "        return\n",
        "    \n",
        "    logger.info(f\"Processing batch {batch_id} with {batch_df.count()} rows\")\n",
        "    \n",
        "    # Write to Lakebase using client\n",
        "    lakebase.write_streaming_batch(batch_df, \"fraud_features\")\n",
        "    \n",
        "    logger.info(f\"Batch {batch_id} written to Lakebase\")\n",
        "\n",
        "\n",
        "# Start streaming query to Lakebase\n",
        "query_lakebase = df_with_fraud_features \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .foreachBatch(write_to_lakebase) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/fraud_detection_checkpoint\") \\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"Streaming to Lakebase PostgreSQL...\")\n",
        "print(f\"Query ID: {query_lakebase.id}\")\n",
        "print(f\"Status: {query_lakebase.status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Monitor and Query Fraud Features\n",
        "\n",
        "Query fraud features from Lakebase for real-time insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for data to process\n",
        "import time\n",
        "print(\"Waiting 30 seconds for data to process...\")\n",
        "time.sleep(30)\n",
        "\n",
        "# Query top users by fraud score\n",
        "query_results = \"\"\"\n",
        "SELECT \n",
        "    user_id,\n",
        "    COUNT(*) as total_transactions,\n",
        "    SUM(is_fraud_prediction) as predicted_frauds,\n",
        "    AVG(fraud_score) as avg_fraud_score,\n",
        "    MAX(fraud_score) as max_fraud_score,\n",
        "    SUM(is_rapid_transaction) as rapid_transactions,\n",
        "    SUM(is_impossible_travel) as impossible_travels,\n",
        "    SUM(is_amount_anomaly) as amount_anomalies\n",
        "FROM fraud_features\n",
        "GROUP BY user_id\n",
        "ORDER BY predicted_frauds DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    result_df = pd.read_sql(query_results, conn)\n",
        "\n",
        "print(\"\\nTop 10 Users by Predicted Fraud Count:\")\n",
        "display(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query high-risk transactions\n",
        "high_risk_query = \"\"\"\n",
        "SELECT \n",
        "    transaction_id,\n",
        "    user_id,\n",
        "    timestamp,\n",
        "    amount,\n",
        "    fraud_score,\n",
        "    is_rapid_transaction,\n",
        "    is_impossible_travel,\n",
        "    is_amount_anomaly,\n",
        "    transactions_last_10min,\n",
        "    velocity_kmh\n",
        "FROM fraud_features\n",
        "WHERE fraud_score >= 50\n",
        "ORDER BY fraud_score DESC, timestamp DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    high_risk_df = pd.read_sql(high_risk_query, conn)\n",
        "\n",
        "print(\"\\nHigh-Risk Transactions (fraud_score >= 50):\")\n",
        "display(high_risk_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-time feature serving example - Get features for specific user\n",
        "def get_user_fraud_features(user_id: str):\n",
        "    \"\"\"\n",
        "    Get real-time fraud features for a user from Lakebase PostgreSQL.\n",
        "    Query latency: <10ms\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        transaction_id,\n",
        "        timestamp,\n",
        "        amount,\n",
        "        user_transaction_count,\n",
        "        transactions_last_hour,\n",
        "        transactions_last_10min,\n",
        "        fraud_score,\n",
        "        is_fraud_prediction\n",
        "    FROM fraud_features\n",
        "    WHERE user_id = %s\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "    \n",
        "    with lakebase.get_connection() as conn:\n",
        "        df = pd.read_sql(query, conn, params=(user_id,))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example: Get features for a user\n",
        "sample_user = \"user_001\"\n",
        "user_features = get_user_fraud_features(sample_user)\n",
        "\n",
        "print(f\"\\nRecent transactions for {sample_user}:\")\n",
        "display(user_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Stop Streaming Queries\n",
        "\n",
        "When done, stop the streaming query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop streaming query\n",
        "if query_lakebase.isActive:\n",
        "    query_lakebase.stop()\n",
        "    print(\"Streaming query stopped\")\n",
        "\n",
        "print(\"\\nAll streaming queries stopped successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated advanced streaming fraud detection using `applyInPandasWithState`:\n",
        "\n",
        "### Key Capabilities\n",
        "\n",
        "1. **Stateful Streaming**: Using `applyInPandasWithState` for complex fraud detection logic\n",
        "2. **State Management**: Maintaining user transaction history across micro-batches\n",
        "3. **Fraud Features**:\n",
        "   - Transaction velocity (counts in time windows)\n",
        "   - IP address change tracking\n",
        "   - Geographic anomalies (impossible travel detection)\n",
        "   - Amount-based anomalies (z-score, ratios)\n",
        "   - Composite fraud scores (0-100)\n",
        "4. **Real-time Serving**: Writing features to Lakebase PostgreSQL for <10ms query latency\n",
        "5. **Production Patterns**: Proper state timeout, watermarking, and checkpointing\n",
        "\n",
        "### Key Benefits of applyInPandasWithState\n",
        "\n",
        "- **Stateful**: Maintain context across streaming batches per user\n",
        "- **Flexible**: Implement any logic using Python/Pandas\n",
        "- **Scalable**: Parallel processing per partition key\n",
        "- **Bounded**: Automatic state cleanup with timeouts (1 hour in this example)\n",
        "- **Fault-tolerant**: State stored in checkpoints\n",
        "\n",
        "### Fraud Detection Logic\n",
        "\n",
        "**Fraud Score Calculation (0-100 points):**\n",
        "- Rapid transactions (5+ in 10 min): +20 points\n",
        "- Impossible travel (>800 km/h): +30 points  \n",
        "- Amount anomaly (z-score > 3): +25 points\n",
        "- Frequent IP changes (5+ total): +15 points\n",
        "- High velocity (10+ in 1 hour): +10 points\n",
        "\n",
        "**Fraud Prediction:** Score >= 50 triggers fraud flag\n",
        "\n",
        "### Real-time Architecture\n",
        "\n",
        "```\n",
        "Streaming Transactions\n",
        "    ↓\n",
        "applyInPandasWithState (per user_id)\n",
        "  • Track transaction history\n",
        "  • Calculate velocity features\n",
        "  • Detect location anomalies\n",
        "  • Monitor IP changes\n",
        "  • Compute fraud scores\n",
        "    ↓\n",
        "Lakebase PostgreSQL (foreachBatch)\n",
        "    ↓\n",
        "Real-time Queries (<10ms latency)\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Integrate with ML models for enhanced fraud scoring\n",
        "- Add alerting for high-risk transactions\n",
        "- Connect to downstream systems (dashboards, notification services)\n",
        "- Tune state timeout and processing trigger intervals\n",
        "- Add more sophisticated fraud detection rules (device fingerprinting, network analysis)\n",
        "- Implement A/B testing for fraud detection thresholds\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
