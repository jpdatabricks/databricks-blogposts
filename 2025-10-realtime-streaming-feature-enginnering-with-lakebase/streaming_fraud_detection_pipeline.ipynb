{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de73176-34e7-4145-8bf6-fc373b28fa8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Streaming Fraud Detection Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end streaming fraud detection pipeline combining:\n",
    "- **Stateless features**: Time, amount, merchant, device, network (from `AdvancedFeatureEngineering`)\n",
    "- **Stateful features**: Transaction velocity, IP tracking, location anomalies (from `FraudDetectionFeaturesProcessor`)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Streaming Source (Rate)\n",
    "    ↓\n",
    "Generate Transactions (TransactionDataGenerator)\n",
    "    ↓\n",
    "Apply Stateless Features (AdvancedFeatureEngineering)\n",
    "    ↓\n",
    "Apply Stateful Fraud Detection (transformWithStateInPandas)\n",
    "    ↓\n",
    "Write to Lakebase PostgreSQL (foreachBatch)\n",
    "    ↓\n",
    "Real-Time Feature Serving (<10ms query latency)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Run `00_setup.ipynb` to create feature tables\n",
    "- Databricks Runtime 17.3+ with Spark 4.0+\n",
    "- Lakebase PostgreSQL instance provisioned\n",
    "\n",
    "## Features Generated\n",
    "\n",
    "**Stateless (~40 columns):**\n",
    "- Time-based: hour, day, business hours, cyclical encodings\n",
    "- Amount-based: log, sqrt, categories, z-scores\n",
    "- Merchant: risk scores\n",
    "- Location, Device, Network: risk indicators\n",
    "\n",
    "**Stateful (~25 columns):**\n",
    "- Velocity: transaction counts in time windows\n",
    "- IP tracking: IP change detection\n",
    "- Location anomalies: impossible travel detection\n",
    "- Amount anomalies: statistical outliers\n",
    "- Fraud scoring: composite 0-100 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5925a17c-4c62-4400-a114-393d04803780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully\nSpark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Import utility modules\n",
    "from utils.data_generator import TransactionDataGenerator\n",
    "from utils.feature_engineering import (\n",
    "    AdvancedFeatureEngineering, \n",
    "    FraudDetectionFeaturesProcessor,\n",
    "    get_fraud_detection_output_schema\n",
    ")\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"All modules imported successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea342fd-087f-4418-91dc-55751a4da71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Configure Lakebase connection and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fabf4d6-fea9-435c-b92c-2bce2217bae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Lakebase connection test successful\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Lakebase PostgreSQL\n\nVerifying fraud_features table...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.lakebase_client:Table stats: 0 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Table exists with 0 rows\n"
     ]
    }
   ],
   "source": [
    "# Lakebase connection configuration\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"rtm-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\"\n",
    "}\n",
    "\n",
    "# Initialize components\n",
    "data_gen = TransactionDataGenerator(spark)\n",
    "feature_engineer = AdvancedFeatureEngineering(spark)\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Test Lakebase connection\n",
    "if lakebase.test_connection():\n",
    "    print(\"Connected to Lakebase PostgreSQL\")\n",
    "else:\n",
    "    raise Exception(\"Failed to connect to Lakebase\")\n",
    "\n",
    "# Verify fraud_features table exists\n",
    "print(\"\\nVerifying fraud_features table...\")\n",
    "try:\n",
    "    stats = lakebase.get_table_stats(\"fraud_features\")\n",
    "    print(f\"  Table exists with {stats['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table not found. Creating it now...\")\n",
    "    lakebase.create_feature_table(\"fraud_features\")\n",
    "    print(\"  Table created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af261522-f8a3-446d-95c3-8ab010e4074c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Generate Streaming Transaction Data\n",
    "\n",
    "Create a streaming source that continuously generates synthetic transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a42abb-4b98-4730-b5c9-e8388343372a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data_generator:Creating streaming transaction source...\nINFO:utils.data_generator:   Rate: 10 transactions/second\nINFO:utils.data_generator:   Users: 50, Merchants: 100\nINFO:utils.data_generator:Streaming source created successfully\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data source created\n\nTransaction schema:\nroot\n |-- timestamp: timestamp (nullable = true)\n |-- transaction_id: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- merchant_id: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- currency: string (nullable = false)\n |-- merchant_category: string (nullable = false)\n |-- payment_method: string (nullable = false)\n |-- ip_address: string (nullable = true)\n |-- device_id: string (nullable = true)\n |-- location_lat: double (nullable = false)\n |-- location_lon: double (nullable = false)\n |-- card_type: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# Generate streaming transaction data\n",
    "df_transactions = data_gen.generate_transaction_data(\n",
    "    num_users=50,           # 50 unique users\n",
    "    num_merchants=100,      # 100 unique merchants\n",
    "    rows_per_second=10      # 10 transactions per second\n",
    ")\n",
    "\n",
    "print(\"Streaming data source created\")\n",
    "print(\"\\nTransaction schema:\")\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228c0478-8d9e-494c-9f06-2d59f20ce5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Apply Stateless Features\n",
    "\n",
    "Apply time-based, amount-based, merchant, location, device, and network features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f35d8c-696a-458a-9a82-17c350209007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.feature_engineering:Applying streaming-compatible feature engineering...\nINFO:utils.feature_engineering:Creating time-based features...\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:utils.feature_engineering:Creating amount-based features...\nINFO:utils.feature_engineering:Creating merchant features (streaming-only)...\nINFO:utils.feature_engineering:Creating location features (streaming-only)...\nINFO:utils.feature_engineering:Creating device features (streaming-only)...\nINFO:utils.feature_engineering:Creating network features (streaming-only)...\nINFO:utils.feature_engineering:Streaming feature engineering completed!\n"
     ]
    }
   ],
   "source": [
    "df_with_stateless_features = feature_engineer.apply_all_features(df_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccb1c16-224a-4efc-be22-d98df03be063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Apply Stateful Fraud Detection\n",
    "\n",
    "Use `transformWithStateInPandas` to maintain per-user state and detect fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f0fef-d09f-4c9a-8222-d45cd47ef79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stateful fraud detection configured\n\nFinal schema with all features:\nroot\n |-- transaction_id: string (nullable = false)\n |-- user_id: string (nullable = false)\n |-- timestamp: timestamp (nullable = false)\n |-- amount: double (nullable = false)\n |-- merchant_id: string (nullable = false)\n |-- ip_address: string (nullable = false)\n |-- latitude: double (nullable = false)\n |-- longitude: double (nullable = false)\n |-- user_transaction_count: integer (nullable = false)\n |-- transactions_last_hour: integer (nullable = false)\n |-- transactions_last_10min: integer (nullable = false)\n |-- ip_changed: integer (nullable = false)\n |-- ip_change_count_total: integer (nullable = false)\n |-- distance_from_last_km: double (nullable = true)\n |-- velocity_kmh: double (nullable = true)\n |-- amount_vs_user_avg_ratio: double (nullable = true)\n |-- amount_vs_user_max_ratio: double (nullable = true)\n |-- amount_zscore: double (nullable = true)\n |-- seconds_since_last_transaction: double (nullable = true)\n |-- is_rapid_transaction: integer (nullable = false)\n |-- is_impossible_travel: integer (nullable = false)\n |-- is_amount_anomaly: integer (nullable = false)\n |-- fraud_score: double (nullable = false)\n |-- is_fraud_prediction: integer (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# Apply stateful fraud detection using transformWithStateInPandas\n",
    "df_with_fraud_features = df_with_stateless_features \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .transformWithStateInPandas(\n",
    "        statefulProcessor=FraudDetectionFeaturesProcessor(),\n",
    "        outputStructType=get_fraud_detection_output_schema(),\n",
    "        outputMode=\"Append\",\n",
    "        timeMode=\"processingTime\"\n",
    "    )\n",
    "\n",
    "print(\"Stateful fraud detection configured\")\n",
    "print(\"\\nFinal schema with all features:\")\n",
    "df_with_fraud_features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4005cb7d-5a33-4066-9ed6-664bac2f7787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Write to Lakebase PostgreSQL\n",
    "\n",
    "Stream all features to Lakebase for real-time serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149557d7-1590-4592-82e3-0f9bcb628195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming pipeline started!\nQuery ID: 3f87cc9d-4be3-45ee-bafc-864e3f9d2481\nStatus: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n\nPipeline: Transactions → Stateless Features → Stateful Fraud Detection → Lakebase\n"
     ]
    }
   ],
   "source": [
    "# Define foreachBatch function\n",
    "def write_to_lakebase(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Processing batch {batch_id} with {batch_df.count()} rows\")\n",
    "    lakebase.write_streaming_batch(batch_df, batch_id, \"fraud_features\")\n",
    "    logger.info(f\"Batch {batch_id} written to Lakebase\")\n",
    "\n",
    "# Start streaming query\n",
    "query = df_with_fraud_features \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_lakebase) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/fraud_pipeline_checkpoint\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming pipeline started!\")\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Status: {query.status}\")\n",
    "print(\"\\nPipeline: Transactions → Stateless Features → Stateful Fraud Detection → Lakebase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7697731-594b-4c1c-8d8d-a95dda31cab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Monitor and Query Results\n",
    "\n",
    "Query fraud features from Lakebase for real-time insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744fc119-5fdf-4331-84f9-b7154debe40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds for data to process...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Python Server ready to receive messages\nINFO:py4j.clientserver:Received command c on object id p2\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 644, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 173, in call\n    raise e\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 170, in call\n    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n  File \"/root/.ipykernel/15828/command-4593315501084475-2799795489\", line 3, in write_to_lakebase\n    if batch_df.isEmpty():\n       ^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 47, in wrapper\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/classic/dataframe.py\", line 302, in isEmpty\n    return self._jdf.isEmpty()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1362, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 310, in deco\n    raise converted from None\n  File \"/databricks/spark/python/pyspark/worker.py\", line 3283, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 3275, in process\n    serializer.dump_stream(out_iter, outfile)\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 1772, in dump_stream\n    super().dump_stream(flatten_iterator(), stream)\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 837, in dump_stream\n    return ArrowStreamSerializer.dump_stream(\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 131, in dump_stream\n    for batch in iterator:\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 830, in init_stream_yield_batches\n    for series in iterator:\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 1769, in flatten_iterator\n    for pdf in iter_pdf:\n    ^^^^^^^^^^^^^^^^^\n  File \"/Workspace/Users/jay.palaniappan@databricks.com/databricks-blogposts/2025-10-realtime-streaming-feature-enginnering-with-lakebase/utils/feature_engineering.py\", line 653, in handleInputRows\n    trans_last_hour = sum(1 for t in prev_times if t >= one_hour_ago)\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 310, in wrapped\n    return f(*args, **kwargs)\n    ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/functions/builtin.py\", line 1638, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n      ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/functions/builtin.py\", line 129, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n      ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/functions/builtin.py\", line 129, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n      ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/classic/column.py\", line 77, in _to_java_column\n    raise PySparkTypeError(\n    ^^^^^^^^^^^^^^^^^\npyspark.errors.exceptions.captured.PythonException: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 3283, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 3275, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 1772, in dump_stream\n    super().dump_stream(flatten_iterator(), stream)\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 837, in dump_stream\n    return ArrowStreamSerializer.dump_stream(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 131, in dump_stream\n    for batch in iterator:\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 830, in init_stream_yield_batches\n    for series in iterator:\n  File \"/databricks/spark/python/pyspark/sql/pandas/serializers.py\", line 1769, in flatten_iterator\n    for pdf in iter_pdf:\n  File \"/Workspace/Users/jay.palaniappan@databricks.com/databricks-blogposts/2025-10-realtime-streaming-feature-enginnering-with-lakebase/utils/feature_engineering.py\", line 653, in handleInputRows\n    trans_last_hour = sum(1 for t in prev_times if t >= one_hour_ago)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 310, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/functions/builtin.py\", line 1638, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/functions/builtin.py\", line 129, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/functions/builtin.py\", line 129, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n                                    ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/classic/column.py\", line 77, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.\n\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n/root/.ipykernel/15828/command-4593315501084477-3160259181:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  fraud_df = pd.read_sql(fraud_query, conn)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTop 10 Users by Fraud Predictions:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4593315501084477>, line 27\u001B[0m\n",
       "\u001B[1;32m     24\u001B[0m     fraud_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_sql(fraud_query, conn)\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTop 10 Users by Fraud Predictions:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 27\u001B[0m display(fraud_df)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:156\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mmake_dataframe())\n",
       "\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpandas.core.frame\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n",
       "\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatabricks.koalas.frame\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyspark.pandas.frame\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \\\n",
       "\u001B[1;32m    158\u001B[0m         \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[1;32m    159\u001B[0m     index_col \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex_col\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1658\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(data, columns\u001B[38;5;241m=\u001B[39mcolumn_names)\n",
       "\u001B[1;32m   1660\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1661\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[0;32m-> 1662\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1663\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1664\u001B[0m     )\n",
       "\u001B[1;32m   1665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pyarrow \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pa\u001B[38;5;241m.\u001B[39mTable):\n",
       "\u001B[1;32m   1666\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from PyArrow Table.\u001B[39;00m\n",
       "\u001B[1;32m   1667\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1668\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1669\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py:485\u001B[0m, in \u001B[0;36mSparkConversionMixin.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\u001B[1;32m    484\u001B[0m converted_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_from_pandas(data, schema, timezone)\n",
       "\u001B[0;32m--> 485\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1727\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1725\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1727\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1728\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1729\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1247\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1240\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1241\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1242\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1243\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1244\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1245\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1246\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1247\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1213\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1211\u001B[0m tupled_data: Iterable[Tuple]\n",
       "\u001B[1;32m   1212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m-> 1213\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\u001B[1;32m   1214\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m   1215\u001B[0m     tupled_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1069\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m   1054\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1055\u001B[0m \u001B[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001B[39;00m\n",
       "\u001B[1;32m   1056\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1066\u001B[0m \u001B[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001B[39;00m\n",
       "\u001B[1;32m   1067\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
       "\u001B[0;32m-> 1069\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema from empty dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1070\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n",
       "\u001B[1;32m   1071\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: can not infer schema from empty dataset"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "can not infer schema from empty dataset"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: can not infer schema from empty dataset\n[Trace ID: 00-c0aacd79d0d0e9bd63f51c36c5804a0d-c28e88bffceb2a85-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-4593315501084477>, line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m     fraud_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_sql(fraud_query, conn)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTop 10 Users by Fraud Predictions:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m display(fraud_df)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:156\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mmake_dataframe())\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpandas.core.frame\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatabricks.koalas.frame\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyspark.pandas.frame\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28minput\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    159\u001B[0m     index_col \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex_col\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1658\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(data, columns\u001B[38;5;241m=\u001B[39mcolumn_names)\n\u001B[1;32m   1660\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1661\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m-> 1662\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1663\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1664\u001B[0m     )\n\u001B[1;32m   1665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pyarrow \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pa\u001B[38;5;241m.\u001B[39mTable):\n\u001B[1;32m   1666\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from PyArrow Table.\u001B[39;00m\n\u001B[1;32m   1667\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1668\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1669\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py:485\u001B[0m, in \u001B[0;36mSparkConversionMixin.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    484\u001B[0m converted_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_from_pandas(data, schema, timezone)\n\u001B[0;32m--> 485\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1727\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1725\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1727\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1728\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1729\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1247\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1240\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1241\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1243\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1244\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1245\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1246\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1247\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1213\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m tupled_data: Iterable[Tuple]\n\u001B[1;32m   1212\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m-> 1213\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m   1214\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m   1215\u001B[0m     tupled_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1069\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001B[39;00m\n\u001B[1;32m   1056\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001B[39;00m\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[0;32m-> 1069\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema from empty dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1070\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n\u001B[1;32m   1071\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n",
        "\u001B[0;31mValueError\u001B[0m: can not infer schema from empty dataset"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wait for data to process\n",
    "import time\n",
    "print(\"Waiting 30 seconds for data to process...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Query top fraud users\n",
    "fraud_query = \"\"\"\n",
    "SELECT \n",
    "    user_id,\n",
    "    COUNT(*) as total_transactions,\n",
    "    SUM(is_fraud_prediction) as predicted_frauds,\n",
    "    AVG(fraud_score) as avg_fraud_score,\n",
    "    MAX(fraud_score) as max_fraud_score,\n",
    "    SUM(is_rapid_transaction) as rapid_transactions,\n",
    "    SUM(is_impossible_travel) as impossible_travels\n",
    "FROM fraud_features\n",
    "GROUP BY user_id\n",
    "HAVING SUM(is_fraud_prediction) > 0\n",
    "ORDER BY predicted_frauds DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "with lakebase.get_connection() as conn:\n",
    "    fraud_df = pd.read_sql(fraud_query, conn)\n",
    "\n",
    "print(\"\\nTop 10 Users by Fraud Predictions:\")\n",
    "display(fraud_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4007f7d-660c-417c-8864-334a2d16680d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Stop Streaming Query\n",
    "\n",
    "Stop the streaming pipeline when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551b6d01-d800-43b8-9608-7f52a3d63c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop streaming query\n",
    "if query.isActive:\n",
    "    query.stop()\n",
    "    print(\"Streaming query stopped\")\n",
    "\n",
    "print(\"\\nPipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "streaming_fraud_detection_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
