{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Real-Time Streaming Fraud Detection Pipeline\n",
        "\n",
        "This notebook demonstrates an end-to-end streaming fraud detection pipeline combining:\n",
        "- **Stateless features**: Time, amount, merchant, device, network (from `AdvancedFeatureEngineering`)\n",
        "- **Stateful features**: Transaction velocity, IP tracking, location anomalies (from `FraudDetectionFeaturesProcessor`)\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Streaming Source (Rate)\n",
        "    \u2193\n",
        "Generate Transactions (TransactionDataGenerator)\n",
        "    \u2193\n",
        "Apply Stateless Features (AdvancedFeatureEngineering)\n",
        "    \u2193\n",
        "Apply Stateful Fraud Detection (transformWithStateInPandas)\n",
        "    \u2193\n",
        "Write to Lakebase PostgreSQL (foreachBatch)\n",
        "    \u2193\n",
        "Real-Time Feature Serving (<10ms query latency)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Run `00_setup.ipynb` to create feature tables\n",
        "- Databricks Runtime 17.3+ with Spark 4.0+\n",
        "- Lakebase PostgreSQL instance provisioned\n",
        "\n",
        "## Features Generated\n",
        "\n",
        "**Stateless (~40 columns):**\n",
        "- Time-based: hour, day, business hours, cyclical encodings\n",
        "- Amount-based: log, sqrt, categories, z-scores\n",
        "- Merchant: risk scores\n",
        "- Location, Device, Network: risk indicators\n",
        "\n",
        "**Stateful (~25 columns):**\n",
        "- Velocity: transaction counts in time windows\n",
        "- IP tracking: IP change detection\n",
        "- Location anomalies: impossible travel detection\n",
        "- Amount anomalies: statistical outliers\n",
        "- Fraud scoring: composite 0-100 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Import utility modules\n",
        "from utils.data_generator import TransactionDataGenerator\n",
        "from utils.feature_engineering import (\n",
        "    AdvancedFeatureEngineering, \n",
        "    FraudDetectionFeaturesProcessor,\n",
        "    get_fraud_detection_output_schema\n",
        ")\n",
        "from utils.lakebase_client import LakebaseClient\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"All modules imported successfully\")\n",
        "print(f\"Spark version: {spark.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Configuration\n",
        "\n",
        "Configure Lakebase connection and initialize components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lakebase connection configuration\n",
        "LAKEBASE_CONFIG = {\n",
        "    \"instance_name\": \"neha-lakebase-demo\",\n",
        "    \"database\": \"databricks_postgres\"\n",
        "}\n",
        "\n",
        "# Initialize components\n",
        "data_gen = TransactionDataGenerator(spark)\n",
        "feature_engineer = AdvancedFeatureEngineering(spark)\n",
        "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
        "\n",
        "# Test Lakebase connection\n",
        "if lakebase.test_connection():\n",
        "    print(\"Connected to Lakebase PostgreSQL\")\n",
        "else:\n",
        "    raise Exception(\"Failed to connect to Lakebase\")\n",
        "\n",
        "# Verify fraud_features table exists\n",
        "print(\"\\nVerifying fraud_features table...\")\n",
        "try:\n",
        "    stats = lakebase.get_table_stats(\"fraud_features\")\n",
        "    print(f\"  Table exists with {stats['total_rows']:,} rows\")\n",
        "except Exception as e:\n",
        "    print(\"  Table not found. Creating it now...\")\n",
        "    lakebase.create_feature_table(\"fraud_features\")\n",
        "    print(\"  Table created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Streaming Transaction Data\n",
        "\n",
        "Create a streaming source that continuously generates synthetic transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate streaming transaction data\n",
        "df_transactions = data_gen.generate_transaction_data(\n",
        "    num_users=50,           # 50 unique users\n",
        "    num_merchants=100,      # 100 unique merchants\n",
        "    rows_per_second=10      # 10 transactions per second\n",
        ")\n",
        "\n",
        "print(\"Streaming data source created\")\n",
        "print(\"\\nTransaction schema:\")\n",
        "df_transactions.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Apply Stateless Features\n",
        "\n",
        "Apply time-based, amount-based, merchant, location, device, and network features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the output schema for fraud detection features\n",
        "# This schema is defined in utils.feature_engineering module\n",
        "output_schema = get_fraud_detection_output_schema()\n",
        "\n",
        "print(\"Output schema loaded from feature_engineering module\")\n",
        "print(f\"Total fields: {len(output_schema.fields)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define Stateful Fraud Detection Schema\n",
        "\n",
        "Define the output schema that combines stateless and stateful features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define complete output schema (stateless + stateful features)\n",
        "# Note: This is a simplified schema - the full schema matches the fraud_features table\n",
        "output_schema = StructType([\n",
        "    # Core transaction fields\n",
        "    StructField(\"transaction_id\", StringType(), False),\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"timestamp\", TimestampType(), False),\n",
        "    StructField(\"amount\", DoubleType(), False),\n",
        "    StructField(\"merchant_id\", StringType(), False),\n",
        "    StructField(\"ip_address\", StringType(), False),\n",
        "    StructField(\"latitude\", DoubleType(), False),\n",
        "    StructField(\"longitude\", DoubleType(), False),\n",
        "    \n",
        "    # Stateful fraud detection features\n",
        "    StructField(\"user_transaction_count\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_hour\", IntegerType(), False),\n",
        "    StructField(\"transactions_last_10min\", IntegerType(), False),\n",
        "    StructField(\"ip_changed\", IntegerType(), False),\n",
        "    StructField(\"ip_change_count_total\", IntegerType(), False),\n",
        "    StructField(\"distance_from_last_km\", DoubleType(), True),\n",
        "    StructField(\"velocity_kmh\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_avg_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_vs_user_max_ratio\", DoubleType(), True),\n",
        "    StructField(\"amount_zscore\", DoubleType(), True),\n",
        "    StructField(\"seconds_since_last_transaction\", DoubleType(), True),\n",
        "    StructField(\"is_rapid_transaction\", IntegerType(), False),\n",
        "    StructField(\"is_impossible_travel\", IntegerType(), False),\n",
        "    StructField(\"is_amount_anomaly\", IntegerType(), False),\n",
        "    StructField(\"fraud_score\", DoubleType(), False),\n",
        "    StructField(\"is_fraud_prediction\", IntegerType(), False)\n",
        "])\n",
        "\n",
        "print(\"Output schema defined with stateful fraud features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Apply Stateful Fraud Detection\n",
        "\n",
        "Use `transformWithStateInPandas` to maintain per-user state and detect fraud patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply stateful fraud detection using transformWithStateInPandas\n",
        "df_with_fraud_features = df_with_stateless_features \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .transformWithStateInPandas(\n",
        "        statefulProcessor=FraudDetectionFeaturesProcessor(),\n",
        "        outputStructType=output_schema,\n",
        "        outputMode=\"Append\",\n",
        "        timeMode=\"None\"\n",
        "    )\n",
        "\n",
        "print(\"Stateful fraud detection configured\")\n",
        "print(\"\\nFinal schema with all features:\")\n",
        "df_with_fraud_features.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Write to Lakebase PostgreSQL\n",
        "\n",
        "Stream all features to Lakebase for real-time serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define foreachBatch function\n",
        "def write_to_lakebase(batch_df, batch_id):\n",
        "    if batch_df.isEmpty():\n",
        "        return\n",
        "    \n",
        "    logger.info(f\"Processing batch {batch_id} with {batch_df.count()} rows\")\n",
        "    lakebase.write_streaming_batch(batch_df, batch_id, \"fraud_features\")\n",
        "    logger.info(f\"Batch {batch_id} written to Lakebase\")\n",
        "\n",
        "# Start streaming query\n",
        "query = df_with_fraud_features \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .foreachBatch(write_to_lakebase) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/fraud_pipeline_checkpoint\") \\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"Streaming pipeline started!\")\n",
        "print(f\"Query ID: {query.id}\")\n",
        "print(f\"Status: {query.status}\")\n",
        "print(\"\\nPipeline: Transactions \u2192 Stateless Features \u2192 Stateful Fraud Detection \u2192 Lakebase\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Monitor and Query Results\n",
        "\n",
        "Query fraud features from Lakebase for real-time insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for data to process\n",
        "import time\n",
        "print(\"Waiting 30 seconds for data to process...\")\n",
        "time.sleep(30)\n",
        "\n",
        "# Query top fraud users\n",
        "fraud_query = \"\"\"\n",
        "SELECT \n",
        "    user_id,\n",
        "    COUNT(*) as total_transactions,\n",
        "    SUM(is_fraud_prediction) as predicted_frauds,\n",
        "    AVG(fraud_score) as avg_fraud_score,\n",
        "    MAX(fraud_score) as max_fraud_score,\n",
        "    SUM(is_rapid_transaction) as rapid_transactions,\n",
        "    SUM(is_impossible_travel) as impossible_travels\n",
        "FROM fraud_features\n",
        "GROUP BY user_id\n",
        "HAVING SUM(is_fraud_prediction) > 0\n",
        "ORDER BY predicted_frauds DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "with lakebase.get_connection() as conn:\n",
        "    fraud_df = pd.read_sql(fraud_query, conn)\n",
        "\n",
        "print(\"\\nTop 10 Users by Fraud Predictions:\")\n",
        "display(fraud_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Stop Streaming Query\n",
        "\n",
        "Stop the streaming pipeline when done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop streaming query\n",
        "if query.isActive:\n",
        "    query.stop()\n",
        "    print(\"Streaming query stopped\")\n",
        "\n",
        "print(\"\\nPipeline complete!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "application/vnd.databricks.v1+notebook": {
      "language": "python",
      "notebookName": "streaming_fraud_detection_pipeline"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}