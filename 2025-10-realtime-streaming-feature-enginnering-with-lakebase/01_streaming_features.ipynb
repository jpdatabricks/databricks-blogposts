{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0c27aa-6037-455b-8921-9a8bdb528f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming Feature Engineering Examples and Usage\n",
    "\n",
    "This notebook demonstrates how to use Spark Structured Streaming to generate realtime features from a syntentic  real-time credit card data source and publish it to Lakebase.\n",
    "\n",
    "**Note**: \n",
    "- All feature engineering methods are implemented in `utils/feature_engineering.py` file \n",
    "- Streaming data is generated using `TransactionDataGenerator` from `utils/data_generator.py` with a rate source\n",
    "- This notebook only demonstrates usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Databricks Runtime 17.3+\n",
    "- Lakebase PostgreSQL instance provisioned\n",
    "- Run `00_setup.ipynb` first to create the feature table\n",
    "\n",
    "## Generated Features\n",
    "\n",
    "- **Time-based**: Hour, day of week, business hours, cyclical encoding (year, month, day, etc.)\n",
    "- **Amount-based**: Log transformations, categories, z-scores\n",
    "- **Merchant**: Risk scores based on merchant category\n",
    "- **Location**: Risk indicators, region classification (if location data available)\n",
    "- **Device**: Device type detection (if device data available)\n",
    "- **Network**: IP classification, private/public indicators (if IP data available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8765ba6f-6719-457d-b638-a028ce632b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d471be9b-42c4-4b76-912f-540de1458bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Import the AdvancedFeatureEngineering class from feature_engineering.py\n",
    "import sys\n",
    "from utils.feature_engineering import AdvancedFeatureEngineering\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\" Spark version: {spark.version}\")\n",
    "print(f\" Feature engineering module imported from feature_engineering.py\")\n",
    "\n",
    "# Lakebase connection configuration\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"neha-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\",    \n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the AdvancedFeatureEngineering class\n",
    "feature_engineer = AdvancedFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5fe351-d9ad-4b21-a4b4-0585b63dca11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Streaming Transaction Data Source\n",
    "\n",
    "Import the data generator and create a **streaming DataFrame** that continuously generates realistic transaction data.\n",
    "\n",
    "**Note**: This uses PySpark Structured Streaming with a rate source - perfect for testing streaming feature engineering!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a03f40-6559-4e6c-8e7d-04f9ba108e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the data generator class\n",
    "from utils.data_generator import TransactionDataGenerator\n",
    "\n",
    "# Initialize generator\n",
    "generator = TransactionDataGenerator()\n",
    "\n",
    "# Create a STREAMING DataFrame\n",
    "# This continuously generates transactions at the specified rate\n",
    "df_streaming = generator.generate_transaction_data(\n",
    "    num_users=10,           # 10 unique users\n",
    "    num_merchants=20,       # 20 unique merchants  \n",
    "    rows_per_second=5       # Generate 5 transactions per second\n",
    ")\n",
    "\n",
    "print(f\" Created streaming data source\")\n",
    "\n",
    "print(f\"\\n Streaming DataFrame Schema:\")\n",
    "df_streaming.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c78bc7-ba8b-416f-81e0-1760cc5fe314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streaming Feature Engineering with Real-Time Features\n",
    "\n",
    "Apply all stateless feature engineering transformations to the streaming DataFrame.\n",
    "\n",
    "This includes time-based, amount-based, merchant, location, device, and network features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d488c7f9-e019-477d-8719-24b75cd37c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply ALL features to streaming DataFrame\n",
    "df_with_features = feature_engineer.apply_all_features(df_streaming)\n",
    "\n",
    "print(\" Features applied to streaming data\")\n",
    "print(f\"\\n Schema with engineered features:\")\n",
    "df_with_features.printSchema()\n",
    "\n",
    "#display(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873484c2-bb9f-4ca4-823d-8361bda509ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.lakebase_client import LakebaseClient\n",
    "import time\n",
    "\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Write to Lakebase PostgreSQL in batches\n",
    "df_with_features.writeStream.foreachBatch(lakebase.write_streaming_batch).start()\n",
    "\n",
    "print(\" Streaming to Lakebase PostgreSQL...\")\n",
    "# time.sleep(60)\n",
    "# query.stop()\n",
    "# print(\" Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197747fa-71f8-487e-887f-8ae314ebe3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.lakebase_client import LakebaseClient\n",
    "import time\n",
    "\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "for_each_writer = lakebase.get_foreach_writer(\n",
    "    creds=lakebase.get_credentials(), table_name=\"transaction_features\", batch_size=2\n",
    ")\n",
    "# Write to Lakebase PostgreSQL for each record\n",
    "\n",
    "df_with_features.writeStream \\\n",
    "  .foreach(for_each_writer) \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .trigger(realTime=\"5 seconds\") \\\n",
    "  .start()\n",
    "\n",
    "print(\" Streaming to Lakebase PostgreSQL...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea345a1b-5476-448a-8e20-d7337ddfa3d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query features\n",
    "stats = lakebase.get_table_stats()\n",
    "print(f\" Total rows: {stats['total_rows']:,}\")\n",
    "\n",
    "recent = lakebase.read_features('SELECT * FROM transaction_features ORDER BY timestamp DESC LIMIT 10')\n",
    "display(recent)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_streaming_features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
