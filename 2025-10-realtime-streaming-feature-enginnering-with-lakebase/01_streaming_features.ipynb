{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0c27aa-6037-455b-8921-9a8bdb528f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering Examples and Usage\n",
    "\n",
    "This notebook demonstrates how to use the `AdvancedFeatureEngineering` class from `feature_engineering.py` for transaction data.\n",
    "\n",
    "**Important**: \n",
    "- All feature engineering methods are implemented in `feature_engineering.py` (single source of truth)\n",
    "- Sample data is generated using `TransactionDataGenerator` from `data_generator.py` \n",
    "- This notebook only demonstrates usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install `dbldatagen` for data generation:\n",
    "```python\n",
    "%pip install dbldatagen\n",
    "dbutils.library.restartPython()\n",
    "```\n",
    "\n",
    "## Available Features\n",
    "\n",
    "- **Time-based**: Hour, day of week, business hours, cyclical encoding\n",
    "- **Amount-based**: Log transformations, categories, statistical features\n",
    "- **Velocity**: Transaction counts/amounts over time windows\n",
    "- **Behavioral**: User patterns, merchant switching\n",
    "- **Location**: Distance calculations, velocity\n",
    "- **Risk**: Risk scoring and anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8765ba6f-6719-457d-b638-a028ce632b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d471be9b-42c4-4b76-912f-540de1458bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Import the AdvancedFeatureEngineering class from feature_engineering.py\n",
    "import sys\n",
    "from feature_engineering import AdvancedFeatureEngineering\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"âœ… Spark version: {spark.version}\")\n",
    "print(f\"âœ… Feature engineering module imported from feature_engineering.py\")\n",
    "\n",
    "# Initialize the AdvancedFeatureEngineering class\n",
    "feature_engineer = AdvancedFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5fe351-d9ad-4b21-a4b4-0585b63dca11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Streaming Transaction Data Source\n",
    "\n",
    "Import the data generator and create a **streaming DataFrame** that continuously generates realistic transaction data.\n",
    "\n",
    "**Note**: This uses PySpark Structured Streaming with a rate source - perfect for testing streaming feature engineering!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a03f40-6559-4e6c-8e7d-04f9ba108e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the data generator class\n",
    "from data_generator import TransactionDataGenerator\n",
    "\n",
    "# Initialize generator\n",
    "generator = TransactionDataGenerator()\n",
    "\n",
    "# Create a STREAMING DataFrame\n",
    "# This continuously generates transactions at the specified rate\n",
    "df_streaming = generator.generate_transaction_data(\n",
    "    num_users=10,           # 10 unique users\n",
    "    num_merchants=20,       # 20 unique merchants  \n",
    "    rows_per_second=5       # Generate 5 transactions per second\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created streaming data source\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Streaming DataFrame Schema:\")\n",
    "df_streaming.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c78bc7-ba8b-416f-81e0-1760cc5fe314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streaming Feature Engineering with Real-Time Features\n",
    "\n",
    "First, let's demonstrate feature engineering on **batch data** (easier to inspect).\n",
    "\n",
    "Use `create_time_based_features()` to extract time-related features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d488c7f9-e019-477d-8719-24b75cd37c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply ALL features to streaming DataFrame\n",
    "df_with_features = feature_engineer.apply_all_features(df_streaming)\n",
    "\n",
    "print(\"âœ… Features applied to streaming data\")\n",
    "print(f\"\\nðŸ“‹ Schema with engineered features:\")\n",
    "df_with_features.printSchema()\n",
    "\n",
    "display(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Lakebase PostgreSQL\n",
    "query = feature_engineer.write_features_to_lakebase(\n",
    "    df=df_with_features,\n",
    "    lakebase_client=lakebase,\n",
    "    table_name=\"transaction_features\"\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Streaming to Lakebase PostgreSQL...\")\n",
    "time.sleep(60)\n",
    "query.stop()\n",
    "print(\"âœ… Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query features\n",
    "stats = lakebase.get_table_stats()\n",
    "print(f\"ðŸ“Š Total rows: {stats['total_rows']:,}\")\n",
    "\n",
    "recent = lakebase.read_features('SELECT * FROM transaction_features ORDER BY timestamp DESC LIMIT 10')\n",
    "display(recent)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
