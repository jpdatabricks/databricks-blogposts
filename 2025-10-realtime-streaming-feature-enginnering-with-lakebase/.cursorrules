# Project Context Rules

## PRIMARY RULE: No Emojis in Code

**CRITICAL: Do NOT use emojis in code unless explicitly requested by the user.**

### What This Means:
- ‚ùå NO emojis in Python code (`.py` files)
- ‚ùå NO emojis in notebook cells (`.ipynb`)
- ‚ùå NO emojis in log messages, print statements, or comments
- ‚ùå NO emojis in function names, variable names, or docstrings
- ‚úÖ Use plain text messages instead

### Examples:

**BAD:**
```python
logger.info("‚úÖ Connection successful")
print("üöÄ Starting process...")
# ‚ùå Don't do this
```

**GOOD:**
```python
logger.info("Connection successful")
print("Starting process...")
# Don't do this
```

### Exceptions:
- Emojis MAY be used in documentation files (README.md, etc.) for visual clarity
- Emojis MAY be used ONLY if the user explicitly requests them

## PRIMARY RULE: Lakebase Definition

**CRITICAL: In this project, "Lakebase" ALWAYS refers to:**
- **Databricks Lakebase PostgreSQL** (OLTP database)
- Port: 5432
- Connection: psycopg2, JDBC, SQLAlchemy
- Query latency: <10ms
- Use case: Real-time feature serving

**Lakebase does NOT refer to:**
- ‚ùå Delta Lake
- ‚ùå File-based storage
- ‚ùå Databricks Feature Store API
- ‚ùå Any path like `/mnt/lakebase/*`

## PRIMARY RULE: Streaming-Only Project

**CRITICAL: This project is EXCLUSIVELY for streaming data processing:**

### What This Means:
1. **ALL code must be streaming-compatible**
   - Use `readStream` and `writeStream`
   - Use `foreachBatch` for sinks
   - Apply watermarks for time-based operations
   - Use bounded state management

2. **NO batch processing code**
   - ‚ùå NO `.read()` for data sources
   - ‚ùå NO `.write()` for data sinks
   - ‚ùå NO batch-only DataFrame operations
   - ‚ùå NO `Window.rangeBetween()` without streaming mode check
   - ‚ùå NO batch-only aggregations

3. **Streaming-compatible patterns ONLY**
   - ‚úÖ `spark.readStream.format("rate")` or other streaming sources
   - ‚úÖ `.withWatermark("timestamp", "10 minutes")`
   - ‚úÖ `window("timestamp", "1 hour")` for time-based grouping
   - ‚úÖ `.writeStream.foreachBatch()` for custom sinks
   - ‚úÖ `.writeStream.format("console")` for debugging

### Code Generation Rules:

**When generating new code:**
- ALWAYS assume streaming context
- NEVER add batch processing logic
- ALWAYS use streaming-compatible window functions
- NEVER use Window.rangeBetween() without streaming mode flag

**When modifying existing code:**
- REMOVE any batch-specific code paths
- CONVERT batch patterns to streaming equivalents
- DELETE batch-only methods and functions
- SIMPLIFY dual-mode code to streaming-only

**When adding features:**
- ONLY implement streaming-compatible aggregations
- USE `window()` function for time-based grouping
- APPLY watermarks for late data handling
- AVOID unbounded state operations

### Examples of What to REMOVE:

```python
# ‚ùå REMOVE: Batch read
df = spark.read.format("delta").load("/path")

# ‚ùå REMOVE: Batch write
df.write.format("delta").save("/path")

# ‚ùå REMOVE: Batch window with rangeBetween
Window.partitionBy("user_id").orderBy("timestamp").rangeBetween(-3600, 0)

# ‚ùå REMOVE: Batch-only aggregations
df.groupBy("user_id").agg(count("*"))  # Without windowing
```

### Examples of What to USE:

```python
# ‚úÖ USE: Streaming read
df = spark.readStream.format("rate").load()

# ‚úÖ USE: Streaming write with foreachBatch
df.writeStream.foreachBatch(lambda batch, id: process(batch, id)).start()

# ‚úÖ USE: Streaming window aggregations
df.groupBy("user_id", window("timestamp", "1 hour")).agg(count("*"))

# ‚úÖ USE: Watermark for late data
df.withWatermark("timestamp", "10 minutes")
```

### Function Signature Pattern:

```python
# ‚ùå AVOID: Dual-mode functions
def process_data(df, streaming_mode=True):
    if streaming_mode:
        # streaming logic
    else:
        # batch logic  # DELETE THIS

# ‚úÖ PREFER: Streaming-only functions
def process_data(df):
    # Only streaming logic
    # No mode flag needed
```

## Code Patterns

### When writing Lakebase connection code:
```python
from lakebase_client import LakebaseClient

lakebase = LakebaseClient(
    host="workspace.cloud.databricks.com",
    port=5432,  # PostgreSQL port
    database="feature_store",
    user="token",
    password=oauth_token
)
```

### When writing features to Lakebase:
```python
# Use foreachBatch for streaming writes to PostgreSQL
query = feature_engineer.write_features_to_lakebase(
    df=df_with_features,
    lakebase_client=lakebase,  # PostgreSQL client
    table_name="transaction_features"
)
```

### When querying features from Lakebase:
```python
# Direct SQL queries to PostgreSQL
features = lakebase.read_features("""
    SELECT * FROM transaction_features
    WHERE user_id = 'user_123'
""")
```

## Architecture

```
Streaming Data 
    ‚Üì
Feature Engineering (PySpark)
    ‚Üì
foreachBatch write
    ‚Üì
Lakebase PostgreSQL (port 5432)
    ‚Üì
SQL queries (<10ms latency)
    ‚Üì
Real-time ML Model Serving
```

## File Conventions

- `lakebase_*.py` - PostgreSQL-related modules
- `*_lakebase_*.ipynb` - Lakebase demo notebooks
- `LAKEBASE_*.md` - Lakebase documentation

## What to NEVER suggest:

1. ‚ùå Delta Lake writes for final feature storage
2. ‚ùå `FeatureStoreClient()` API
3. ‚ùå `.format("delta")` for Lakebase writes
4. ‚ùå File paths for feature serving
5. ‚ùå Referring to Delta Lake as "Lakebase"
6. ‚ùå Batch processing code (`.read()`, `.write()`)
7. ‚ùå Dual-mode functions with `streaming_mode` flags
8. ‚ùå `Window.rangeBetween()` without bounded state
9. ‚ùå Unbounded stateful operations
10. ‚ùå Batch-only aggregations without time windows

## What to ALWAYS suggest:

1. ‚úÖ `LakebaseClient` for connections
2. ‚úÖ psycopg2 for database operations
3. ‚úÖ `foreachBatch` for streaming writes
4. ‚úÖ SQL queries for feature retrieval
5. ‚úÖ Port 5432 for connections
6. ‚úÖ `.readStream` and `.writeStream` for all data operations
7. ‚úÖ `.withWatermark()` for time-based operations
8. ‚úÖ `window()` function for time-based grouping
9. ‚úÖ Streaming-only function signatures (no mode flags)
10. ‚úÖ Bounded state management patterns

## Technology Stack

- **Streaming:** PySpark Structured Streaming
- **Feature Storage:** Lakebase PostgreSQL (NOT Delta Lake)
- **Connection:** psycopg2-binary
- **Query Language:** PostgreSQL SQL
- **Runtime:** Databricks Runtime 13.0+ ML

## Performance Expectations

- Write latency: 50-100ms
- Query latency: <10ms
- Concurrency: High
- Transaction support: ACID

## Documentation References

When discussing this project, always reference:
1. `.cursorrules` (this file) - Terminology and conventions
2. `lakebase_client.py` - Implementation reference
3. `00_setup_and_configuration.ipynb` - Setup instructions
4. `README.md` - Project overview and quick start

## Common Mistakes to Avoid

1. Confusing Lakebase PostgreSQL with Delta Lake
2. Using Delta Lake APIs when Lakebase is meant
3. Suggesting file-based storage for real-time serving
4. Recommending Feature Store API instead of direct PostgreSQL
5. Adding batch processing code to a streaming-only project
6. Using `Window.rangeBetween()` in streaming context
7. Creating dual-mode functions with `streaming_mode` flags
8. Forgetting to apply watermarks for time-based operations
9. Using unbounded state operations in streaming

## When User Says "Lakebase"

Always interpret as:
- **Lakebase PostgreSQL** (OLTP database at port 5432)
- Real-time transactional database
- NOT a data lake or file storage

## When User Says "Process Data" or "Read/Write Data"

Always interpret as:
- **Streaming operations** using `readStream` and `writeStream`
- Real-time processing with bounded state
- NOT batch operations (`.read()` or `.write()`)

## Code Review Checklist

When reviewing code in this project, verify:
- [ ] "Lakebase" refers to PostgreSQL, not Delta Lake
- [ ] Connections use port 5432
- [ ] Writes use `foreachBatch` or `write_batch`
- [ ] Queries are SQL-based (not Spark DataFrame reads)
- [ ] Documentation mentions PostgreSQL explicitly
- [ ] ALL data operations use `.readStream` and `.writeStream`
- [ ] NO batch processing code (`.read()`, `.write()`)
- [ ] Watermarks applied for time-based operations
- [ ] Time-based grouping uses `window()` function
- [ ] NO `Window.rangeBetween()` in streaming context
- [ ] NO dual-mode functions with `streaming_mode` parameter
- [ ] State management is bounded (not unbounded)

## Example Conversation Flow

**User:** "Write features to Lakebase"
**Correct Response:** Write features to Lakebase PostgreSQL using streaming: `df.writeStream.foreachBatch(lakebase_client.write_streaming_batch).start()`

**User:** "Query features from Lakebase"
**Correct Response:** Query features from Lakebase PostgreSQL using SQL: `lakebase.read_features("SELECT ...")`

**User:** "Setup Lakebase"
**Correct Response:** Provision Lakebase PostgreSQL instance in Databricks (Compute ‚Üí OLTP Database)

**User:** "Read transaction data"
**Correct Response:** Use streaming read: `df = spark.readStream.format("rate").load()` (NOT batch `.read()`)

**User:** "Add feature engineering"
**Correct Response:** Add streaming-compatible features using `window()` and watermarks (NOT `Window.rangeBetween()`)

**User:** "Process the data"
**Correct Response:** Use streaming pipeline with `.writeStream.foreachBatch()` (NOT batch `.write()`)

## Version Control

This context rule is the source of truth for:
- Project terminology
- Architecture decisions
- Technology choices
- Code patterns
- Streaming-only requirements

Last updated: October 2025

