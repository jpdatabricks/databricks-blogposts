{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Examples and Usage\n",
    "\n",
    "This notebook demonstrates how to use the `AdvancedFeatureEngineering` class from `feature_engineering.py` for transaction data.\n",
    "\n",
    "**Important**: \n",
    "- All feature engineering methods are implemented in `feature_engineering.py` (single source of truth)\n",
    "- Sample data is generated using `TransactionDataGenerator` from `data_generator.py` \n",
    "- This notebook only demonstrates usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install `dbldatagen` for data generation:\n",
    "```python\n",
    "%pip install dbldatagen\n",
    "dbutils.library.restartPython()\n",
    "```\n",
    "\n",
    "## Available Features\n",
    "\n",
    "- **Time-based**: Hour, day of week, business hours, cyclical encoding\n",
    "- **Amount-based**: Log transformations, categories, statistical features\n",
    "- **Velocity**: Transaction counts/amounts over time windows\n",
    "- **Behavioral**: User patterns, merchant switching\n",
    "- **Location**: Distance calculations, velocity\n",
    "- **Risk**: Risk scoring and anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Import the AdvancedFeatureEngineering class from feature_engineering.py\n",
    "import sys\n",
    "sys.path.append('/Workspace/Repos/your-repo-path')  # Update with your actual path\n",
    "from feature_engineering import AdvancedFeatureEngineering\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"\u2705 Spark version: {spark.version}\")\n",
    "print(f\"\u2705 Feature engineering module imported from feature_engineering.py\")\n",
    "\n",
    "# Initialize the AdvancedFeatureEngineering class\n",
    "feature_engineer = AdvancedFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Transaction Data Source\n",
    "\n",
    "Import the data generator and create a **streaming DataFrame** that continuously generates realistic transaction data.\n",
    "\n",
    "**Note**: This uses PySpark Structured Streaming with a rate source - perfect for testing streaming feature engineering!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data generator class\n",
    "from data_generator import TransactionDataGenerator\n",
    "\n",
    "# Initialize generator\n",
    "generator = TransactionDataGenerator()\n",
    "\n",
    "# Create a STREAMING DataFrame\n",
    "# This continuously generates transactions at the specified rate\n",
    "df_streaming = generator.generate_transaction_data(\n",
    "    num_users=10,           # 10 unique users\n",
    "    num_merchants=20,       # 20 unique merchants  \n",
    "    rows_per_second=5       # Generate 5 transactions per second\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Created streaming data source\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Streaming DataFrame Schema:\")\n",
    "df_streaming.printSchema()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Feature Engineering with Real-Time Features\n",
    "\n",
    "First, let's demonstrate feature engineering on **batch data** (easier to inspect).\n",
    "\n",
    "Use `create_time_based_features()` to extract time-related features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ALL features to streaming DataFrame\n",
    "df_with_features = feature_engineer.apply_all_features(df_streaming)\n",
    "\n",
    "print(\"\u2705 Features applied to streaming data\")\n",
    "print(f\"\\n\ud83d\udccb Schema with engineered features:\")\n",
    "df_with_features.printSchema()\n",
    "\n",
    "# Run streaming query to preview features (30 seconds)\n",
    "print(\"\\n\ud83d\ude80 Starting streaming query with features...\")\n",
    "query = df_with_features.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 3) \\\n",
    "    .start()\n",
    "\n",
    "import time\n",
    "time.sleep(30)\n",
    "query.stop()\n",
    "\n",
    "print(\"\\n\u2705 Streaming feature engineering complete\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}