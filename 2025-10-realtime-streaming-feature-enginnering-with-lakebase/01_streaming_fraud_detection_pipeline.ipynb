{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de73176-34e7-4145-8bf6-fc373b28fa8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Streaming Fraud Detection Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end streaming fraud detection pipeline that combines stateless and stateful features.\n",
    "\n",
    "## Features\n",
    "\n",
    "**Stateless Features (~40 columns):**\n",
    "- Time-based: hour, day, business hours, cyclical encodings\n",
    "- Amount-based: log, sqrt, categories, z-scores\n",
    "- Merchant: risk scores based on category\n",
    "- Location: risk indicators, region classification\n",
    "- Device: device type detection\n",
    "- Network: IP classification\n",
    "\n",
    "**Stateful Features (~15 columns):**\n",
    "- Velocity: transaction counts in time windows (10 min, 1 hour)\n",
    "- IP tracking: IP change detection and counts\n",
    "- Location anomalies: impossible travel detection (velocity > 800 km/h)\n",
    "- Amount anomalies: z-score calculation vs user history\n",
    "- Fraud scoring: composite 0-100 score with prediction flag\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Streaming Source (rate)\n",
    "    ↓\n",
    "Generate Transactions (synthetic data)\n",
    "    ↓\n",
    "Apply Stateless Features (AdvancedFeatureEngineering)\n",
    "    ↓\n",
    "Apply Stateful Fraud Detection (transformWithStateInPandas)\n",
    "    ↓\n",
    "Write to Lakebase PostgreSQL (foreachBatch)\n",
    "    ↓\n",
    "Real-Time Feature Serving (<10ms query latency)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Run `00_setup.ipynb` first to create `transaction_features` table\n",
    "- Databricks Runtime 17.3+ (with Spark 4.0+ for transformWithStateInPandas)\n",
    "- Lakebase PostgreSQL instance provisioned\n",
    "\n",
    "## Output\n",
    "\n",
    "All features (stateless + stateful) are written to:\n",
    "- **Table**: `transaction_features` (~70+ columns)\n",
    "- **Write latency**: 50-100ms per micro-batch\n",
    "- **Query latency**: <10ms for real-time serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f55e757-87f6-49ee-a744-90925d831089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5925a17c-4c62-4400-a114-393d04803780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Import utility modules\n",
    "from utils.data_generator import TransactionDataGenerator\n",
    "from utils.feature_engineering import (\n",
    "    AdvancedFeatureEngineering, \n",
    "    FraudDetectionFeaturesProcessor,\n",
    "    get_fraud_detection_output_schema\n",
    ")\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"All modules imported successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea342fd-087f-4418-91dc-55751a4da71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Configure Lakebase connection and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fabf4d6-fea9-435c-b92c-2bce2217bae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:utils.lakebase_client:Lakebase connection test successful\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Lakebase PostgreSQL\n",
      "\n",
      "Verifying fraud_features table...\n",
      "0.68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "ERROR:utils.lakebase_client:Database error: relation \"fraud_features\" does not exist\n",
      "LINE 9:             FROM fraud_features\n",
      "                         ^\n",
      "\n",
      "ERROR:utils.lakebase_client:Error getting table stats: relation \"fraud_features\" does not exist\n",
      "LINE 9:             FROM fraud_features\n",
      "                         ^\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Table not found. Creating it now...\n",
      "0.68.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:utils.lakebase_client:Created unified feature table: transaction_features (~70+ columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Lakebase connection configuration\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"rtm-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\"\n",
    "}\n",
    "\n",
    "# Initialize components\n",
    "data_gen = TransactionDataGenerator(spark)\n",
    "feature_engineer = AdvancedFeatureEngineering(spark)\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Test Lakebase connection\n",
    "if lakebase.test_connection():\n",
    "    print(\"Connected to Lakebase PostgreSQL\")\n",
    "else:\n",
    "    raise Exception(\"Failed to connect to Lakebase\")\n",
    "\n",
    "# Verify transaction_features table exists\n",
    "print(\"\\nVerifying transaction_features table...\")\n",
    "try:\n",
    "    stats = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"  Table exists with {stats['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table not found. Please run 00_setup.ipynb first!\")\n",
    "    raise Exception(\"transaction_features table does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af261522-f8a3-446d-95c3-8ab010e4074c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Generate Streaming Transaction Data\n",
    "\n",
    "Create a streaming source that continuously generates synthetic transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a42abb-4b98-4730-b5c9-e8388343372a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data_generator:Creating streaming transaction source...\n",
      "INFO:utils.data_generator:   Rate: 10 transactions/second\n",
      "INFO:utils.data_generator:   Users: 50, Merchants: 100\n",
      "INFO:utils.data_generator:Streaming source created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming data source created\n",
      "\n",
      "Transaction schema:\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- merchant_id: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- currency: string (nullable = false)\n",
      " |-- merchant_category: string (nullable = false)\n",
      " |-- payment_method: string (nullable = false)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- location_lat: double (nullable = false)\n",
      " |-- location_lon: double (nullable = false)\n",
      " |-- card_type: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate streaming transaction data\n",
    "df_transactions = data_gen.generate_transaction_data(\n",
    "    num_users=50,           # 50 unique users\n",
    "    num_merchants=100,      # 100 unique merchants\n",
    "    rows_per_second=10      # 10 transactions per second\n",
    ")\n",
    "\n",
    "print(\"Streaming data source created\")\n",
    "print(\"\\nTransaction schema:\")\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228c0478-8d9e-494c-9f06-2d59f20ce5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Apply Stateless Features\n",
    "\n",
    "Apply time-based, amount-based, merchant, location, device, and network features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f35d8c-696a-458a-9a82-17c350209007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.feature_engineering:Applying streaming-compatible feature engineering...\n",
      "INFO:utils.feature_engineering:Creating time-based features...\n",
      "INFO:utils.feature_engineering:Creating amount-based features...\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:utils.feature_engineering:Creating merchant features (streaming-only)...\n",
      "INFO:utils.feature_engineering:Creating location features (streaming-only)...\n",
      "INFO:utils.feature_engineering:Creating device features (streaming-only)...\n",
      "INFO:utils.feature_engineering:Creating network features (streaming-only)...\n",
      "INFO:utils.feature_engineering:Streaming feature engineering completed!\n"
     ]
    }
   ],
   "source": [
    "df_with_stateless_features = feature_engineer.apply_all_features(df_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccb1c16-224a-4efc-be22-d98df03be063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Apply Stateful Features\n",
    "\n",
    "Use `transformWithStateInPandas` to maintain per-user state and detect fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f0fef-d09f-4c9a-8222-d45cd47ef79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stateful fraud detection configured\n",
      "\n",
      "Final schema with all features:\n",
      "root\n",
      " |-- transaction_id: string (nullable = false)\n",
      " |-- user_id: string (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      " |-- amount: double (nullable = false)\n",
      " |-- merchant_id: string (nullable = false)\n",
      " |-- ip_address: string (nullable = false)\n",
      " |-- latitude: double (nullable = false)\n",
      " |-- longitude: double (nullable = false)\n",
      " |-- user_transaction_count: integer (nullable = false)\n",
      " |-- transactions_last_hour: integer (nullable = false)\n",
      " |-- transactions_last_10min: integer (nullable = false)\n",
      " |-- ip_changed: integer (nullable = false)\n",
      " |-- ip_change_count_total: integer (nullable = false)\n",
      " |-- distance_from_last_km: double (nullable = true)\n",
      " |-- velocity_kmh: double (nullable = true)\n",
      " |-- amount_vs_user_avg_ratio: double (nullable = true)\n",
      " |-- amount_vs_user_max_ratio: double (nullable = true)\n",
      " |-- amount_zscore: double (nullable = true)\n",
      " |-- seconds_since_last_transaction: double (nullable = true)\n",
      " |-- is_rapid_transaction: integer (nullable = false)\n",
      " |-- is_impossible_travel: integer (nullable = false)\n",
      " |-- is_amount_anomaly: integer (nullable = false)\n",
      " |-- fraud_score: double (nullable = false)\n",
      " |-- is_fraud_prediction: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply stateful fraud detection using transformWithStateInPandas\n",
    "df_with_fraud_features = df_with_stateless_features \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .transformWithStateInPandas(\n",
    "        statefulProcessor=FraudDetectionFeaturesProcessor(),\n",
    "        outputStructType=get_fraud_detection_output_schema(),\n",
    "        outputMode=\"Update\",\n",
    "        timeMode=\"processingTime\"\n",
    "    )\n",
    "\n",
    "print(\"Stateful fraud detection configured\")\n",
    "print(\"\\nFinal schema with all features:\")\n",
    "df_with_fraud_features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4005cb7d-5a33-4066-9ed6-664bac2f7787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Write to Lakebase\n",
    "\n",
    "Stream all features to Lakebase for real-time serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149557d7-1590-4592-82e3-0f9bcb628195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming pipeline started!\n",
      "Query ID: 51b7ac8b-ddeb-453d-aa10-8208583c2212\n",
      "Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "\n",
      "Pipeline: Transactions → Stateless Features → Stateful Fraud Detection → Lakebase\n"
     ]
    }
   ],
   "source": [
    "# Define foreachBatch function to write to Lakebase PostgreSQL\n",
    "def write_to_lakebase(batch_df, batch_id):\n",
    "    \"\"\"Write each micro-batch to transaction_features table\"\"\"\n",
    "    lakebase.write_streaming_batch(batch_df, batch_id, \"transaction_features\")\n",
    "    logger.info(f\"Batch {batch_id} written to Lakebase\")\n",
    "\n",
    "# Start streaming query\n",
    "query = df_with_fraud_features \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(write_to_lakebase) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/fraud_pipeline_checkpoint\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming pipeline started!\")\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Status: {query.status}\")\n",
    "print(\"\\nPipeline flow:\")\n",
    "print(\"  Transactions → Stateless Features → Stateful Fraud Detection → Lakebase\")\n",
    "print(\"\\nWriting to:\")\n",
    "print(\"  Table: transaction_features\")\n",
    "print(\"  Trigger: 10 seconds\")\n",
    "print(\"  Checkpoint: /tmp/fraud_pipeline_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4007f7d-660c-417c-8864-334a2d16680d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Stop Streaming Query\n",
    "\n",
    "Stop the streaming pipeline when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551b6d01-d800-43b8-9608-7f52a3d63c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop streaming query\n",
    "if query.isActive:\n",
    "    query.stop()\n",
    "    print(\"Streaming query stopped\")\n",
    "\n",
    "print(\"\\nPipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_streaming_fraud_detection_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
