{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de73176-34e7-4145-8bf6-fc373b28fa8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Streaming Fraud Detection Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end streaming fraud detection pipeline that combines stateless and stateful features.\n",
    "\n",
    "## Features\n",
    "\n",
    "**Stateless Features (~40 columns):**\n",
    "- Time-based: hour, day, business hours, cyclical encodings\n",
    "- Amount-based: log, sqrt, categories, z-scores\n",
    "- Merchant: risk scores based on category\n",
    "- Location: risk indicators, region classification\n",
    "- Device: device type detection\n",
    "- Network: IP classification\n",
    "\n",
    "**Stateful Features (~15 columns):**\n",
    "- Velocity: transaction counts in time windows (10 min, 1 hour)\n",
    "- IP tracking: IP change detection and counts\n",
    "- Location anomalies: impossible travel detection (velocity > 800 km/h)\n",
    "- Amount anomalies: z-score calculation vs user history\n",
    "- Fraud scoring: composite 0-100 score with prediction flag\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Streaming Source (rate)\n",
    "    ↓\n",
    "Generate Transactions (synthetic data)\n",
    "    ↓\n",
    "Apply Stateless Features (AdvancedFeatureEngineering)\n",
    "    ↓\n",
    "Apply Stateful Fraud Detection (transformWithState)\n",
    "    ↓\n",
    "Write to Lakebase PostgreSQL in Realtime Mode (foreach)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Run `00_setup.ipynb` first to create `transaction_features` table\n",
    "- Databricks Runtime 17.3+ (with Spark 4.0+ for transformWithStateInPandas)\n",
    "- Lakebase PostgreSQL instance provisioned\n",
    "\n",
    "## Output\n",
    "\n",
    "All features (stateless + stateful) are written to:\n",
    "- **Table**: `transaction_features` (~20+ columns)\n",
    "- **Write latency**: 400ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f55e757-87f6-49ee-a744-90925d831089",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Spark Configs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "dbutils.library.restartPython()\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.providerClass\", \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\")\n",
    "spark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5925a17c-4c62-4400-a114-393d04803780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dbldatagen._version:Version : VersionInfo(major='0', minor='4', patch='0', release='', build='')\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Import utility modules\n",
    "from utils.data_generator import TransactionDataGenerator\n",
    "from utils.feature_engineering import (\n",
    "    AdvancedFeatureEngineering, \n",
    "    FraudDetectionFeaturesProcessor,\n",
    "    get_fraud_detection_output_schema\n",
    ")\n",
    "from utils.lakebase_client import LakebaseClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea342fd-087f-4418-91dc-55751a4da71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Configure Lakebase connection and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fabf4d6-fea9-435c-b92c-2bce2217bae5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LAKEBASE CONFIG"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Lakebase connection test successful\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Lakebase PostgreSQL\n\nVerifying transaction_features table...\n0.68.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.lakebase_client:Table stats: 865 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Table exists with 865 rows\n"
     ]
    }
   ],
   "source": [
    "# Lakebase connection configuration\n",
    "LAKEBASE_CONFIG = {\n",
    "    \"instance_name\": \"rtm-lakebase-demo\",\n",
    "    \"database\": \"databricks_postgres\"\n",
    "}\n",
    "\n",
    "# Initialize components\n",
    "data_gen = TransactionDataGenerator(spark)\n",
    "feature_engineer = AdvancedFeatureEngineering(spark)\n",
    "lakebase = LakebaseClient(**LAKEBASE_CONFIG)\n",
    "\n",
    "# Test Lakebase connection\n",
    "if lakebase.test_connection():\n",
    "    print(\"Connected to Lakebase PostgreSQL\")\n",
    "else:\n",
    "    raise Exception(\"Failed to connect to Lakebase\")\n",
    "\n",
    "# Verify transaction_features table exists\n",
    "print(\"\\nVerifying transaction_features table...\")\n",
    "try:\n",
    "    stats = lakebase.get_table_stats(\"transaction_features\")\n",
    "    print(f\"  Table exists with {stats['total_rows']:,} rows\")\n",
    "except Exception as e:\n",
    "    print(\"  Table not found. Please run 00_setup.ipynb first!\")\n",
    "    raise Exception(\"transaction_features table does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af261522-f8a3-446d-95c3-8ab010e4074c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2-A: Generate Streaming Transaction Data\n",
    "\n",
    "Create a streaming source that continuously generates synthetic transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a42abb-4b98-4730-b5c9-e8388343372a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data_generator:Creating synthetic transaction source with dbldatagen...\nINFO:utils.data_generator:   Rows: 1000\nINFO:utils.data_generator:   Users: 1000, Merchants: 1000\nINFO:utils.data_generator:Synthetic source created successfully\n"
     ]
    }
   ],
   "source": [
    "# Generate streaming transaction data\n",
    "df_transactions = data_gen.generate_transaction_data(\n",
    "    num_users=1000,           #1000 unique users\n",
    "    num_merchants=1000,      # 1000 unique merchants\n",
    "    rows_per_second=100      # 100 transactions per second\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab4ce8b-5aaf-4c9d-acb3-43417d972274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2-B: Write Stream Data to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5bb6a0-b78f-488d-bc54-f1febbbeb77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[STREAMING] Kafka producer started successfully\n  Query ID: 9b556d07-caa9-4f40-89af-04858efbb254\n  Query Name: None\n  Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n  Kafka Topic: transactions_source_stream\n  Checkpoint: /tmp/transaction-data-generator-checkpoint\n  Serialization: Protobuf (Confluent Schema Registry)\n\n"
     ]
    }
   ],
   "source": [
    "# Retrieve secrets from Databricks secret scope\n",
    "KAFKA_KEY = dbutils.secrets.get(scope = \"jaypalaniappan\", key = \"KAFKA_KEY\")\n",
    "KAFKA_SECRET = dbutils.secrets.get(scope = \"jaypalaniappan\", key = \"KAFKA_SECRET\")\n",
    "KAFKA_SERVER = dbutils.secrets.get(scope = \"jaypalaniappan\", key = \"KAFKA_SERVER\")\n",
    "KAFKA_TOPIC = \"transactions_source_stream\"\n",
    "\n",
    "#Generator Configuration\n",
    "CHECKPOINT_LOCATION = \"/tmp/transaction-data-generator-checkpoint\"\n",
    "dbutils.fs.rm(CHECKPOINT_LOCATION, True) #remove old checkpoint\n",
    "\n",
    "#Convert row to JSON String\n",
    "json_df = df_transactions.select(to_json(struct(*[col(c) for c in df_transactions.columns])).alias(\"value\"), col(\"transaction_id\").alias(\"key\"))\n",
    "\n",
    "kafkaWriter = (\n",
    "    json_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{KAFKA_KEY}' password='{KAFKA_SECRET}';\")\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"topic\", KAFKA_TOPIC)\n",
    "    .option(\"failOnDataLoss\", \"true\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_LOCATION)\n",
    ")\n",
    "\n",
    "kafkaQuery = kafkaWriter.start()\n",
    "\n",
    "print(f\"\"\"\n",
    "[STREAMING] Kafka producer started successfully\n",
    "  Query ID: {kafkaQuery.id}\n",
    "  Query Name: {kafkaQuery.name}\n",
    "  Status: {kafkaQuery.status}\n",
    "  Kafka Topic: {KAFKA_TOPIC}\n",
    "  Checkpoint: {CHECKPOINT_LOCATION}\n",
    "  Serialization: Protobuf (Confluent Schema Registry)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859d6274-372c-42f0-8aaa-6d14c9ae1cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3-A: Read Data from Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5514be69-e7cc-4ce2-8d4f-e78b74593273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "value_schema = \"STRUCT<transaction_id: STRING, user_id: STRING, merchant_id: STRING, amount: DOUBLE, currency: STRING, merchant_category: STRING, payment_method: STRING, ip_address: STRING, device_id: STRING, latitude: DOUBLE, longitude: DOUBLE, card_type: STRING, timestamp: TIMESTAMP>\"\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{KAFKA_KEY}' password='{KAFKA_SECRET}';\")\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")  \n",
    "    .load()\n",
    ") \\\n",
    ".select(\n",
    "    from_json(col(\"value\").cast(\"string\"), \n",
    "              value_schema).alias(\"value\"), \n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\")) \\\n",
    ".selectExpr(\"value.*\", \"kafka_timestamp\") \\\n",
    "  .drop(\"timestamp\") \\\n",
    "  .withColumnRenamed(\"kafka_timestamp\", \"timestamp\")\n",
    "\n",
    "#display(kafka_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228c0478-8d9e-494c-9f06-2d59f20ce5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3-B: Apply Stateless Features\n",
    "\n",
    "Apply time-based, amount-based, merchant, location, device, and network features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f35d8c-696a-458a-9a82-17c350209007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.feature_engineering:Applying streaming-compatible feature engineering...\nINFO:utils.feature_engineering:Creating time-based features...\nINFO:utils.feature_engineering:Creating amount-based features...\nINFO:utils.feature_engineering:Creating merchant features (streaming-only)...\nINFO:utils.feature_engineering:Skipping stateless location features (optimized out)\nINFO:utils.feature_engineering:Skipping device features (optimized out)\nINFO:utils.feature_engineering:Creating network features (streaming-only)...\nINFO:utils.feature_engineering:Streaming feature engineering completed!\n"
     ]
    }
   ],
   "source": [
    "df_with_stateless_features = feature_engineer.apply_all_features(kafka_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccb1c16-224a-4efc-be22-d98df03be063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Apply Stateful Features\n",
    "\n",
    "Use `transformWithStateInPandas` to maintain per-user state and detect fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f0fef-d09f-4c9a-8222-d45cd47ef79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply stateful fraud detection using transformWithState\n",
    "df_with_fraud_features = df_with_stateless_features \\\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .transformWithState(\n",
    "        statefulProcessor=FraudDetectionFeaturesProcessor(),\n",
    "        outputStructType=get_fraud_detection_output_schema(),\n",
    "        outputMode=\"Update\",\n",
    "        timeMode=\"processingTime\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4005cb7d-5a33-4066-9ed6-664bac2f7787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Write to Lakebase\n",
    "\n",
    "Stream all features to Lakebase for real-time serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33378362-5a0c-4c15-a106-e571c56c0d8f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clear Checkpoint Location"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm('/tmp/fraud_pipeline_checkpoint', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149557d7-1590-4592-82e3-0f9bcb628195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68.0\n"
     ]
    }
   ],
   "source": [
    "#Get Schema from dataframe\n",
    "table_schema = df_with_fraud_features.schema\n",
    "\n",
    "#Initialize lakebase writer\n",
    "lakebase_writer  = lakebase.get_foreach_writer(column_names=table_schema.names, batch_size=1)\n",
    "\n",
    "#Start streaming query\n",
    "query = df_with_fraud_features \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreach(lakebase_writer) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/fraud_pipeline_checkpoint\") \\\n",
    "    .trigger(realTime=\"5 minutes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4007f7d-660c-417c-8864-334a2d16680d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Stop Streaming Query\n",
    "\n",
    "Stop the streaming pipeline when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551b6d01-d800-43b8-9608-7f52a3d63c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Stop streaming query\n",
    "# if query.isActive:\n",
    "#     query.stop()\n",
    "#     print(\"Streaming query stopped\")\n",
    "\n",
    "# print(\"\\nPipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_streaming_fraud_detection_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
