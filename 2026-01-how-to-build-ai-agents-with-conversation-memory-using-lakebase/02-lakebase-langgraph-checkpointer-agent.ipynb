{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e411b194-dd96-4131-ab8a-5b8ba9914df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq databricks-langchain[memory] uv databricks-agents mlflow-skinny[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0550d280-c496-4a2e-9bdc-814a428dee43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec4cb9-6706-473c-8cce-ad05defa9f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mosaic AI Agent Framework: Author and deploy a Stateful Agent using Databricks Lakebase and LangGraph\n",
    "This notebook demonstrates how to build a stateful agent using the Mosaic AI Agent Framework and LangGraph, with Lakebase as the agent’s durable memory and checkpoint store. In this notebook, you will:\n",
    "1. Author a Stateful Agent graph with LakeBase (the new Postgres database in Databricks) and Langgraph to manage state using thread ids in a Databricks Agent \n",
    "2. Wrap the LangGraph agent with MLflow ChatAgent to ensure compatibility with Databricks features\n",
    "3. Test the agent's behavior locally\n",
    "4. Register model to Unity Catalog, log and deploy the agent for use in apps and Playground\n",
    "\n",
    "We use [PostgresSaver in Langgraph](https://api.python.langchain.com/en/latest/checkpoint/langchain_postgres.checkpoint.PostgresSaver.html) to open a connection with our Lakebase, pass it into the checkpoint and pass that into the LangGraph Agent\n",
    "\n",
    "## Why use Lakebase?\n",
    "Stateful agents need a place to persist, resume, and inspect their work. Lakebase provides a managed, UC-governed store for agent state:\n",
    "- Durable, resumable state. Automatically capture threads, intermediate checkpoints, tool outputs, and node state after each graph step—so you can resume, branch, or replay any point in time.\n",
    "- Queryable & observable. Because state lands in the Lakehouse, you can use SQL (or notebooks) to audit conversations and build upon other Databricks functionality like dashboards\n",
    "- Governed by Unity Catalog. Apply data permissions, lineage, and auditing to AI state, just like any other table.\n",
    "\n",
    "## What are Stateful Agents?\n",
    "Unlike stateless LLM calls, a stateful agent keeps and reuses context across steps and sessions. Each new conversation is tracked with a thread ID, which represents the logical task or dialogue stream. This way, you can pick up an existing thread and continue the conversation with your Agent.\n",
    "\n",
    "## Prerequisites\n",
    "- Create a Lakebase instance, see Databricks documentation ([AWS](https://docs.databricks.com/aws/en/oltp/create/) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/oltp/create/)). \n",
    "- You can create a Lakebase instance by going to SQL Warehouses -> Lakebase Postgres -> Create database instance. You will need to retrieve values from the \"Connection details\" section of your Lakebase to fill out this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3511ac-953a-4707-bea3-be4376f4ed42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"catalog\", defaultValue=\"bo_cheng_dnb_demos\", label=\"catalog\")\n",
    "dbutils.widgets.text(name=\"schema\", defaultValue=\"agents\", label=\"schema\")\n",
    "dbutils.widgets.text(name=\"model\", defaultValue=\"memory_agent\", label=\"model\")\n",
    "dbutils.widgets.text(\n",
    "    name=\"DATABRICKS_CLIENT_ID\", defaultValue=\"\", label=\"DATABRICKS_CLIENT_ID\"\n",
    ")\n",
    "dbutils.widgets.text(\n",
    "    name=\"DATABRICKS_CLIENT_SECRET\", defaultValue=\"\", label=\"DATABRICKS_CLIENT_SECRET\"\n",
    ")\n",
    "dbutils.widgets.text(name=\"secret_scope\", defaultValue=\"dbdemos\", label=\"secret_scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7a72f0-bc6e-489d-8ec3-f4d7ba4238aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_tiered_model_name='bo_cheng_dnb_demos.agents.memory_agent'\n"
     ]
    }
   ],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "model = dbutils.widgets.get(\"model\")\n",
    "# LLM_ENDPOINT = dbutils.widgets.get(\"foundation_model\")\n",
    "assert (\n",
    "    len(catalog) > 0 and len(schema) > 0 and len(model) > 0\n",
    "), \"Please provide a valid catalog, schema, and model name\"\n",
    "three_tiered_model_name = f\"{catalog}.{schema}.{model}\"\n",
    "print(f\"{three_tiered_model_name=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a036ef50-66ac-45ec-b678-64d4c3fb3a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing secret scope: dbdemos\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "DATABRICKS_HOST = w.config.host\n",
    "\n",
    "secret_scope_name = dbutils.widgets.get(\"secret_scope\")\n",
    "\n",
    "# if needed create a secret scope\n",
    "if secret_scope_name != \"dbdemos\":\n",
    "    w.secrets.create_scope(scope=secret_scope_name)\n",
    "else:\n",
    "    print(f\"Using existing secret scope: {secret_scope_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632ff6d6-902c-478e-8d6f-2c8e743b6d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no DATABRICKS_CLIENT_ID is provided\nno DATABRICKS_CLIENT_ID is provided\n"
     ]
    }
   ],
   "source": [
    "if dbutils.widgets.get(\"DATABRICKS_CLIENT_ID\") == \"\":\n",
    "    print(\"no DATABRICKS_CLIENT_ID is provided\")\n",
    "else:\n",
    "    w.secrets.put_secret(\n",
    "        scope=secret_scope_name,\n",
    "        key=\"DATABRICKS_CLIENT_ID\",\n",
    "        string_value=dbutils.widgets.get(\"DATABRICKS_CLIENT_ID\"),\n",
    "    )\n",
    "if dbutils.widgets.get(\"DATABRICKS_CLIENT_SECRET\") == \"\":\n",
    "    print(\"no DATABRICKS_CLIENT_ID is provided\")\n",
    "else:\n",
    "    w.secrets.put_secret(\n",
    "        scope=secret_scope_name,\n",
    "        key=\"DATABRICKS_CLIENT_SECRET\",\n",
    "        string_value=dbutils.widgets.get(\"DATABRICKS_CLIENT_SECRET\"),\n",
    "    )\n",
    "w.secrets.put_secret(\n",
    "    scope=secret_scope_name, key=\"DATABRICKS_HOST\", string_value=DATABRICKS_HOST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea052166-1d1a-44c3-9408-07a16b1bd28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"DATABRICKS_CLIENT_ID\"] = dbutils.secrets.get(\n",
    "#     scope=secret_scope_name, key=\"DATABRICKS_CLIENT_ID\"\n",
    "# )\n",
    "# os.environ[\"DATABRICKS_CLIENT_SECRET\"] = dbutils.secrets.get(\n",
    "#     scope=secret_scope_name, key=\"DATABRICKS_CLIENT_SECRET\"\n",
    "# )\n",
    "\n",
    "# os.unsetenv(\"DATABRICKS_CLIENT_ID\")\n",
    "# os.unsetenv(\"DATABRICKS_CLIENT_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bb5e31-3252-4dc6-92e9-7013f1e83d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakebase Config\n",
    "- Enable Postgres native role login\n",
    "- Might need to wait a few min for pg roles to apply\n",
    "- Create new catalog with PostgreSQL Database: `databricks_postgres` schema off lakebase instance for querying purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2caaac13-e6d5-4475-8ea9-b8cb3abd21ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checkpoint tables are ready.\n"
     ]
    }
   ],
   "source": [
    "# First-time checkpoint table setup\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import CheckpointSaver\n",
    "\n",
    "# --- TODO: Fill in Lakebase instance name ---\n",
    "INSTANCE_NAME = \"bo-test-lakebase-3\"\n",
    "\n",
    "# Create tables if missing\n",
    "with CheckpointSaver(instance_name=INSTANCE_NAME) as saver:\n",
    "    saver.setup()  # sets up checkpoint tables\n",
    "    print(\"✅ Checkpoint tables are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec24de0-62ab-4ac7-a7de-17b15cff0e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, TypedDict\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    CheckpointSaver,\n",
    ")\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-gpt-5-2\"\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "    You are an cybersecurity assistant.\n",
    "    You are given a task and you must complete it.\n",
    "    Use the following routine to support the customer.\n",
    "    # Routine:\n",
    "    1. Provide the get_cyber_threat_info tool the type of threat being asked about.\n",
    "    2. Use the source ip address provided in step 1 as input for the get_user_info tool to retrieve user specific info.\n",
    "    Use the following tools to complete the task:\n",
    "    {tools}\"\"\"\n",
    "\n",
    "############################################\n",
    "# Lakebase configuration\n",
    "############################################\n",
    "# TODO: Fill in Lakebase instance name\n",
    "LAKEBASE_INSTANCE_NAME = \"bo-test-lakebase-3\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent,enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/en/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "# Example UC tools; add your own as needed\n",
    "UC_TOOL_NAMES: list[str] = [\n",
    "    \"bo_cheng_dnb_demos.agents.get_cyber_threat_info\",\n",
    "    \"bo_cheng_dnb_demos.agents.get_user_info\",\n",
    "]\n",
    "if UC_TOOL_NAMES:\n",
    "    uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
    "    tools.extend(uc_toolkit.tools)\n",
    "\n",
    "# Use Databricks vector search indexes as tools\n",
    "# See https://docs.databricks.com/en/generative-ai/agent-framework/unstructured-retrieval-tools.html#locally-develop-vector-search-retriever-tools-with-ai-bridge\n",
    "# List to store vector search tool instances for unstructured retrieval.\n",
    "VECTOR_SEARCH_TOOLS = []\n",
    "\n",
    "# To add vector search retriever tools,\n",
    "# use VectorSearchRetrieverTool and create_tool_info,\n",
    "# then append the result to TOOL_INFOS.\n",
    "# Example:\n",
    "# VECTOR_SEARCH_TOOLS.append(\n",
    "#     VectorSearchRetrieverTool(\n",
    "#         index_name=\"\",\n",
    "#         # filters=\"...\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "tools.extend(VECTOR_SEARCH_TOOLS)\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    \"\"\"Stateful agent using ResponsesAgent with pooled Lakebase checkpointing.\"\"\"\n",
    "\n",
    "    def __init__(self, lakebase_config: dict[str, Any]):\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "\n",
    "        self.model = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "        self.system_prompt = SYSTEM_PROMPT\n",
    "        self.model_with_tools = self.model.bind_tools(tools) if tools else self.model\n",
    "\n",
    "    def _create_graph(self, checkpointer: Any):\n",
    "        def should_continue(state: AgentState):\n",
    "            messages = state[\"messages\"]\n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "                return \"continue\"\n",
    "            return \"end\"\n",
    "\n",
    "        preprocessor = (\n",
    "            RunnableLambda(\n",
    "                lambda state: [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "            if self.system_prompt\n",
    "            else RunnableLambda(lambda state: state[\"messages\"])\n",
    "        )\n",
    "        model_runnable = preprocessor | self.model_with_tools\n",
    "\n",
    "        def call_model(state: AgentState, config: RunnableConfig):\n",
    "            response = model_runnable.invoke(state, config)\n",
    "            return {\"messages\": [response]}\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "\n",
    "        if tools:\n",
    "            workflow.add_node(\"tools\", ToolNode(tools))\n",
    "            workflow.add_conditional_edges(\n",
    "                \"agent\", should_continue, {\"continue\": \"tools\", \"end\": END}\n",
    "            )\n",
    "            workflow.add_edge(\"tools\", \"agent\")\n",
    "        else:\n",
    "            workflow.add_edge(\"agent\", END)\n",
    "\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        return workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "    def _get_or_create_thread_id(self, request: ResponsesAgentRequest) -> str:\n",
    "        \"\"\"Get thread_id from request or create a new one.\n",
    "\n",
    "        Priority:\n",
    "        1. Use thread_id from custom_inputs if present\n",
    "        2. Use conversation_id from chat context if available\n",
    "        3. Generate a new UUID\n",
    "\n",
    "        Returns:\n",
    "            thread_id: The thread identifier to use for this conversation\n",
    "        \"\"\"\n",
    "        ci = dict(request.custom_inputs or {})\n",
    "\n",
    "        if \"thread_id\" in ci:\n",
    "            return ci[\"thread_id\"]\n",
    "\n",
    "        # using conversation id from chat context as thread id\n",
    "        # https://mlflow.org/docs/latest/api_reference/python_api/mlflow.types.html#mlflow.types.agent.ChatContext\n",
    "        if request.context and getattr(request.context, \"conversation_id\", None):\n",
    "            return request.context.conversation_id\n",
    "\n",
    "        # Generate new thread_id\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(\n",
    "            output=outputs, custom_outputs=request.custom_inputs\n",
    "        )\n",
    "\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        thread_id = self._get_or_create_thread_id(request)\n",
    "        ci = dict(request.custom_inputs or {})\n",
    "        ci[\"thread_id\"] = thread_id\n",
    "        request.custom_inputs = ci\n",
    "\n",
    "        # Convert incoming Responses messages to ChatCompletions format\n",
    "        # LangChain will automatically convert from ChatCompletions to LangChain format\n",
    "        cc_msgs = self.prep_msgs_for_cc_llm([i.model_dump() for i in request.input])\n",
    "        langchain_msgs = cc_msgs\n",
    "        checkpoint_config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        with CheckpointSaver(instance_name=LAKEBASE_INSTANCE_NAME) as checkpointer:\n",
    "            graph = self._create_graph(checkpointer)\n",
    "\n",
    "            for event in graph.stream(\n",
    "                {\"messages\": langchain_msgs},\n",
    "                checkpoint_config,\n",
    "                stream_mode=[\"updates\", \"messages\"],\n",
    "            ):\n",
    "                if event[0] == \"updates\":\n",
    "                    for node_data in event[1].values():\n",
    "                        if len(node_data.get(\"messages\", [])) > 0:\n",
    "                            yield from output_to_responses_items_stream(\n",
    "                                node_data[\"messages\"]\n",
    "                            )\n",
    "                elif event[0] == \"messages\":\n",
    "                    try:\n",
    "                        chunk = event[1][0]\n",
    "                        if isinstance(chunk, AIMessageChunk) and chunk.content:\n",
    "                            yield ResponsesAgentStreamEvent(\n",
    "                                **self.create_text_delta(\n",
    "                                    delta=chunk.content, item_id=chunk.id\n",
    "                                ),\n",
    "                            )\n",
    "                    except Exception as exc:\n",
    "                        logger.error(\"Error streaming chunk: %s\", exc)\n",
    "\n",
    "\n",
    "# ----- Export model -----\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphResponsesAgent(LAKEBASE_INSTANCE_NAME)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36beb42b-d89b-4bde-adde-2509bcc03028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unitycatalog.ai.core.base:Setting global UC Function client to DatabricksFunctionClient with default configuration.\nINFO:databricks_ai_bridge.lakebase:lakebase pool ready: host=instance-cd00746e-b544-45c8-9f08-5062a0858c7d.database.cloud.databricks.com db=databricks_postgres min=1 max=10 cache=3000s\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 19:30:07,745 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 19:30:07,745 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 19:30:07,763 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 19:30:07,763 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 19:30:07,767 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 19:30:07,767 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 19:30:08,148 5046 INFO to_table Executing plan <Truncated message due to truncation error>\n2026-01-08 19:30:08,148 5046 INFO to_table Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2026-01-08 19:30:08,153 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 19:30:08,153 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 19:30:08,156 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 19:30:08,156 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 19:30:11,102 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 19:30:11,102 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 19:30:11,109 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 19:30:11,109 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 19:30:11,111 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 19:30:11,111 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 19:30:11,259 5046 INFO to_table Executing plan <Truncated message due to truncation error>\n2026-01-08 19:30:11,259 5046 INFO to_table Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2026-01-08 19:30:11,262 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 19:30:11,262 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 19:30:11,265 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 19:30:11,265 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'response', 'output': [{'type': 'function_call', 'id': 'lc_run--019b9f16-7000-78e3-bd78-94983030ded9', 'call_id': 'call_leoNewTFtEi4QQ8oBaDiPmV4', 'name': 'bo_cheng_dnb_demos__agents__get_cyber_threat_info', 'arguments': '{\"threat_type\": \"malware\"}'}, {'type': 'function_call_output', 'call_id': 'call_leoNewTFtEi4QQ8oBaDiPmV4', 'output': '{\"format\": \"SCALAR\", \"value\": \"Threat ID: 8, Timestamp: 2025-09-08 02:35:06.534731, Source IP: 192.168.1.21, Protocol: TCP, Detection Tool: IDS\"}'}, {'type': 'function_call', 'id': 'lc_run--019b9f16-7e87-7fb0-9c8b-4ab1df32bef2', 'call_id': 'call_9LUc1bpclZXHLhAYxxSe2K1F', 'name': 'bo_cheng_dnb_demos__agents__get_user_info', 'arguments': '{\"source_ip\": \"192.168.1.21\"}'}, {'type': 'function_call_output', 'call_id': 'call_9LUc1bpclZXHLhAYxxSe2K1F', 'output': '{\"format\": \"SCALAR\", \"value\": \"Username: George Miller, Department: Finance, Email: george.miller@corp.com, IP Address: 192.168.1.21, Location: New York\"}'}, {'type': 'message', 'id': 'lc_run--019b9f16-84a2-7940-82ac-a196699bfb48', 'content': [{'text': 'The latest **malware** threat (Threat ID **8**, detected **2025-09-08 02:35:06.534731** via **IDS**) originated from source IP **192.168.1.21**, which maps to:\\n\\n- **User:** George Miller  \\n- **Department:** Finance  \\n- **Email:** george.miller@corp.com  \\n- **Location:** New York', 'type': 'output_text'}], 'role': 'assistant'}], 'custom_outputs': {'thread_id': '522eba5f-4b99-48d6-941b-015f6b9727ab'}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-173a315b8e4eb871ce4d27585d76b1ee\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-173a315b8e4eb871ce4d27585d76b1ee)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "result = AGENT.predict(\n",
    "    {\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Who committed the latest malware threat?\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(result.model_dump(exclude_none=True))\n",
    "thread_id = result.custom_outputs[\"thread_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d011adc9-c127-4a57-b805-d3172c65d995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databricks_ai_bridge.lakebase:lakebase pool ready: host=instance-cd00746e-b544-45c8-9f08-5062a0858c7d.database.cloud.databricks.com db=databricks_postgres min=1 max=10 cache=3000s\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2: {'object': 'response', 'output': [{'type': 'message', 'id': 'lc_run--019b9f69-a8c5-7bc3-ab19-f08795eddbd5', 'content': [{'text': '**George Miller** was just mentioned.', 'type': 'output_text'}], 'role': 'assistant'}], 'custom_outputs': {'thread_id': '522eba5f-4b99-48d6-941b-015f6b9727ab'}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-70343c2f84828c48f898418c1d65de3e\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-70343c2f84828c48f898418c1d65de3e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Message 2, include thread ID and notice how agent remembers context from previous predict message\n",
    "response2 = AGENT.predict(\n",
    "    {\n",
    "        \"input\": [{\"role\": \"user\", \"content\": \"Who was just mentioned?\"}],\n",
    "        \"custom_inputs\": {\"thread_id\": thread_id},\n",
    "    }\n",
    ")\n",
    "print(\"Response 2:\", response2.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fef865-8ca5-4d0f-83a6-657b3edccf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 View Logged Model at: https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/f930eaa2963d4e668ba4e4cb275dd25e/models/m-e503f6a894044bf1bab4f00f7e8fb5aa?o=1444828305810485\n2026/01/08 20:29:41 INFO mlflow.pyfunc: Predicting on input example to validate output\n2026/01/08 20:29:41 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\nINFO:databricks_ai_bridge.lakebase:lakebase pool ready: host=instance-cd00746e-b544-45c8-9f08-5062a0858c7d.database.cloud.databricks.com db=databricks_postgres min=1 max=10 cache=3000s\n2026/01/08 20:29:42 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 20:29:43,991 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:43,991 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:43,999 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:43,999 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:44,001 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:44,001 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 20:29:44,052 5046 ERROR _handle_rpc_error GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1726, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"<frozen _collections_abc>\", line 330, in __next__\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 139, in send\n    if not self._has_next():\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 200, in _has_next\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 172, in _has_next\n    self._current = self._call_iter(\n                    ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 297, in _call_iter\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 277, in _call_iter\n    return iter_fun()\n           ^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n    return self._next()\n           ^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=54689871-917b-4fe1-a9dc-af496126a68a, reason=INACTIVITY_TIMEOUT]. (requestId=be46e4b9-fa08-4a17-837a-96f9163980b8)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=54689871-917b-4fe1-a9dc-af496126a68a, reason=INACTIVITY_TIMEOUT]. (requestId=be46e4b9-fa08-4a17-837a-96f9163980b8)\", grpc_status:9, created_time:\"2026-01-08T20:29:44.051558111+00:00\"}\"\n>\n2026-01-08 20:29:44,052 5046 ERROR _handle_rpc_error GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1726, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"<frozen _collections_abc>\", line 330, in __next__\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 139, in send\n    if not self._has_next():\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 200, in _has_next\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 172, in _has_next\n    self._current = self._call_iter(\n                    ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 297, in _call_iter\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 277, in _call_iter\n    return iter_fun()\n           ^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n    return self._next()\n           ^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=54689871-917b-4fe1-a9dc-af496126a68a, reason=INACTIVITY_TIMEOUT]. (requestId=be46e4b9-fa08-4a17-837a-96f9163980b8)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=54689871-917b-4fe1-a9dc-af496126a68a, reason=INACTIVITY_TIMEOUT]. (requestId=be46e4b9-fa08-4a17-837a-96f9163980b8)\", grpc_status:9, created_time:\"2026-01-08T20:29:44.051558111+00:00\"}\"\n>\nERROR:pyspark.sql.connect.client.logging:GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1726, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"<frozen _collections_abc>\", line 330, in __next__\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 139, in send\n    if not self._has_next():\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 200, in _has_next\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 172, in _has_next\n    self._current = self._call_iter(\n                    ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 297, in _call_iter\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 277, in _call_iter\n    return iter_fun()\n           ^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n    return self._next()\n           ^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=54689871-917b-4fe1-a9dc-af496126a68a, reason=INACTIVITY_TIMEOUT]. (requestId=be46e4b9-fa08-4a17-837a-96f9163980b8)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=54689871-917b-4fe1-a9dc-af496126a68a, reason=INACTIVITY_TIMEOUT]. (requestId=be46e4b9-fa08-4a17-837a-96f9163980b8)\", grpc_status:9, created_time:\"2026-01-08T20:29:44.051558111+00:00\"}\"\n>\nWARNING:unitycatalog.ai.core.utils.retry_utils:Session expired. Retrying attempt 1 of 5. Refreshing session and retrying after 1 seconds...\nINFO:unitycatalog.ai.core.databricks:Refreshing Databricks client and Spark session due to session expiration.\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 20:29:45,142 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:45,142 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:45,145 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:45,145 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:45,146 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:45,146 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 20:29:45,522 5046 INFO to_table Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:45,522 5046 INFO to_table Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:45,525 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:45,525 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:45,527 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:45,527 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 20:29:47,184 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:47,184 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:47,187 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:47,187 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:47,188 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:47,188 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 20:29:47,327 5046 INFO to_table Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:47,327 5046 INFO to_table Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:47,330 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:47,330 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:47,332 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:47,332 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n2026/01/08 20:29:54 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\nINFO:databricks_ai_bridge.lakebase:lakebase pool ready: host=instance-cd00746e-b544-45c8-9f08-5062a0858c7d.database.cloud.databricks.com db=databricks_postgres min=1 max=10 cache=3000s\n2026/01/08 20:29:55 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 20:29:56,550 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:56,550 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:56,561 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:56,561 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:56,562 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:56,562 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 20:29:56,696 5046 INFO to_table Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:56,696 5046 INFO to_table Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:56,701 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:56,701 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:56,702 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:56,702 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\n2026-01-08 20:29:57,699 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:57,699 5046 INFO execute_command Execute command for command <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Execute command for command <Truncated message due to truncation error>\n2026-01-08 20:29:57,702 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:57,702 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:57,704 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:57,704 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2026-01-08 20:29:57,835 5046 INFO to_table Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:57,835 5046 INFO to_table Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2026-01-08 20:29:57,839 5046 INFO _execute_and_fetch ExecuteAndFetch\n2026-01-08 20:29:57,839 5046 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2026-01-08 20:29:57,840 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2026-01-08 20:29:57,840 5046 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import mlflow\n",
    "from databricks_langchain import VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksFunction,\n",
    "    DatabricksServingEndpoint,\n",
    "    DatabricksLakebase,\n",
    "    DatabricksVectorSearchIndex,\n",
    ")  # we are adding DatabricksLakebase resource type\n",
    "from mlflow.models.auth_policy import AuthPolicy, SystemAuthPolicy, UserAuthPolicy\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "from agent import LLM_ENDPOINT_NAME, LAKEBASE_INSTANCE_NAME, tools\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# TODO: Manually include additional underlying resources if needed and update values for endpoint/lakebase\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME),\n",
    "    DatabricksLakebase(database_instance_name=LAKEBASE_INSTANCE_NAME),\n",
    "]\n",
    "for tool in tools:\n",
    "    if isinstance(tool, VectorSearchRetrieverTool):\n",
    "        resources.extend(tool.resources)\n",
    "    elif isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "# System policy: resources accessed with system credentials\n",
    "system_policy = SystemAuthPolicy(resources=resources)\n",
    "\n",
    "# User policy: API scopes for OBO access\n",
    "api_scopes = [\n",
    "    \"sql.statement-execution\",\n",
    "    \"mcp.genie\",\n",
    "    \"mcp.external\",\n",
    "    \"catalog.connections\",\n",
    "    \"mcp.vectorsearch\",\n",
    "    \"vectorsearch.vector-search-indexes\",\n",
    "    \"iam.current-user:read\",\n",
    "    \"sql.warehouses\",\n",
    "    \"dashboards.genie\",\n",
    "    \"serving.serving-endpoints\",\n",
    "    \"iam.access-control:read\",\n",
    "    \"apps.apps\",\n",
    "    \"mcp.functions\",\n",
    "    \"vectorsearch.vector-search-endpoints\",\n",
    "]\n",
    "user_policy = UserAuthPolicy(api_scopes=api_scopes)\n",
    "\n",
    "input_example = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"What is an LLM agent?\"}],\n",
    "    \"custom_inputs\": {\"thread_id\": \"example-thread-123\"},\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        pip_requirements=[\n",
    "            f\"databricks-langchain[memory]=={get_distribution('databricks-langchain[memory]').version}\",\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a986456a-f881-46e3-99fe-99b0ef7758ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228308fcfee745a48056fcf07cc45345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 20:30:14,681 5046 INFO Falling back to download via Files API: Unsuccessful download. Response status: 404, body: b'{\\n  \"error_code\" : \"NOT_FOUND\",\\n  \"message\" : \"The file being accessed is not found.\",\\n  \"details\" : [ {\\n    \"@type\" : \"type.googleapis.com/google.rpc.ErrorInfo\",\\n    \"reason\" : \"FILES_API_FILE_NOT_FOUND\",\\n    \"domain\" : \"filesystem.databricks.com\"\\n  } ]\\n}'\nINFO:databricks.sdk.mixins.files:Falling back to download via Files API: Unsuccessful download. Response status: 404, body: b'{\\n  \"error_code\" : \"NOT_FOUND\",\\n  \"message\" : \"The file being accessed is not found.\",\\n  \"details\" : [ {\\n    \"@type\" : \"type.googleapis.com/google.rpc.ErrorInfo\",\\n    \"reason\" : \"FILES_API_FILE_NOT_FOUND\",\\n    \"domain\" : \"filesystem.databricks.com\"\\n  } ]\\n}'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d076c3fcb6234908b516d46fcdf78e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 20:30:17 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c14d8bc7b8042cd98c7e9d36df76fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 20:30:17,829 5046 INFO Falling back to download via Files API: Unsuccessful download. Response status: 404, body: b'{\\n  \"error_code\" : \"NOT_FOUND\",\\n  \"message\" : \"The file being accessed is not found.\",\\n  \"details\" : [ {\\n    \"@type\" : \"type.googleapis.com/google.rpc.ErrorInfo\",\\n    \"reason\" : \"FILES_API_FILE_NOT_FOUND\",\\n    \"domain\" : \"filesystem.databricks.com\"\\n  } ]\\n}'\nINFO:databricks.sdk.mixins.files:Falling back to download via Files API: Unsuccessful download. Response status: 404, body: b'{\\n  \"error_code\" : \"NOT_FOUND\",\\n  \"message\" : \"The file being accessed is not found.\",\\n  \"details\" : [ {\\n    \"@type\" : \"type.googleapis.com/google.rpc.ErrorInfo\",\\n    \"reason\" : \"FILES_API_FILE_NOT_FOUND\",\\n    \"domain\" : \"filesystem.databricks.com\"\\n  } ]\\n}'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36809f7cfc964108a64a5b29d9f1a027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 20:30:24 INFO mlflow.utils.virtualenv: Creating a new environment in /tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275 with python version 3.11.10 using uv\nUsing CPython 3.11.10 interpreter at: \u001B[36m/usr/bin/python3.11\u001B[39m\nCreating virtual environment at: \u001B[36m/tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275\u001B[39m\nActivate with: \u001B[32msource /tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275/bin/activate\u001B[39m\n2026/01/08 20:30:24 INFO mlflow.utils.virtualenv: Installing dependencies\n\u001B[2mUsing Python 3.11.10 environment at: /tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275\u001B[0m\n\u001B[2mResolved \u001B[1m3 packages\u001B[0m \u001B[2min 86ms\u001B[0m\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m pip \u001B[2m(1.7MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m setuptools \u001B[2m(1.2MiB)\u001B[0m\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m pip\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m setuptools\n\u001B[2mPrepared \u001B[1m3 packages\u001B[0m \u001B[2min 122ms\u001B[0m\u001B[0m\n\u001B[2mInstalled \u001B[1m3 packages\u001B[0m \u001B[2min 16ms\u001B[0m\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpip\u001B[0m\u001B[2m==24.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msetuptools\u001B[0m\u001B[2m==75.1.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mwheel\u001B[0m\u001B[2m==0.38.4\u001B[0m\n\u001B[2mUsing Python 3.11.10 environment at: /tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275\u001B[0m\n\u001B[2mResolved \u001B[1m151 packages\u001B[0m \u001B[2min 1.11s\u001B[0m\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m mlflow \u001B[2m(8.6MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m mlflow-skinny \u001B[2m(2.4MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m pydantic-core \u001B[2m(2.0MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m mlflow-tracing \u001B[2m(1.3MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m pandas \u001B[2m(12.2MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m zstandard \u001B[2m(5.3MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m cryptography \u001B[2m(4.3MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m kiwisolver \u001B[2m(1.4MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m sqlalchemy \u001B[2m(3.2MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m tiktoken \u001B[2m(1.1MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m pillow \u001B[2m(6.7MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m numpy \u001B[2m(17.4MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m databricks-connect \u001B[2m(2.3MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m scikit-learn \u001B[2m(8.7MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m grpcio \u001B[2m(6.3MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m langchain-community \u001B[2m(2.4MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m pyarrow \u001B[2m(45.5MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m fonttools \u001B[2m(4.8MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m aiohttp \u001B[2m(1.7MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m openai \u001B[2m(1.0MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m scipy \u001B[2m(34.2MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m matplotlib \u001B[2m(8.3MiB)\u001B[0m\n\u001B[36m\u001B[1mDownloading\u001B[0m\u001B[39m psycopg-binary \u001B[2m(4.9MiB)\u001B[0m\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m tiktoken\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m kiwisolver\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m aiohttp\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m mlflow-tracing\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m pydantic-core\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m databricks-connect\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m sqlalchemy\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m cryptography\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m psycopg-binary\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m zstandard\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m fonttools\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m mlflow-skinny\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m openai\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m grpcio\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m pillow\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m langchain-community\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m scikit-learn\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m matplotlib\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m mlflow\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m numpy\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m scipy\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m pyarrow\n \u001B[36m\u001B[1mDownloaded\u001B[0m\u001B[39m pandas\n\u001B[2mPrepared \u001B[1m150 packages\u001B[0m \u001B[2min 4.96s\u001B[0m\u001B[0m\n\u001B[2mInstalled \u001B[1m150 packages\u001B[0m \u001B[2min 408ms\u001B[0m\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1maiohappyeyeballs\u001B[0m\u001B[2m==2.6.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1maiohttp\u001B[0m\u001B[2m==3.13.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1maiohttp-retry\u001B[0m\u001B[2m==2.9.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1maiosignal\u001B[0m\u001B[2m==1.4.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1malembic\u001B[0m\u001B[2m==1.17.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mannotated-doc\u001B[0m\u001B[2m==0.0.4\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mannotated-types\u001B[0m\u001B[2m==0.7.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1manyio\u001B[0m\u001B[2m==4.12.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mattrs\u001B[0m\u001B[2m==25.4.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mblinker\u001B[0m\u001B[2m==1.9.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcachetools\u001B[0m\u001B[2m==6.2.4\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcertifi\u001B[0m\u001B[2m==2026.1.4\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcffi\u001B[0m\u001B[2m==2.0.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcharset-normalizer\u001B[0m\u001B[2m==3.4.4\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mclick\u001B[0m\u001B[2m==8.3.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcloudpickle\u001B[0m\u001B[2m==3.1.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcontourpy\u001B[0m\u001B[2m==1.3.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcryptography\u001B[0m\u001B[2m==46.0.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mcycler\u001B[0m\u001B[2m==0.12.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdatabricks-ai-bridge\u001B[0m\u001B[2m==0.11.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdatabricks-connect\u001B[0m\u001B[2m==16.1.7\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdatabricks-langchain\u001B[0m\u001B[2m==0.12.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdatabricks-mcp\u001B[0m\u001B[2m==0.5.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdatabricks-sdk\u001B[0m\u001B[2m==0.77.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdatabricks-vectorsearch\u001B[0m\u001B[2m==0.63\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdataclasses-json\u001B[0m\u001B[2m==0.6.7\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdeprecation\u001B[0m\u001B[2m==2.1.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdistro\u001B[0m\u001B[2m==1.9.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mdocker\u001B[0m\u001B[2m==7.1.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mfastapi\u001B[0m\u001B[2m==0.128.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mflask\u001B[0m\u001B[2m==3.1.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mflask-cors\u001B[0m\u001B[2m==6.0.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mfonttools\u001B[0m\u001B[2m==4.61.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mfrozenlist\u001B[0m\u001B[2m==1.8.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgitdb\u001B[0m\u001B[2m==4.0.12\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgitpython\u001B[0m\u001B[2m==3.1.46\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgoogle-auth\u001B[0m\u001B[2m==2.47.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgoogleapis-common-protos\u001B[0m\u001B[2m==1.72.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgraphene\u001B[0m\u001B[2m==3.4.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgraphql-core\u001B[0m\u001B[2m==3.2.7\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgraphql-relay\u001B[0m\u001B[2m==3.2.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgreenlet\u001B[0m\u001B[2m==3.3.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgrpcio\u001B[0m\u001B[2m==1.76.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgrpcio-status\u001B[0m\u001B[2m==1.76.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mgunicorn\u001B[0m\u001B[2m==23.0.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mh11\u001B[0m\u001B[2m==0.16.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mhttpcore\u001B[0m\u001B[2m==1.0.9\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mhttpx\u001B[0m\u001B[2m==0.28.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mhttpx-sse\u001B[0m\u001B[2m==0.4.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mhuey\u001B[0m\u001B[2m==2.6.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1midna\u001B[0m\u001B[2m==3.11\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mimportlib-metadata\u001B[0m\u001B[2m==8.7.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mitsdangerous\u001B[0m\u001B[2m==2.2.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjinja2\u001B[0m\u001B[2m==3.1.6\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjiter\u001B[0m\u001B[2m==0.12.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjoblib\u001B[0m\u001B[2m==1.5.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjsonpatch\u001B[0m\u001B[2m==1.33\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjsonpointer\u001B[0m\u001B[2m==3.0.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjsonschema\u001B[0m\u001B[2m==4.26.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mjsonschema-specifications\u001B[0m\u001B[2m==2025.9.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mkiwisolver\u001B[0m\u001B[2m==1.4.9\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangchain\u001B[0m\u001B[2m==1.2.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangchain-classic\u001B[0m\u001B[2m==1.0.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangchain-community\u001B[0m\u001B[2m==0.4.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangchain-core\u001B[0m\u001B[2m==1.2.6\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangchain-mcp-adapters\u001B[0m\u001B[2m==0.2.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangchain-text-splitters\u001B[0m\u001B[2m==1.1.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlanggraph\u001B[0m\u001B[2m==1.0.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlanggraph-checkpoint\u001B[0m\u001B[2m==3.0.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlanggraph-checkpoint-postgres\u001B[0m\u001B[2m==3.0.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlanggraph-prebuilt\u001B[0m\u001B[2m==1.0.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlanggraph-sdk\u001B[0m\u001B[2m==0.3.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mlangsmith\u001B[0m\u001B[2m==0.6.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmako\u001B[0m\u001B[2m==1.3.10\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmarkupsafe\u001B[0m\u001B[2m==3.0.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmarshmallow\u001B[0m\u001B[2m==3.26.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmatplotlib\u001B[0m\u001B[2m==3.10.8\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmcp\u001B[0m\u001B[2m==1.25.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmlflow\u001B[0m\u001B[2m==3.8.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmlflow-skinny\u001B[0m\u001B[2m==3.8.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmlflow-tracing\u001B[0m\u001B[2m==3.8.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmultidict\u001B[0m\u001B[2m==6.7.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mmypy-extensions\u001B[0m\u001B[2m==1.1.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mnest-asyncio\u001B[0m\u001B[2m==1.6.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mnumpy\u001B[0m\u001B[2m==1.26.4\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mopenai\u001B[0m\u001B[2m==2.14.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mopentelemetry-api\u001B[0m\u001B[2m==1.39.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mopentelemetry-proto\u001B[0m\u001B[2m==1.39.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mopentelemetry-sdk\u001B[0m\u001B[2m==1.39.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mopentelemetry-semantic-conventions\u001B[0m\u001B[2m==0.60b1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1morjson\u001B[0m\u001B[2m==3.11.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mormsgpack\u001B[0m\u001B[2m==1.12.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpackaging\u001B[0m\u001B[2m==25.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpandas\u001B[0m\u001B[2m==2.3.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpillow\u001B[0m\u001B[2m==12.1.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpropcache\u001B[0m\u001B[2m==0.4.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mprotobuf\u001B[0m\u001B[2m==6.33.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpsycopg\u001B[0m\u001B[2m==3.3.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpsycopg-binary\u001B[0m\u001B[2m==3.3.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpsycopg-pool\u001B[0m\u001B[2m==3.3.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpy4j\u001B[0m\u001B[2m==0.10.9.7\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpyarrow\u001B[0m\u001B[2m==22.0.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpyasn1\u001B[0m\u001B[2m==0.6.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpyasn1-modules\u001B[0m\u001B[2m==0.4.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpycparser\u001B[0m\u001B[2m==2.23\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpydantic\u001B[0m\u001B[2m==2.12.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpydantic-core\u001B[0m\u001B[2m==2.41.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpydantic-settings\u001B[0m\u001B[2m==2.12.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpyjwt\u001B[0m\u001B[2m==2.10.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpyparsing\u001B[0m\u001B[2m==3.3.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpython-dateutil\u001B[0m\u001B[2m==2.9.0.post0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpython-dotenv\u001B[0m\u001B[2m==1.2.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpython-multipart\u001B[0m\u001B[2m==0.0.21\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpytz\u001B[0m\u001B[2m==2025.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mpyyaml\u001B[0m\u001B[2m==6.0.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mreferencing\u001B[0m\u001B[2m==0.37.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mregex\u001B[0m\u001B[2m==2025.11.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mrequests\u001B[0m\u001B[2m==2.32.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mrequests-toolbelt\u001B[0m\u001B[2m==1.0.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mrpds-py\u001B[0m\u001B[2m==0.30.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mrsa\u001B[0m\u001B[2m==4.9.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mscikit-learn\u001B[0m\u001B[2m==1.8.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mscipy\u001B[0m\u001B[2m==1.16.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msix\u001B[0m\u001B[2m==1.17.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msmmap\u001B[0m\u001B[2m==5.0.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msniffio\u001B[0m\u001B[2m==1.3.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msqlalchemy\u001B[0m\u001B[2m==2.0.45\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msqlparse\u001B[0m\u001B[2m==0.5.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1msse-starlette\u001B[0m\u001B[2m==3.1.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mstarlette\u001B[0m\u001B[2m==0.50.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtabulate\u001B[0m\u001B[2m==0.9.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtenacity\u001B[0m\u001B[2m==9.1.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mthreadpoolctl\u001B[0m\u001B[2m==3.6.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtiktoken\u001B[0m\u001B[2m==0.12.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtqdm\u001B[0m\u001B[2m==4.67.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtyping-extensions\u001B[0m\u001B[2m==4.15.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtyping-inspect\u001B[0m\u001B[2m==0.9.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtyping-inspection\u001B[0m\u001B[2m==0.4.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mtzdata\u001B[0m\u001B[2m==2025.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1munitycatalog-ai\u001B[0m\u001B[2m==0.3.2\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1munitycatalog-client\u001B[0m\u001B[2m==0.3.1\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1munitycatalog-langchain\u001B[0m\u001B[2m==0.3.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1murllib3\u001B[0m\u001B[2m==2.6.3\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1muuid-utils\u001B[0m\u001B[2m==0.13.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1muvicorn\u001B[0m\u001B[2m==0.40.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mwerkzeug\u001B[0m\u001B[2m==3.1.5\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mxxhash\u001B[0m\u001B[2m==3.6.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1myarl\u001B[0m\u001B[2m==1.22.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mzipp\u001B[0m\u001B[2m==3.23.0\u001B[0m\n \u001B[32m+\u001B[39m \u001B[1mzstandard\u001B[0m\u001B[2m==0.25.0\u001B[0m\n2026/01/08 20:30:31 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275/bin/activate && python -c \"\"']'\n2026/01/08 20:30:31 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /tmp/virtualenv_envs/mlflow-566d5b5bb65661f5ec3ac5733acc4ffc8caad275/bin/activate && python /local_disk0/.ephemeral_nfs/envs/pythonEnv-a496fdac-f50d-4727-9ae7-0d54fb6b5702/lib/python3.11/site-packages/mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py --model-uri file:///local_disk0/user_tmp_data/spark-a496fdac-f50d-4727-9ae7-0d/tmpokrjnbud/agent --content-type json --input-path /local_disk0/user_tmp_data/spark-a496fdac-f50d-4727-9ae7-0d/tmpch3i4ra5/input.json']'\nINFO:unitycatalog.ai.core.base:Setting global UC Function client to DatabricksFunctionClient with default configuration.\nINFO:databricks_ai_bridge.lakebase:lakebase pool ready: host=instance-cd00746e-b544-45c8-9f08-5062a0858c7d.database.cloud.databricks.com db=databricks_postgres min=1 max=10 cache=3000s\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:unitycatalog.ai.core.databricks:Using databricks connect to execute functions with serverless compute.\nINFO:unitycatalog.ai.core.utils.retry_utils:Successfully re-acquired connection to a serverless instance.\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\": \"response\", \"output\": [{\"type\": \"function_call\", \"id\": \"lc_run--019b9f4d-efcf-71a3-a9af-c42220f1735d\", \"call_id\": \"call_fojVbEI3kqY3F7xOJxkXXJwH\", \"name\": \"bo_cheng_dnb_demos__agents__get_cyber_threat_info\", \"arguments\": \"{\\\"threat_type\\\": \\\"LLM agent\\\"}\"}, {\"type\": \"function_call_output\", \"call_id\": \"call_fojVbEI3kqY3F7xOJxkXXJwH\", \"output\": \"{\\\"format\\\": \\\"SCALAR\\\", \\\"value\\\": \\\"Threat ID: 8, Timestamp: 2025-09-08 02:35:06.534731, Source IP: 192.168.1.21, Protocol: TCP, Detection Tool: IDS\\\"}\"}, {\"type\": \"function_call\", \"id\": \"lc_run--019b9f4d-f77d-7a00-984d-c4e65d1fa220\", \"call_id\": \"call_eS73VboEqVKtpTxZX7zuua8t\", \"name\": \"bo_cheng_dnb_demos__agents__get_user_info\", \"arguments\": \"{\\\"source_ip\\\": \\\"192.168.1.21\\\"}\"}, {\"type\": \"function_call_output\", \"call_id\": \"call_eS73VboEqVKtpTxZX7zuua8t\", \"output\": \"{\\\"format\\\": \\\"SCALAR\\\", \\\"value\\\": \\\"Username: George Miller, Department: Finance, Email: george.miller@corp.com, IP Address: 192.168.1.21, Location: New York\\\"}\"}, {\"type\": \"message\", \"id\": \"lc_run--019b9f4d-fef0-7723-84a4-6609fa737532\", \"content\": [{\"text\": \"An **LLM agent** is a system that uses a **large language model** to **carry out tasks autonomously or semi-autonomously** by *planning*, *taking actions*, and *adapting based on results*\\u2014not just answering with text.\\n\\n**In practice, an LLM agent:**\\n- breaks a goal into steps (plan),\\n- uses **tools/APIs** (search, databases, code execution, ticketing, etc.),\\n- reads the tool outputs (observe),\\n- repeats until the task is done.\\n\\n**Example:** \\u201cInvestigate this phishing email\\u201d \\u2192 the agent extracts IOCs, checks reputation services, searches internal logs, and drafts a response recommendation.\\n\\n(For internal context, a security event tagged \\u201cLLM agent\\u201d was observed from **192.168.1.21** associated with **George Miller (Finance, New York)**; if you\\u2019re asking due to that alert, I can suggest what to validate next.)\", \"type\": \"output_text\"}], \"role\": \"assistant\"}], \"custom_outputs\": {\"thread_id\": \"example-thread-123\"}}"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 20:30:52 INFO mlflow.tracing.export.async_export_queue: Flushing the async trace logging queue before program exit. This may take a while...\n"
     ]
    }
   ],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data=input_example,\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc29adc0-2738-4fbb-8e0f-92e93bd50617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'bo_cheng_dnb_demos.agents.memory_agent' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c60ccf32ab4230ba4753ad3ec19f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac70a04b0af4ef0875bf0b80a554f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Closing down clientserver connection\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:py4j.clientserver:Closing down clientserver connection\n\uD83D\uDD17 Created version '37' of model 'bo_cheng_dnb_demos.agents.memory_agent': https://e2-demo-field-eng.cloud.databricks.com/explore/data/models/bo_cheng_dnb_demos/agents/memory_agent/version/37?o=1444828305810485\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri,\n",
    "    name=UC_MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be4813e-2d48-4dff-b900-d9d7c9d2d013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc82c13f2ff48fe885509c5382843f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Deployment of bo_cheng_dnb_demos.agents.memory_agent version 37 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n\n    View status: https://e2-demo-field-eng.cloud.databricks.com/ml/endpoints/agents_bo_cheng_dnb_demos-agents-memory_agent/?o=1444828305810485\n    Review App: https://e2-demo-field-eng.cloud.databricks.com/ml/review-v2/chat?endpoint=agents_bo_cheng_dnb_demos-agents-memory_agent&o=1444828305810485\n\nYou can refer back to the links above from the endpoint detail page at https://e2-demo-field-eng.cloud.databricks.com/ml/endpoints/agents_bo_cheng_dnb_demos-agents-memory_agent/?o=1444828305810485.\n\nTo set up monitoring for your deployed agent, see:\nhttps://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/production-monitoring\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Deployment(model_name='bo_cheng_dnb_demos.agents.memory_agent', model_version='37', endpoint_name='agents_bo_cheng_dnb_demos-agents-memory_agent', served_entity_name='bo_cheng_dnb_demos-agents-memory_agent_37', query_endpoint='https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/agents_bo_cheng_dnb_demos-agents-memory_agent/served-models/bo_cheng_dnb_demos-agents-memory_agent_37/invocations?o=1444828305810485', endpoint_url='https://e2-demo-field-eng.cloud.databricks.com/ml/endpoints/agents_bo_cheng_dnb_demos-agents-memory_agent/?o=1444828305810485', review_app_url='https://e2-demo-field-eng.cloud.databricks.com/ml/review-v2/chat?endpoint=agents_bo_cheng_dnb_demos-agents-memory_agent&o=1444828305810485')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    environment_vars={\n",
    "        \"DATABRICKS_HOST\": \"{{secrets/dbdemos/DATABRICKS_HOST}}\",\n",
    "        \"DATABRICKS_CLIENT_ID\": \"{{secrets/dbdemos/DATABRICKS_CLIENT_ID}}\",\n",
    "        \"DATABRICKS_CLIENT_SECRET\": \"{{secrets/dbdemos/DATABRICKS_CLIENT_SECRET}}\",\n",
    "    },\n",
    "    tags={\"endpointSource\": \"playground\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d140d2fc-c5ce-42ad-b45d-a269de8b7507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Note that  can specify individual users or groups.\n",
    "agents.set_permissions(\n",
    "    model_name=UC_MODEL_NAME,\n",
    "    users=[\"users\"],\n",
    "    permission_level=agents.PermissionLevel.CAN_QUERY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af7987e-5dbe-4fa3-ae99-a7ca0f0578ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See docs for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fc3451-8d75-47ff-82e0-ed29102f09ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nWaiting for endpoint to deploy.  This can take 10 - 20 minutes.................."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "endpoint_name: str = f\"agents_{catalog}-{schema}-{model}\"\n",
    "print(\"\\nWaiting for endpoint to deploy.  This can take 10 - 20 minutes.\", end=\"\")\n",
    "w = WorkspaceClient()\n",
    "while (\n",
    "    w.serving_endpoints.get(endpoint_name).state.ready == EndpointStateReady.NOT_READY\n",
    "    or w.serving_endpoints.get(endpoint_name).state.config_update\n",
    "    == EndpointStateConfigUpdate.IN_PROGRESS\n",
    "):\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3c7c50-a349-4cbf-9c28-b6b9a8614c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent/v1/responses\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "ep = w.serving_endpoints.get(endpoint_name)\n",
    "print(ep.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac4a02e-42c0-4479-a657-4c48ba3dc6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_name: str = f\"agents_{catalog}-{schema}-{model}\"\n",
    "res = get_deploy_client(\"databricks\").predict(\n",
    "    endpoint=endpoint_name,\n",
    "    inputs={\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Who did I just mention?\",\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 400,\n",
    "        \"custom_inputs\": {\"thread_id\": thread_id},\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6a6bb4-a944-40db-9726-5e09adf4fe82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just mentioned **George Miller**.\n"
     ]
    }
   ],
   "source": [
    "if \"output\" in res:\n",
    "    print(res[\"output\"][0][\"content\"][-1][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "02-lakebase-langgraph-checkpointer-agent",
   "widgets": {
    "DATABRICKS_CLIENT_ID": {
     "currentValue": "",
     "nuid": "18a2b9fc-2a9d-4b4a-9745-a987f8273c27",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "DATABRICKS_CLIENT_ID",
      "name": "DATABRICKS_CLIENT_ID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "DATABRICKS_CLIENT_ID",
      "name": "DATABRICKS_CLIENT_ID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "DATABRICKS_CLIENT_SECRET": {
     "currentValue": "",
     "nuid": "f336e7aa-427b-40a8-9abd-d4b99be576b8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "DATABRICKS_CLIENT_SECRET",
      "name": "DATABRICKS_CLIENT_SECRET",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "DATABRICKS_CLIENT_SECRET",
      "name": "DATABRICKS_CLIENT_SECRET",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "bo_cheng_dnb_demos",
     "nuid": "2b1943f2-92cd-4f7a-a824-a27ce3498e62",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bo_cheng_dnb_demos",
      "label": "catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bo_cheng_dnb_demos",
      "label": "catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model": {
     "currentValue": "memory_agent",
     "nuid": "fe1c81e8-14a0-417e-ac97-d1e2dedace82",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "memory_agent",
      "label": "model",
      "name": "model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "memory_agent",
      "label": "model",
      "name": "model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "agents",
     "nuid": "62018a9f-5147-4245-8c23-b83350be3cbb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "agents",
      "label": "schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "agents",
      "label": "schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "secret_scope": {
     "currentValue": "dbdemos",
     "nuid": "a3917bf5-f120-4e11-82a8-fd1ea3803bd6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbdemos",
      "label": "secret_scope",
      "name": "secret_scope",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbdemos",
      "label": "secret_scope",
      "name": "secret_scope",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}