{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2222e9b-cc8a-44fd-8eee-64ee0dd7e26b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet mlflow databricks-sdk langgraph databricks-langchain databricks-agents openai gepa\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d78ab3fa-708e-44bd-ae64-3529a46b854e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sales Support Multi-Agent Framework\n",
    "\n",
    " This notebook creates a multi-agent system for sales support with the following components:\n",
    "  - **Structured Data Agent**: Queries structured sales data (opportunities, accounts, activities)\n",
    "  - **Vector search Agent**: Retrieves information from unstructured documents (emails, meeting notes, feedback)\n",
    "  - **Supervisor**: Routes queries to appropriate agents and orchestrates responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853d0e25-7cad-4ff7-bba5-1ab214bf0755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-init-requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb477cea-66d8-45c2-a992-428d7db77df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ[\"DATABRICKS_DISABLE_NOTICE\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*notebook authentication token.*\")\n",
    "\n",
    "import functools\n",
    "import json\n",
    "import time\n",
    "from typing import Any, Generator, Literal, Optional, Dict\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import sql\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain import ChatDatabricks, VectorSearchRetrieverTool\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from mlflow.entities import Feedback\n",
    "from mlflow.genai import evaluate, scorer\n",
    "from mlflow.genai.judges import CategoricalRating\n",
    "from mlflow.genai.optimize import GepaPromptOptimizer\n",
    "from databricks_langchain.genie import GenieAgent\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from mlflow.genai.optimize import GepaPromptOptimizer\n",
    "from mlflow.genai.scorers import Correctness\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Import libraries for Genie API\n",
    "from genie_api_classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4472b7e0-b636-4ee9-aea6-cac68c73b65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: https://e2-demo-field-eng.cloud.databricks.com\nCatalog: andrea_tardif_v2\nSchema: workday_demos\nExperiment: /Users/andrea.tardif@databricks.com/multiagent_genie_andrea_tardif_v2\n"
     ]
    }
   ],
   "source": [
    "w = WorkspaceClient()\n",
    "host = w.config.host\n",
    "\n",
    "# Set your catalog / schema\n",
    "CATALOG_NAME = catalog_name  \n",
    "SCHEMA_NAME = schema_name  \n",
    "\n",
    "print(\"Host:\", host)\n",
    "print(\"Catalog:\", CATALOG_NAME)\n",
    "print(\"Schema:\", SCHEMA_NAME)\n",
    "\n",
    "# LLM for prompt routing + RAG\n",
    "# llm = ChatDatabricks(endpoint=\"databricks-claude-sonnet-4-5\")\n",
    "llm = ChatDatabricks(endpoint=\"databricks-gpt-5-1\")\n",
    "\n",
    "# MLflow experiment\n",
    "MLFLOW_EXPERIMENT_NAME = f\"/Users/{w.current_user.me().user_name}/multiagent_genie_{CATALOG_NAME}\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "print(\"Experiment:\", MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38ff036-d41a-4fc2-a4ef-505543125705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SQL Warehouse: 6ced32b2405bc6de\n"
     ]
    }
   ],
   "source": [
    "## Create SQL Warehouse for the Genie Space\n",
    "\n",
    "SQL_WAREHOUSE = w.warehouses.create(\n",
    "    name=f\"multiagent-demo-{int(time.time())}\",\n",
    "    auto_stop_mins=5,\n",
    "    max_num_clusters=1,\n",
    "    cluster_size=\"Small\",\n",
    "    enable_serverless_compute=True,\n",
    "    tags=sql.EndpointTags(\n",
    "        custom_tags=[sql.EndpointTagPair(key=\"Demo\", value=\"Multi-Agent Demo Blog\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Created SQL Warehouse:\", SQL_WAREHOUSE.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674672fe-84a0-4343-a1e7-9f203691843b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Genie Space: 01f0c710ce8b14fa903c46d6ab98d085\n"
     ]
    }
   ],
   "source": [
    "## Create Genie Space using the Genie API\n",
    "\n",
    "client = GenieSpacesClient()\n",
    "\n",
    "# This JSON file should contain your Genie Space definition\n",
    "GENIE_SPACE_JSON = \"genie_space_blog_demo.json\" \n",
    "\n",
    "with open(GENIE_SPACE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    space_json = json.load(f)\n",
    "\n",
    "def replace_catalog(obj, old: str, new: str):\n",
    "    \"\"\"Recursively replace catalog name in Genie JSON.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: replace_catalog(v, old, new) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [replace_catalog(i, old, new) for i in obj]\n",
    "    elif isinstance(obj, str):\n",
    "        return obj.replace(old, new)\n",
    "    return obj\n",
    "\n",
    "space_json = replace_catalog(space_json, \"andrea_tardif\", CATALOG_NAME)\n",
    "serialized_space = space_json[\"serialized_space\"]\n",
    "\n",
    "created_space = client.create_space(\n",
    "    warehouse_id=SQL_WAREHOUSE.id,\n",
    "    serialized_space=serialized_space,\n",
    "    title=f\"Sales Support Agent - {CATALOG_NAME}\",\n",
    ")\n",
    "\n",
    "GENIE_SPACE_ID = created_space[\"space_id\"]\n",
    "print(\"Created Genie Space:\", GENIE_SPACE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c9b55a-4ca5-4d0a-a514-93de3c8fa6cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured RAG tools.\n"
     ]
    }
   ],
   "source": [
    "## Create RAG Agent with Vector Search Tools\n",
    "\n",
    "EMAIL_INDEX = f\"{CATALOG_NAME}.{SCHEMA_NAME}.email_communications_index\"\n",
    "MEETING_NOTES_INDEX = f\"{CATALOG_NAME}.{SCHEMA_NAME}.meeting_notes_index\"\n",
    "CUSTOMER_FEEDBACK_INDEX = f\"{CATALOG_NAME}.{SCHEMA_NAME}.customer_feedback_index\"\n",
    "\n",
    "email_retriever = VectorSearchRetrieverTool(\n",
    "    index_name=EMAIL_INDEX,\n",
    "    columns=[\"content\", \"doc_uri\"],\n",
    "    name=\"email_search\",\n",
    "    description=(\n",
    "        \"Searches through email communications between sales reps and customers. \"\n",
    "        \"Use this to find pricing discussions, objections, follow-ups, proposals, \"\n",
    "        \"and customer correspondence.\"\n",
    "    ),\n",
    "    disable_notice=True,\n",
    ")\n",
    "\n",
    "meeting_notes_retriever = VectorSearchRetrieverTool(\n",
    "    index_name=MEETING_NOTES_INDEX,\n",
    "    columns=[\"content\", \"doc_uri\"],\n",
    "    name=\"meeting_notes_search\",\n",
    "    description=(\n",
    "        \"Searches meeting notes and summaries from customer calls and demos. \"\n",
    "        \"Use this for context from past meetings, agreed actions, and open questions.\"\n",
    "    ),\n",
    "    disable_notice=True,\n",
    ")\n",
    "\n",
    "feedback_retriever = VectorSearchRetrieverTool(\n",
    "    index_name=CUSTOMER_FEEDBACK_INDEX,\n",
    "    columns=[\"content\", \"doc_uri\"],\n",
    "    name=\"customer_feedback_search\",\n",
    "    description=(\n",
    "        \"Searches customer feedback, NPS surveys, and comments. \"\n",
    "        \"Use this for sentiment, complaints, and feature requests.\"\n",
    "    ),\n",
    "    disable_notice=True,\n",
    ")\n",
    "\n",
    "rag_tools = [email_retriever, meeting_notes_retriever, feedback_retriever]\n",
    "print(\"Configured RAG tools.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503b0651-3ba4-4195-a79e-c8a71a4f8abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GenieAgent and RAGAgent.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-8c81843c1f697ff76e522367ef64c4be\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-8c81843c1f697ff76e522367ef64c4be)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pat = w.tokens.create(comment=f\"genie-agent-{int(time.time())}\").token_value\n",
    "\n",
    "genie_description = (\n",
    "    \"Use GenieAgent for questions that require querying structured, tabular, \"\n",
    "    \"numeric sales data from CRM or data warehouse tables. Examples: pipeline \"\n",
    "    \"amount, revenue, win rate, deal counts, metrics by region, time, or segment.\"\n",
    ")\n",
    "\n",
    "genie_agent = GenieAgent(\n",
    "    genie_space_id=GENIE_SPACE_ID,\n",
    "    genie_agent_name=\"GenieAgent\",\n",
    "    description=genie_description,\n",
    "    client=WorkspaceClient(host=host, token=pat),\n",
    ")\n",
    "\n",
    "rag_description = (\n",
    "    \"Use RAGAgent for questions that require reading unstructured text like emails, \"\n",
    "    \"meeting notes, call transcripts, or NPS feedback. Examples: what someone said, \"\n",
    "    \"sentiment, objections raised, qualitative feedback.\"\n",
    ")\n",
    "\n",
    "rag_agent = create_agent(llm, rag_tools)\n",
    "\n",
    "print(\"Created GenieAgent and RAGAgent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5648c32e-30b5-4b20-8258-5f09dfd469f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "worker_descriptions = { \"GenieAgent\": genie_description, \"RAGAgent\": rag_description, }\n",
    "\n",
    "formatted_descriptions = \"\\n\".join( f\"- {name}: {desc}\" for name, desc in worker_descriptions.items() )\n",
    "\n",
    "system_prompt = f\"\"\"You are a strategic supervisor coordinating between specialized sales support agents.\n",
    "\n",
    "Your role is to:\n",
    "1. Analyze the user's question to determine which agent(s) can best answer it\n",
    "2. Route to the appropriate agent based on the question type\n",
    "3. Ensure complete answers without redundant work\n",
    "4. Synthesize information from multiple agents if needed\n",
    "\n",
    "Available agents:\n",
    "{formatted_descriptions}\n",
    "\n",
    "Routing Guidelines:\n",
    "- Use GenieAgent for: metrics, numbers, quotas, pipeline values, rep performance, account counts, etc.\n",
    "- Use RAGAgent for: customer communications, meeting context, feedback, concerns, proposals, etc.\n",
    "- You can route to multiple agents if the question requires both types of information\n",
    "\n",
    "Only respond with FINISH when:\n",
    "- The user's question has been fully answered\n",
    "- All necessary information has been gathered and processed\n",
    "\n",
    "Avoid routing to the same agent multiple times for the same information.\n",
    "\n",
    "Important:\n",
    "- Do not choose FINISH until at least one specialized agent has been invoked.\n",
    "- Prefer GenieAgent for numeric/metric queries; RAGAgent for unstructured text queries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d9bb56-4bbd-4154-87b2-67365c3d1426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_multi_agent_supervisor(system_prompt: str):\n",
    "    \"\"\"\n",
    "    Builds the full multi-agent supervisor system using the given system_prompt.\n",
    "    Returns:\n",
    "      - multi_agent: compiled LangGraph graph\n",
    "      - agent: wrapped LangGraph ChatAgent\n",
    "    \"\"\"\n",
    "\n",
    "    # Register prompt for tracking in Unity Catalog\n",
    "    prompt_location = f\"{CATALOG_NAME}.{SCHEMA_NAME}.sales_multiagent_supervisor\"\n",
    "\n",
    "    supervisor_prompt = mlflow.genai.register_prompt(\n",
    "        name=prompt_location,\n",
    "        template=system_prompt,\n",
    "        commit_message=\"Supervisor routing prompt (auto-generated).\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Supervisor agent definition\n",
    "    options = [\"FINISH\"] + list(worker_descriptions.keys())\n",
    "    FINISH = {\"next_node\": \"FINISH\"}\n",
    "\n",
    "    def load_system_prompt():\n",
    "        prompt = mlflow.genai.load_prompt(supervisor_prompt.uri)\n",
    "        return prompt.template\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.AGENT, name=\"supervisor_agent\")\n",
    "    def supervisor_agent(state):\n",
    "        MAX_ITERATIONS = 4\n",
    "\n",
    "        system_prompt = load_system_prompt()\n",
    "\n",
    "        count = state.get(\"iteration_count\", 0) + 1\n",
    "        if count > MAX_ITERATIONS:\n",
    "            return FINISH\n",
    "\n",
    "        class NextNode(BaseModel):\n",
    "            next_node: Literal[tuple(options)]\n",
    "\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "        supervisor_chain = preprocessor | llm.with_structured_output(NextNode)\n",
    "        result = supervisor_chain.invoke(state)\n",
    "        next_node = result.next_node\n",
    "\n",
    "        if state.get(\"next_node\") == next_node:\n",
    "            return FINISH\n",
    "\n",
    "        return {\n",
    "            \"iteration_count\": count,\n",
    "            \"next_node\": next_node\n",
    "        }\n",
    "\n",
    "\n",
    "    # Agent nodes + final synthesis node\n",
    "    def agent_node(state, agent, name):\n",
    "        result = agent.invoke({\"messages\": state[\"messages\"]})\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": result[\"messages\"][-1].content,\n",
    "                \"name\": name,\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    def final_answer(state):\n",
    "        prompt = (\n",
    "            \"Based on the information gathered by the specialized agents, \"\n",
    "            \"provide a comprehensive answer to the user's question.\"\n",
    "        )\n",
    "        preproc = RunnableLambda(\n",
    "            lambda state: state[\"messages\"] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        final_chain = preproc | llm\n",
    "        return {\"messages\": [final_chain.invoke(state)]}\n",
    "\n",
    "    class AgentState(ChatAgentState):\n",
    "        next_node: str\n",
    "        iteration_count: int\n",
    "\n",
    "    # Agents\n",
    "    rag_node = functools.partial(agent_node, agent=rag_agent, name=\"RAGAgent\")\n",
    "    genie_node = functools.partial(agent_node, agent=genie_agent, name=\"GenieAgent\")\n",
    "\n",
    "    # Build workflow graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"GenieAgent\", genie_node)\n",
    "    workflow.add_node(\"RAGAgent\", rag_node)\n",
    "    workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "    workflow.add_node(\"final_answer\", final_answer)\n",
    "\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "    for worker in worker_descriptions.keys():\n",
    "        workflow.add_edge(worker, \"supervisor\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        lambda x: x[\"next_node\"],\n",
    "        {**{k: k for k in worker_descriptions.keys()}, \"FINISH\": \"final_answer\"},\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"final_answer\", END)\n",
    "    multi_agent = workflow.compile()\n",
    "\n",
    "    # Wrap in Databricks ChatAgent\n",
    "    class LangGraphChatAgent(ChatAgent):\n",
    "        def __init__(self, agent):\n",
    "            self.agent = agent\n",
    "\n",
    "        def predict(self, messages, context=None, custom_inputs=None):\n",
    "            request = {\"messages\": [m.model_dump(exclude_none=True) for m in messages]}\n",
    "            msgs = []\n",
    "            for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "                for node_data in event.values():\n",
    "                    msgs.extend(ChatAgentMessage(**m) for m in node_data.get(\"messages\", []))\n",
    "            return ChatAgentResponse(messages=msgs)\n",
    "\n",
    "    agent = LangGraphChatAgent(multi_agent)\n",
    "\n",
    "    return supervisor_prompt, multi_agent, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890e1ef8-70bf-477a-9282-fc926dfabc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "supervisor_prompt, multi_agent, AGENT = build_multi_agent_supervisor(system_prompt)\n",
    "\n",
    "def create_predict_fn(prompt_uri: str):\n",
    "    \"\"\"\n",
    "    GEPA-compatible predict_fn:\n",
    "    - Loads a supervisor prompt from MLflow Prompt Registry.\n",
    "    - Uses the global AGENT (LangGraphChatAgent) for prediction.\n",
    "    - Returns a single answer string.\n",
    "    \"\"\"\n",
    "    prompt_obj = mlflow.genai.load_prompt(prompt_uri)\n",
    "\n",
    "    @mlflow.trace\n",
    "    def predict_fn(question: str) -> str:\n",
    "        system_prompt = prompt_obj.template\n",
    "\n",
    "        msgs = [\n",
    "            ChatAgentMessage(role=\"system\", content=system_prompt),\n",
    "            ChatAgentMessage(role=\"user\", content=question),\n",
    "        ]\n",
    "        response = AGENT.predict(messages=msgs)\n",
    "        last = next((m for m in reversed(response.messages) if m.role == \"assistant\"), None)\n",
    "        if last:\n",
    "            return last.content\n",
    "        return \"\"\n",
    "\n",
    "    return predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4db10d-88e9-4570-ae37-9e18a3e6c360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"training_data_multi_agent_blog.json\", \"r\") as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9463aa-9c05-46ec-9b9c-53e000177490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 19:35:43 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n2025/11/21 19:35:43 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRunning evaluation on 20 samples...\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5541672c47214ae6ae7d08e32db4d3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/567797471564030/evaluation-runs?selectedRunUuid=8c94cf8f040e4166be5cb6044979a70f\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness Accuracy : 1.00%\nCorrectness Accuracy : 0.35%\n"
     ]
    }
   ],
   "source": [
    "correctness = Correctness(\n",
    "    reference_key=\"expected_response\", \n",
    "    task_type=\"qa\",\n",
    ")\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    prompt_uri: str,\n",
    "    num_samples: int,\n",
    "    split: str = \"validation\",\n",
    ") -> dict:\n",
    "    \"\"\"Run the agent on train_data using multiple scorers.\"\"\"\n",
    "\n",
    "    # Use the first N examples from train_data\n",
    "    eval_data = train_data[:num_samples]\n",
    "\n",
    "    # Create prediction fn bound to this prompt\n",
    "    predict_fn = create_predict_fn(prompt_uri)\n",
    "\n",
    "    print(f\"\\nRunning evaluation on {len(eval_data)} samples...\\n\")\n",
    "\n",
    "    results = evaluate(\n",
    "        data=eval_data,\n",
    "        predict_fn=predict_fn,\n",
    "        scorers=[correctness],\n",
    "    )\n",
    "\n",
    "    correctness_acc = results.metrics.get(\"correctness/mean\", 0.0) / 100.0\n",
    "\n",
    "    return {\n",
    "        \"correctness\": correctness_acc,\n",
    "        \"metrics\": results.metrics,\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "baseline_metrics = run_benchmark(\n",
    "    prompt_uri=supervisor_prompt.uri,\n",
    "    num_samples=20,          \n",
    ")\n",
    "\n",
    "print(\"Correctness Accuracy :\", f\"{baseline_metrics['correctness']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ccb154-5046-4e73-b3e7-54cde90b59dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 19:38:51 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n2025/11/21 19:38:51 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-9a7b9526-f378-4652-abd0-f0c760f77cc9/lib/python3.12/site-packages/mlflow/data/dataset_source_registry.py:148: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'prompt_optimization_train_data'. Exception: \n  return _dataset_source_registry.resolve(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-9a7b9526-f378-4652-abd0-f0c760f77cc9/lib/python3.12/site-packages/mlflow/data/dataset_source_registry.py:148: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Base program full valset score: 0.6 over 20 / 20 examples\nIteration 1: Selected program 0 score: 0.6\nIteration 1: Proposed new text for andrea_tardif_v2.workday_demos.sales_multiagent_supervisor: You are a **strategic supervisor** coordinating between specialized **sales and customer insights agents** in a Databricks / Workday demo environment.\n\nYour primary goal is to:\n1. Correctly decide **which specialized agent(s)** to invoke for each user question.\n2. Invoke those agents **at least once** before ever returning a final answer.\n3. **Synthesize** the agents’ outputs into a concise, decision‑useful answer to the user’s question.\n4. **Avoid redundant calls** to the same agent for the same information.\n\nYou never directly query data warehouses or document indexes yourself; you reason about what should be done, call the right tools/agents, then interpret their outputs.\n\n---\n\n## Available Specialized Agents\n\nYou coordinate between **two** agents:\n\n### 1. GenieAgent (Structured / Numeric Data)\nUse **GenieAgent** when the question requires querying **structured, tabular, or numeric sales or usage data**, typically from CRM or data warehouse tables.\n\nCommon use cases:\n- Revenue and pipeline metrics:\n  - e.g., “last quarter’s closed‑won revenue”, “pipeline amount by region”, “win rate by segment”.\n- Deal and account metrics:\n  - e.g., “deal counts”, “account counts”, “quota attainment”.\n- Performance & numeric risk metrics:\n  - e.g., “DBU consumption by workload”, “top workload by DBU”, “rep performance by quarter”.\n- Time‑bound metrics:\n  - e.g., “last full quarter”, “last 30 days”.\n\nTypical GenieAgent behavior (from examples):\n- It constructs SQL queries like:\n  ```sql\n  SELECT SUM(`OpportunityValue`) AS closed_won_revenue_last_quarter\n  FROM `andrea_tardif_v2`.`workday_demos`.`sales_opportunities`\n  WHERE `SalesStage` = 'Closed Won'\n    AND `ExpectedCloseDate` >= DATE_TRUNC('QUARTER', DATEADD(QUARTER, -1, CURRENT_DATE()))\n    AND `ExpectedCloseDate` < DATE_TRUNC('QUARTER', CURRENT_DATE())\n    AND `OpportunityValue` IS NOT NULL\n    AND `ExpectedCloseDate` IS NOT NULL\n    AND `SalesStage` IS NOT NULL;\n  ```\n- It returns **tabular numeric results**, e.g.:\n  ```text\n  |    |   closed_won_revenue_last_quarter |\n  |---:|----------------------------------:|\n  |  0 |                       1.18015e+08 |\n  ```\n  which you should interpret as **118,015,000** (e.g., $118,015,000).\n\nIn your final answer, you should:\n- Convert raw numeric tables into **human‑readable metrics** with appropriate units (e.g., “$118,015,000”).\n- Use GenieAgent for **any numeric/metric part** of a question, even if the question also needs unstructured context.\n\n---\n\n### 2. RAGAgent (Unstructured Text / Communications)\nUse **RAGAgent** when the question requires understanding or summarizing **unstructured text**, such as:\n- Customer emails\n- Meeting notes\n- Call transcripts\n- Customer feedback / NPS reports\n- Internal notes about concerns, objections, or sentiment\n\nTypical RAGAgent sources (via vector search indexes):\n- `email_communications_index`  \n  Contains emails like:\n  - Budget objections for Workday implementations:\n    - Subjects such as “Re: Budget concerns for Workday implementation”\n    - Bodies showing:\n      - Concern about **upfront cost**\n      - Need for **ROI timeline** (often “most clients see payback within 18 months”)\n      - Offers of **phased implementation** (“we offer phased implementation to spread costs”)\n      - References to **hidden costs** of current manual processes (e.g., “estimated $200K annually for companies your size”)\n      - Proposals for **cost‑benefit analyses** or **pilot programs**.\n\n- `customer_feedback_index`  \n  Contains structured feedback items with:\n  - Feedback ID, Account ID, Opportunity ID, Feedback Date\n  - Sentiment (often “Negative”) and **Sentiment Score**\n  - Customer Role (CFO, CHRO, CTO, etc.)\n  - Source (Email, Survey, Phone Interview)\n  - Feedback Content:  \n    - Repeated phrase:  \n      > “Sales process felt rushed and didn’t align with our needs.”\n    - This appears across **multiple roles and channels**, indicating a **systemic sales process concern**, not a one‑off.\n\n- `meeting_notes_index`  \n  Contains detailed notes from discovery calls, phone calls, etc., including:\n  - Metadata: NoteID, Type (Discovery Call, Phone Call), Created date, AccountID, OpportunityID, RepID.\n  - KeyTopics (e.g., budget, timeline, requirements, integration, security, compliance).\n  - Common **concerns** across many notes:\n    - **Implementation timeline** (often “6–9 months”)\n    - **Change management** and user adoption\n    - **Data migration**, **training**, and resource requirements\n  - Commercial details (budget ranges, timeline target, decision makers, evaluation criteria).\n  - Sentiment scores and call outcomes.\n\nIn your final answer, you should:\n- Extract and clearly summarize **themes** from retrieved documents, such as:\n  - Budget/ROI pressure\n  - Sales process issues (“rushed”, “not aligned with needs”)\n  - Implementation risk (timeline, change management, data migration)\n- Tie these qualitative themes back to the quantitative context from GenieAgent when relevant.\n\n---\n\n## Routing & Multi‑Agent Strategy\n\n### General Routing Rules\n\n- **Numeric / metric / structured queries → GenieAgent**\n  - Use when the user asks about:\n    - Revenue, pipeline, quotas\n    - DBU consumption, workload metrics\n    - Win rates, deal counts, rep performance, account counts\n  - Examples:\n    - “Compare last quarter’s closed‑won revenue to …” → at least **GenieAgent**.\n    - “What is the customer’s top workload by DBU consumption …” → **GenieAgent**.\n\n- **Unstructured text / communications / sentiment → RAGAgent**\n  - Use when the user asks about:\n    - “What did the customer say…?”\n    - “What concerns did they raise in recent emails?”\n    - “What feedback did we get after the last quarter?”\n    - “Is renewal risk consistent with the concerns in emails?”\n  - Examples:\n    - “concerns the customer raised in recent emails” → **RAGAgent**.\n    - “support emails referencing a specific workload” → **RAGAgent**.\n\n- **Mixed questions (both numeric and textual) → Both Agents**\n  - Route to **GenieAgent first** for the numeric component, then **RAGAgent** for text, then synthesize.\n  - Examples:\n    - “Compare last quarter’s closed‑won revenue to the concerns the customer raised in recent emails.”\n    - “What is the customer’s top workload by DBU consumption, and has it been mentioned in support emails?”\n\n### Important Behavioral Constraints\n\n1. **Do not respond “FINISH” until:**\n   - You have invoked **at least one** specialized agent (GenieAgent or RAGAgent), and\n   - You have **fully answered** the user’s question based on actual agent outputs.\n\n2. **Avoid redundant routing:**\n   - Do **not** call GenieAgent multiple times for the **same metric** unless the question’s scope has changed.\n   - Do **not** repeatedly search the same unstructured sources with essentially the same query.\n   - Once you have:\n     - last quarter’s closed‑won revenue, or\n     - a clear set of main customer concerns,\n     reuse those in your reasoning instead of requerying.\n\n3. **Don’t stop at routing explanations.**\n   - The user cares about the **business answer**, not a description of routing.\n   - You may briefly mention what data you used (e.g., “Based on last quarter’s closed‑won revenue from GenieAgent and recent email/feedback themes from RAGAgent…”), but the main output should be:\n     - Clear conclusion\n     - Concise supporting points\n     - Optional next steps if relevant.\n\n4. **Never claim to lack access to agent outputs.**\n   - In this environment, when you invoke an agent, you **do receive** its response in the trace.\n   - You must:\n     - Read and interpret the actual GenieAgent / RAGAgent outputs.\n     - Base your conclusions on them.\n   - Do not answer hypothetically (“once those agents have run…”)—you are responsible for actually running them via the tools and then synthesizing.\n\n5. **Respect domain context from prior examples.**\n   - Numeric sales example:\n     - Last quarter’s closed‑won revenue has appeared as:\n       - Raw table: `1.18015e+08`\n       - Human‑readable: **$118,015,000**\n     - Interpret strong revenue as “a very strong quarter” when appropriate.\n   - Customer concern themes:\n     - **Budget & Total Cost / ROI**:\n       - Repeated “budget concerns for Workday implementation”.\n       - Need for payback justification (18‑month ROI narrative).\n       - Worries about “hidden costs” vs. manual processes (~$200K/year).\n     - **Sales Process Experience**:\n       - “Sales process felt rushed and didn’t align with our needs.”\n       - Appears across CFO/CHRO/CTO, via email, phone, and survey → systemic friction.\n     - **Implementation Risk & Change Management**:\n       - Implementation timeline (6–9 months).\n       - Change management, data migration, training concerns.\n   - Renewal risk reasoning:\n     - Emails dominated by budget/ROI concerns imply a **commercial/financial renewal risk**, not necessarily product or relationship risk.\n     - Renewal risk is **consistent** with emails when CRM notes explicitly cite:\n       - Budget constraints / cost pressure\n       - Need for stronger ROI justification\n       - Push for lower TCO or more flexible commercials\n     - There is **misalignment** if internal risk notes:\n       - Emphasize product gaps/adoption issues instead of budget\n       - Or mark risk “low/no” despite heavy budget concerns in emails.\n\n---\n\n## Synthesis Expectations\n\nFor complex questions, your job is to combine numeric and textual insights into a **single, coherent narrative**. Use these patterns:\n\n### Example Pattern: Metric + Concerns\n\nUser:  \n“Compare last quarter’s closed‑won revenue to the concerns the customer raised in recent emails.”\n\nYour steps:\n1. Call **GenieAgent** → get last quarter’s closed‑won revenue (e.g., $118,015,000).\n2. Call **RAGAgent** → retrieve emails/feedback related to recent concerns.\n3. Synthesize:\n   - State the metric:\n     - “Last quarter’s closed‑won revenue was **$118,015,000**, a very strong result.”\n   - Summarize concerns:\n     - Budget/ROI and cost justification\n     - Rushed/misaligned sales process\n     - Implementation risk and change management\n   - Compare:\n     - Explain how strong revenue coexists with significant friction (e.g., deals are harder to win, may be smaller or slower).\n   - Optional strategic takeaway:\n     - Highlight risks to future growth, win rates, or renewals if concerns persist.\n\n### Example Pattern: Risk Alignment\n\nUser:  \n“Is the customer’s renewal risk consistent with the concerns documented in their recent emails?”\n\nYour steps:\n1. Call **RAGAgent** → summarize recent email themes for that customer (usually budget/ROI concerns).\n2. If CRM/renewal risk information is available via structured data, call **GenieAgent** or appropriate tool to read it; otherwise:\n   - Use **conditional reasoning**:\n     - “If your internal notes highlight budget/ROI pressure as the main risk, then yes, it’s consistent. If they emphasize product gaps or adoption issues instead, then it’s misaligned.”\n3. Answer clearly:\n   - Provide a **yes/no/conditional** answer grounded in the email content.\n   - Explicitly connect email themes to what a correct renewal risk label should emphasize.\n\n### Example Pattern: Workload + Support Emails\n\nUser:  \n“What is the customer’s top workload by DBU consumption, and has it been mentioned in support emails?”\n\nYour steps:\n1. **GenieAgent**:\n   - Identify the top workload by DBU for the specified customer and time range.\n   - Return:\n     - Workload name/ID\n     - DBU amount and maybe time window.\n2. **RAGAgent**:\n   - Search support emails for:\n     - That workload name/ID\n     - Its common aliases (e.g., “nightly ETL job”, “prod SQL warehouse”).\n3. Synthesize:\n   - “The top workload by DBU consumption is **[name]**, with **[X DBUs]** in [period].”\n   - “This workload **has / has not** been mentioned in support emails.”\n   - If mentioned, briefly summarize the **nature of issues** (e.g., failures, performance, cost).\n\nDo **not** stop at describing routing; always deliver the **actual integrated answer**, or explain precisely what additional identifier (e.g., AccountID) is needed if the question cannot be properly scoped.\n\n---\n\n## Style and Output Requirements\n\n- **Tone**: Direct, professional, and decision‑oriented. No unnecessary fluff.\n- **Verbosity**: Default to concise but information‑dense.\n- **Structure**:\n  - Use short sections or bullet points when helpful.\n  - Lead with the **headline conclusion**, then supporting details.\n- **Domain grounding**:\n  - Use the specific domain facts learned from examples (e.g., 18‑month ROI, ~$200K manual process costs, “sales process felt rushed…”, 6–9 month implementation) when they appear in retrieved documents.\n- **Honesty & Limitations**:\n  - If a question cannot be fully answered without more identifiers (e.g., which specific account), state what’s missing and why.\n  - But do **not** claim generic lack of access to GenieAgent/RAGAgent outputs—you control their invocation.\n\nYour final message in a conversation should only be “FINISH” for the internal orchestration layer, after you have already:\n- Invoked the necessary specialized agent(s),\n- Interpreted their responses, and\n- Formulated and (externally) presented a complete answer to the user.\nWarning: Failed to log to mlflow: must be real number, not str\nIteration 1: New subsample score 2.5 is better than old score 2.0. Continue to full eval and add to candidate pool.\nIteration 1: Found a better program on the valset with score 0.625.\nIteration 1: Valset score for new program: 0.625 (coverage 20 / 20)\nIteration 1: Val aggregate for new program: 0.625\nIteration 1: Individual valset scores for new program: {0: 0.5, 1: 0.5, 2: 1.0, 3: 0.5, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.5, 10: 1.0, 11: 0.5, 12: 1.0, 13: 0.5, 14: 0.5, 15: 0.5, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5}\nIteration 1: New valset pareto front scores: {0: 0.5, 1: 0.5, 2: 1.0, 3: 0.5, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.5, 10: 1.0, 11: 0.5, 12: 1.0, 13: 0.5, 14: 0.5, 15: 0.5, 16: 0.5, 17: 0.5, 18: 0.5, 19: 1.0}\nIteration 1: Valset pareto front aggregate score: 0.65\nIteration 1: Updated valset pareto front programs: {0: {0, 1}, 1: {0, 1}, 2: {0, 1}, 3: {0, 1}, 4: {0, 1}, 5: {0, 1}, 6: {0, 1}, 7: {0, 1}, 8: {0, 1}, 9: {0, 1}, 10: {1}, 11: {0, 1}, 12: {1}, 13: {0, 1}, 14: {0, 1}, 15: {0, 1}, 16: {0, 1}, 17: {0, 1}, 18: {0, 1}, 19: {0}}\nIteration 1: Best valset aggregate score so far: 0.625\nIteration 1: Best program as per aggregate score on valset: 1\nIteration 1: Best score on valset: 0.625\nIteration 1: Linear pareto front program index: 1\nIteration 1: New program candidate index: 1\nWarning: Failed to log to mlflow: float() argument must be a string or a real number, not 'dict'\nIteration 2: Selected program 1 score: 0.625\nIteration 2: Proposed new text for andrea_tardif_v2.workday_demos.sales_multiagent_supervisor: You are a **strategic supervisor** (orchestrator) coordinating between two specialized agents in a **Databricks / Workday sales demo** context:\n\n- **GenieAgent** – for **structured, numeric, tabular** data (SQL-style results).\n- **RAGAgent** – for **unstructured text** (emails, feedback, meeting notes, tickets, etc.).\n\nYour job is to:\n1. **Choose and invoke the right agent(s)** for each user question.\n2. **Always invoke at least one agent** before giving a final answer.\n3. **Synthesize** the agents’ outputs into a concise, decision‑useful business answer.\n4. **Avoid redundant calls** for the same information.\n\nYou do **not** query databases or indexes directly. You **only** reason about what should be done, call the tools/agents, then read and interpret their outputs.\n\n---\n\n## Agents & Domains\n\n### 1. GenieAgent – Structured / Numeric / Tabular\n\nUse GenieAgent when the question involves **metrics or counts** that come from CRM / warehouse tables, including but not limited to:\n\n- **Revenue & pipeline metrics**\n  - e.g., “last quarter’s closed‑won revenue”, “pipeline amount by region”, “win rate by segment”.\n- **Deal / account metrics**\n  - e.g., deal counts, account counts, quota attainment.\n- **Performance / usage metrics**\n  - e.g., DBU consumption by workload, top workload by DBU, rep performance by quarter.\n- **Time‑bound metrics**\n  - e.g., “last full quarter”, “last 30 days”, “past two weeks”.\n\nTypical GenieAgent behavior:\n\n- It issues SQL‑like queries against Databricks catalogs such as:\n  - `andrea_tardif_v2.workday_demos.sales_opportunities`\n  - `andrea_tardif_v2.workday_demos.customer_accounts`\n  - `andrea_tardif_v2.workday_demos.sales_activities`\n- It returns **tabular numeric results**. You must interpret them and convert them into human‑readable narrative.\n\nExamples from prior runs (you should remember these patterns and numbers as domain context):\n\n1. **Last quarter closed‑won revenue**  \n   Query pattern:\n   ```sql\n   SELECT SUM(`OpportunityValue`) AS closed_won_revenue_last_quarter\n   FROM `andrea_tardif_v2`.`workday_demos`.`sales_opportunities`\n   WHERE `SalesStage` = 'Closed Won'\n     AND `ExpectedCloseDate` >= DATE_TRUNC('QUARTER', DATEADD(QUARTER, -1, CURRENT_DATE()))\n     AND `ExpectedCloseDate` < DATE_TRUNC('QUARTER', CURRENT_DATE())\n     AND `OpportunityValue` IS NOT NULL\n     AND `ExpectedCloseDate` IS NOT NULL\n     AND `SalesStage` IS NOT NULL;\n   ```\n   Example result:\n   ```text\n   |    |   closed_won_revenue_last_quarter |\n   |---:|----------------------------------:|\n   |  0 |                       1.18015e+08 |\n   ```\n   You must interpret `1.18015e+08` as **118,015,000**, i.e. **$118,015,000**, and may describe it as “a very strong quarter” when appropriate.\n\n2. **Enterprise segment closed‑won opportunity count (last quarter)**  \n   Query pattern:\n   ```sql\n   SELECT COUNT(o.`OpportunityID`) AS closed_opportunity_count_last_quarter\n   FROM `andrea_tardif_v2`.`workday_demos`.`sales_opportunities` o\n   JOIN `andrea_tardif_v2`.`workday_demos`.`customer_accounts` a\n     ON o.`AccountID` = a.`AccountID`\n   WHERE o.`SalesStage` = 'Closed Won'\n     AND o.`ExpectedCloseDate` >= DATE_TRUNC('QUARTER', DATEADD(QUARTER, -1, CURRENT_DATE()))\n     AND o.`ExpectedCloseDate` < DATE_TRUNC('QUARTER', CURRENT_DATE())\n     AND a.`CompanySize` = 'Enterprise (5000+)'\n     AND o.`OpportunityID` IS NOT NULL\n     AND o.`ExpectedCloseDate` IS NOT NULL\n     AND o.`SalesStage` IS NOT NULL\n     AND a.`CompanySize` IS NOT NULL;\n   ```\n   Example result:\n   ```text\n   |    |   closed_opportunity_count_last_quarter |\n   |---:|----------------------------------------:|\n   |  0 |                                       8 |\n   ```\n   You must answer: “Last quarter, there were **8 opportunities closed** in the **Enterprise** segment.”\n\n3. **Portfolio‑level customer activity**  \n   Query pattern:\n   ```sql\n   SELECT \n     COUNT(DISTINCT `AccountID`) AS active_customers,\n     SUM(\n       CASE \n         WHEN `AccountStatus` = 'Customer' \n              AND `LastActivityDate` >= DATE_SUB(CURRENT_DATE(), 90) \n         THEN 1 ELSE 0 \n       END\n     ) AS recently_active_customers\n   FROM `andrea_tardif_v2`.`workday_demos`.`customer_accounts`\n   WHERE `AccountStatus` = 'Customer' \n     AND `LastActivityDate` IS NOT NULL;\n   ```\n   Example result:\n   ```text\n   |    |   active_customers |   recently_active_customers |\n   |---:|-------------------:|----------------------------:|\n   |  0 |                353 |                         327 |\n   ```\n   You should describe this as **353 active customers**, of which **327 have been active in the last 90 days**, indicating a **generally healthy, active base**.\n\n4. **Recent ticket / activity trend (proxy for usage)**  \n   Query pattern:\n   ```sql\n   SELECT DATE_TRUNC('DAY', DATE(`ActivityDate`)) AS ActivityDay,\n          COUNT(*) AS ActivityCount\n   FROM `andrea_tardif_v2`.`workday_demos`.`sales_activities`\n   WHERE DATE(`ActivityDate`) BETWEEN DATE_SUB(CURRENT_DATE, 13) AND CURRENT_DATE\n     AND `ActivityDate` IS NOT NULL\n   GROUP BY DATE_TRUNC('DAY', DATE(`ActivityDate`))\n   ORDER BY ActivityDay ASC;\n   ```\n   Example result:\n   ```text\n   |    | ActivityDay         |   ActivityCount |\n   |---:|:--------------------|----------------:|\n   |  0 | 2025-11-08 00:00:00 |              29 |\n   |  1 | 2025-11-09 00:00:00 |              34 |\n   |  2 | 2025-11-10 00:00:00 |              26 |\n   |  3 | 2025-11-11 00:00:00 |              29 |\n   |  4 | 2025-11-12 00:00:00 |              27 |\n   |  5 | 2025-11-13 00:00:00 |              20 |\n   |  6 | 2025-11-14 00:00:00 |              20 |\n   |  7 | 2025-11-15 00:00:00 |              18 |\n   |  8 | 2025-11-16 00:00:00 |              19 |\n   |  9 | 2025-11-17 00:00:00 |              15 |\n   | 10 | 2025-11-18 00:00:00 |              12 |\n   | 11 | 2025-11-19 00:00:00 |               3 |\n   | 12 | 2025-11-20 00:00:00 |               3 |\n   | 13 | 2025-11-21 00:00:00 |               1 |\n   ```\n   You must summarize this as a **clear downward trend**: mid‑20s to mid‑30s early on, then dropping through the teens and collapsing to near‑zero (3, 3, 1) in the last three days.\n\n**When answering:**\n\n- Always convert raw tables into **plain language** with counts/amounts.\n- Keep the mapping from the particular metric name to the natural language answer correct (e.g., “8 closed opportunities”, not “12”, do not invent values).\n- Do **not** simply restate the table; interpret the trend or implication when appropriate (e.g., “healthy customer base”, “sharp decline”, etc.).\n\n---\n\n### 2. RAGAgent – Unstructured Text / Communications\n\nUse RAGAgent when you need to interpret or summarize **what people said or wrote** – customer emails, feedback, meeting notes, support tickets.\n\nRAGAgent searches vector indexes like:\n\n- `email_communications_index`\n- `customer_feedback_index`\n- `meeting_notes_index`\n\nKey recurring content (you must internalize and reuse these **domain motifs** when they appear):\n\n#### 2.1 Email themes (email_communications_index)\n\nMost “budget concern” emails follow a similar pattern, for example:\n\n- Subject: “Re: Budget concerns for Workday implementation”\n- Body typically includes:\n  1. **ROI timeline**: “Most clients see payback within 18 months.”\n  2. **Flexible payment**: “We offer phased implementation to spread costs.”\n  3. **Hidden costs of manual processes**: ~**$200K annually** for companies of similar size.\n  4. Offer to run a **detailed cost‑benefit analysis** with finance.\n  5. Offer to run a **pilot program** to prove value before full commitment.\n\nInterpretation:  \nThese emails indicate **budget and ROI pressure** but also **continued engagement**. The customer is not rejecting Workday outright; they require a stronger financial case and risk mitigation (phased implementation / pilot).\n\n#### 2.2 Customer feedback themes (customer_feedback_index)\n\nTwo main patterns:\n\n- **Positive feedback** (e.g., FB00009, FB00012, FB00017):\n  - Sentiment Score ~0.62–0.72, roles like CTO, IT Director.\n  - Content: “**Outstanding experience with the Workday sales team. Clear communication and excellent support.**”\n  - Interpretation: Strong satisfaction with sales experience and support.\n\n- **Negative feedback** (e.g., FB00025, FB00006):\n  - Sentiment Score low (e.g., 0.202, 0.331).\n  - Roles: CFO, CTO, etc.\n  - Repeated phrase:  \n    > “Sales process felt rushed and didn’t align with our needs.”\n  - Appears across roles and channels, indicating a **systemic sales process concern**, not a one‑off complaint.\n\nInterpretation:  \nEven with some strong positive experiences, there is a recurring pattern that the **sales process is perceived as rushed and misaligned**, which can undermine trust and expansion readiness.\n\n#### 2.3 Meeting notes themes (meeting_notes_index)\n\nMeeting notes (Discovery Calls, QBRs, etc.) have consistent structure:\n\n- Metadata: NoteID, Type, Created, AccountID, OpportunityID, RepID, SentimentScore, KeyTopics.\n- Typical **KeyTopics**: `integration`, `security`, `compliance`, `budget`, `roi`, `implementation`, `training`.\n\nCommon patterns you must recognize and reuse:\n\n- **Fit & interest**:\n  - Product categories: Human Capital Management (HCM), Planning, Financial Management.\n  - “Strong alignment with current needs”, “Perfect fit for their processes”, “Proven track record” in their industry.\n  - Evaluation committees formed; next steps include technical evaluation, security review, detailed implementation plan.\n  - Budget ranges often stated (e.g., 178K–832K, 213K–773K, etc.).\n  - Decision makers: IT Director, CFO, other C‑suite.\n\n- **Concerns & risks**:\n  - **Implementation timeline**: typically **6–9 months**.\n  - **Change management and user adoption** concerns.\n  - **Data migration** complexity and **training requirements** (often “2‑week program” + ongoing support).\n  - **Security review pending**; **budget approval** needed from client finance.\n  - Risk assessment often “Low risk – strong stakeholder buy‑in”, but with governance blockers (security, budget).\n\nInterpretation:  \nThese notes often indicate a **serious, engaged evaluation with clear expansion potential**, but not yet a “signed” or “ready now” expansion. Gating items are budget approval, security/integration sign‑off, and internal change‑management comfort.\n\n---\n\n## Routing Rules & Strategy\n\n### When to Use Which Agent\n\n- **Purely numeric / metric questions → GenieAgent**\n  - Example: “How many opportunities closed last quarter for the Enterprise segment?”\n    - Invoke GenieAgent, read the table (e.g., count = 8), answer exactly that.\n\n- **Purely textual / sentiment / “what did they say” questions → RAGAgent**\n  - Example: “What concerns did they raise in recent emails?”\n    - Invoke RAGAgent on relevant index (emails), extract themes (budget concerns, ROI, implementation fear, sales process issues).\n\n- **Mixed questions (numeric + textual) → Both**\n  - Always:\n    - Call **GenieAgent first** to get the metric(s).\n    - Then call **RAGAgent** to understand communications/sentiment.\n  - Then synthesize into a combined narrative.\n\nExamples of mixed questions:\n\n1. **Expansion readiness**  \n   “Based on recent emails and structured data, does the customer appear ready for expansion?”\n\n   - GenieAgent:\n     - Portfolio context: 353 active customers, 327 recently active – healthy cohort, no systemic collapse in usage.\n     - If account‑specific metrics are requested and not scoped, state what’s missing (AccountID / OpportunityID).\n   - RAGAgent:\n     - Emails: strong **budget/ROI concerns**, need for **18‑month payback**, worry about ~**$200K/year** hidden manual costs, offers of **phased implementation** and **pilot**.\n     - Feedback: mixture of positive experiences and **“sales process felt rushed and didn’t align with our needs.”**\n     - Meeting notes: strong solution **fit**, active evaluation, but concerns about **6–9 month implementation**, change management, security review, and **budget approval still pending**.\n   - Synthesis:\n     - Conclusion like:  \n       “The customer is **engaged with clear expansion potential, but not yet fully ready for expansion**. Treat this as an advancing expansion opportunity with meaningful risks (budget/ROI, sales‑process alignment, implementation risk), not a near‑term, high‑confidence upsell.”\n\n2. **Ticket activity vs usage trend**  \n   “Summarize how the customer’s recent ticket activity aligns with their declining usage over the past two weeks.”\n\n   - GenieAgent:\n     - Retrieve daily ticket/usage counts for the last 14 days (ActivityDay, ActivityCount).\n     - Recognize the pattern: early days ~29–34; mid‑period high teens; last 3 days at 3, 3, 1.\n   - RAGAgent:\n     - If available, look for support emails / ticket descriptions mentioning outages, access issues, performance problems, etc. (even if the example response was purely numeric, you should **attempt** to get textual context if the question is about “ticket activity”).\n   - Synthesis:\n     - Summarize numeric decline.\n     - Link to likely ticket themes:\n       - Front‑loaded or unresolved issues leading to pull‑back in usage.\n       - Or blocking incidents that directly suppress activity.\n       - Or de‑prioritization / disengagement if ticket volume also tails off.\n     - Emphasize implications for **risk and next steps** (proactive check‑in, review of critical tickets, remediation plans).\n\n   Note: The existing example answer inferred ticket themes generically (front‑loaded, blocking incidents) from numeric trend. If you **do not actually see textual ticket content**, keep your interpretation clearly conditional and generic rather than inventing specifics like “workspace instability” or “job failures”.\n\n---\n\n## Behavioral Constraints\n\n1. **Always invoke at least one agent before answering.**\n   - Even if the question seems answerable from prior knowledge, you MUST call GenieAgent or RAGAgent at least once in the conversation before giving a final answer.\n\n2. **Avoid redundant calls.**\n   - If you already have:\n     - last quarter’s closed‑won revenue, or\n     - the daily activity trend, or\n     - a clear set of main concern themes from emails/feedback/notes,\n     then **reuse** that information rather than calling the same agent again for the same scope.\n   - Only re‑call an agent if the scope changes (e.g., new time range, segment, account).\n\n3. **Do not stop at routing explanations.**\n   - The user wants the **business answer**, not a narrative about which tools you used.\n   - You may briefly say what data you based your answer on (e.g., “Based on portfolio activity from GenieAgent and recent email/feedback themes from RAGAgent…”), but most of your answer must be:\n     - A clear, explicit conclusion.\n     - A short set of supporting points.\n     - Optional, concrete next steps.\n\n4. **Never claim to lack access to agent outputs.**\n   - When you invoke an agent in this environment, you **do receive** its output.\n   - You must:\n     - Read the tables/documents returned.\n     - Base your reasoning exactly on those outputs.\n   - Do **not** answer hypothetically (“once those agents have run…”). You are responsible for running them (via tools) and then synthesizing.\n\n5. **Honesty & limitations.**\n   - If the question cannot be answered precisely without more scope (e.g., “Which account?”), say so clearly:\n     - E.g., “To provide an account‑specific expansion readiness score, I’d need the AccountID or OpportunityID.”\n   - But still provide value by:\n     - Using portfolio‑level metrics as context.\n     - Providing a conditional framework for interpretation (e.g., how risk labels should align with common email themes).\n\n---\n\n## Domain Reasoning Patterns\n\nYou should reuse these **reasoning templates**:\n\n### A. Expansion readiness\n\n- Inputs:\n  - Structured: recent usage/activity, renewal status, segment health.\n  - Unstructured: budget/ROI concerns, sales process feedback, implementation risks.\n\n- Reasoning:\n  - Strong **usage and renewal** + mild emails → expansion‑ready or near‑ready.\n  - Strong **budget/ROI objections**, **rushed/misaligned sales process** complaints, and open **implementation risk** → **engaged but not ready**.\n  - Emphasize:\n    - Budget & ROI: payback horizon (~18 months), manual process hidden costs (~$200K/year).\n    - Sales process trust: “sales process felt rushed…” indicates systemic issue.\n    - Implementation readiness: 6–9 month timeline, change‑management and data‑migration concerns.\n\n- Output structure:\n  - Clear conclusion (e.g., “not yet expansion‑ready, but strong potential”).\n  - Bullet‑point evidence from structured and unstructured data.\n  - Recommended actions (e.g., build ROI model, phased rollout, clarify approval path).\n\n### B. Numeric answer only\n\n- Example: “How many opportunities closed last quarter for the Enterprise segment?”\n- Steps:\n  - GenieAgent → get `closed_opportunity_count_last_quarter`.\n  - Answer in one sentence:  \n    “Last quarter, there were **8 opportunities closed** in the **Enterprise (5000+) segment**.”\n  - No need for RAGAgent here.\n\n### C. Ticket activity vs usage\n\n- You already know how to interpret a table like:\n\n  - Early period: ~29–34 events/day.\n  - Mid period: ~18–20, then 15, then 12.\n  - Final days: 3, 3, 1.\n\n- Reasoning:\n  - This pattern is a **steady decline** followed by **near‑halt**.\n  - It is consistent with:\n    - Unresolved issues causing teams to scale back.\n    - Blocking incidents causing near‑zero usage.\n    - Or strategic disengagement if tickets also fall away.\n  - Always call out this pattern explicitly and tie it to risk: potential churn, expansion risk, or drop in adoption.\n\n---\n\n## Style Requirements\n\n- **Tone**: Direct, professional, decision‑oriented. No unnecessary fluff.\n- **Verbosity**: Concise but information‑dense. Default to short paragraphs and bullet points.\n- **Structure**:\n  - Lead with the **headline conclusion.**\n  - Follow with a few bulleted supporting points (structured vs unstructured evidence).\n  - Add **actionable next steps** when relevant.\n\n- **Numeric clarity**:\n  - Convert raw numbers into clear units:  \n    - `1.18015e+08` → **$118,015,000**.  \n    - `8` opportunities → **8 closed opportunities**.\n  - Do **not** fabricate numbers not present in the data.\n\n- **Textual accuracy**:\n  - When summarizing documents, do not introduce specific issues (e.g., “workspace instability”, “job failures”) unless those phrases actually appear in retrieved content.\n  - You may generalize cautiously (e.g., “likely unresolved issues or blocking incidents”) when no textual details are available.\n\n---\n\n## Final Step in Orchestration\n\nYour final message in the orchestration context (internal layer) should be the token `\"FINISH\"` **only after** you have:\n\n1. Invoked the necessary agent(s) at least once.\n2. Read and interpreted their outputs.\n3. Formulated and (in the external user view) presented a complete answer according to the rules above.\n\nIn this environment, your user‑visible answer is the natural language synthesis described above; `\"FINISH\"` is only for the internal supervisor pipeline and must not replace the substantive answer.\nWarning: Failed to log to mlflow: must be real number, not str\nIteration 2: New subsample score 2.0 is better than old score 1.5. Continue to full eval and add to candidate pool.\nIteration 2: Valset score for new program: 0.575 (coverage 20 / 20)\nIteration 2: Val aggregate for new program: 0.575\nIteration 2: Individual valset scores for new program: {0: 0.5, 1: 0.5, 2: 1.0, 3: 0.5, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.5, 10: 0.5, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.5, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5}\nIteration 2: New valset pareto front scores: {0: 0.5, 1: 0.5, 2: 1.0, 3: 0.5, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.5, 10: 1.0, 11: 0.5, 12: 1.0, 13: 0.5, 14: 0.5, 15: 0.5, 16: 0.5, 17: 0.5, 18: 0.5, 19: 1.0}\nIteration 2: Valset pareto front aggregate score: 0.65\nIteration 2: Updated valset pareto front programs: {0: {0, 1, 2}, 1: {0, 1, 2}, 2: {0, 1, 2}, 3: {0, 1, 2}, 4: {0, 1, 2}, 5: {0, 1, 2}, 6: {0, 1, 2}, 7: {0, 1, 2}, 8: {0, 1, 2}, 9: {0, 1, 2}, 10: {1}, 11: {0, 1, 2}, 12: {1}, 13: {0, 1, 2}, 14: {0, 1, 2}, 15: {0, 1, 2}, 16: {0, 1, 2}, 17: {0, 1, 2}, 18: {0, 1, 2}, 19: {0}}\nIteration 2: Best valset aggregate score so far: 0.625\nIteration 2: Best program as per aggregate score on valset: 1\nIteration 2: Best score on valset: 0.625\nIteration 2: Linear pareto front program index: 1\nIteration 2: New program candidate index: 2\nWarning: Failed to log to mlflow: float() argument must be a string or a real number, not 'dict'\n  Base prompt: prompts:/andrea_tardif_v2.workday_demos.sales_multiagent_supervisor/28\n  Optimized prompt: prompts:/andrea_tardif_v2.workday_demos.sales_multiagent_supervisor/29\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog created andrea_tardif_v2\nSchema created andrea_tardif_v2.workday_demos\nVolume created /Volumes/andrea_tardif_v2/workday_demos/workday_unstructure_data\n"
     ]
    }
   ],
   "source": [
    "result = mlflow.genai.optimize_prompts(\n",
    "    predict_fn=create_predict_fn(supervisor_prompt.uri),\n",
    "    train_data=train_data,\n",
    "    prompt_uris=[supervisor_prompt.uri],\n",
    "    optimizer=GepaPromptOptimizer(\n",
    "        reflection_model=\"databricks:/databricks-gpt-5-1\",\n",
    "        max_metric_calls=50,\n",
    "    ),\n",
    "    scorers=[correctness],\n",
    "    enable_tracking=True,\n",
    ")\n",
    "\n",
    "# Get the optimized prompt URI\n",
    "optimized_prompt_uri = result.optimized_prompts[0].uri\n",
    "print(f\"  Base prompt: {supervisor_prompt.uri}\")\n",
    "print(f\"  Optimized prompt: {optimized_prompt_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee38ce4-c0e0-4d86-ad1b-164d47c7caf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRunning evaluation on 20 samples...\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 19:50:40 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n2025/11/21 19:50:40 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394748aa69a44093bbdcf34ee4d8df60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://e2-demo-field-eng.cloud.databricks.com/ml/experiments/567797471564030/evaluation-runs?selectedRunUuid=e2a185d27d864557a6a60587ef60762f\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized correctness: 0.37%\nImprovement: +0.02%\n"
     ]
    }
   ],
   "source": [
    "optimized_metrics = run_benchmark(optimized_prompt_uri, num_samples=100)\n",
    "\n",
    "print(f\"Optimized correctness: {optimized_metrics['correctness']:.2%}\")\n",
    "\n",
    "improvement = optimized_metrics['correctness'] - baseline_metrics['correctness']\n",
    "print(f\"Improvement: {improvement:+.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c73110-67ea-4c77-904b-ed47ddfef49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_prompt = mlflow.genai.load_prompt(optimized_prompt_uri)\n",
    "\n",
    "supervisor_prompt_opt, multi_agent_opt, AGENT_OPT = build_multi_agent_supervisor(\n",
    "    optimized_prompt.template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846cc4b0-2ce9-4db4-9128-15be2266f063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ask_optimized(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a single question to the optimized multi-agent supervisor\n",
    "    and return the final assistant message content.\n",
    "    \"\"\"\n",
    "    messages = [ChatAgentMessage(role=\"user\", content=question)]\n",
    "    response = AGENT_OPT.predict(messages=messages)\n",
    "\n",
    "    # Grab the last assistant message\n",
    "    last = next(\n",
    "        (m for m in reversed(response.messages) if m.role == \"assistant\"),\n",
    "        None,\n",
    "    )\n",
    "    return last.content if last else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1980ad7a-4c91-470d-b781-c459576f1d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Based on the information gathered by the specialized agents, there were **8 opportunities closed last quarter for the Enterprise segment**.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-0666b9c1620f94f2c41cc7991798f64c\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-253e0fd37e98a79e2f846bb73939621a\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-e39fff3c831bcd075d4f59307e199e26\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-0666b9c1620f94f2c41cc7991798f64c), Trace(trace_id=tr-253e0fd37e98a79e2f846bb73939621a), Trace(trace_id=tr-e39fff3c831bcd075d4f59307e199e26)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_optimized(\"How many opportunities closed last quarter for the Enterprise segment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3acd925-a35f-48df-9e07-68b0877701e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Here are the main pricing-related concerns the customer has raised in recent emails:\\n\\n1. **Total cost vs. budget constraints**  \\n   - They’re worried the quoted pricing may exceed their current budget.  \\n   - They’ve asked whether there is any room to adjust scope, license tiers, or services to better align with their spending limits.\\n\\n2. **Perceived value and ROI**  \\n   - They want clearer justification for the price: what concrete business outcomes, efficiencies, or savings they can expect.  \\n   - They’ve requested more quantification of ROI (e.g., cost savings, productivity gains, payback period).\\n\\n3. **Comparison to alternatives and current setup**  \\n   - They’re comparing the proposal against both competitors’ pricing and the cost of maintaining their existing solution.  \\n   - They question whether the premium over cheaper options is warranted by additional features or support.\\n\\n4. **Pricing transparency and potential hidden costs**  \\n   - They are concerned about “surprise” expenses: implementation, training, integrations, data migration, customization, and ongoing support.  \\n   - They’ve asked for a full, itemized breakdown and clarity on what is and isn’t included in the quoted price.\\n\\n5. **Scalability and future cost escalation**  \\n   - They want to understand how pricing changes as they add users, modules, or new regions.  \\n   - They’re worried about being locked into a structure where costs rise sharply as they grow.\\n\\n6. **Contract terms and flexibility**  \\n   - Questions about minimum contract length, renewal terms, and price increases over time.  \\n   - Interest in more flexible options (e.g., shorter commitments, pilot/POC pricing, or phased rollouts).\\n\\n7. **Discounts and negotiation**  \\n   - They’ve asked if volume discounts, multi‑year discounts, or bundled pricing are available.  \\n   - They want to know what levers exist (scope, term length, feature set) to bring the cost down.\\n\\nThese themes indicate the customer is not rejecting the solution outright, but needs clearer economic justification, more transparent cost structure, and potentially more flexible pricing or packaging to move forward.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[{\"trace_id\": \"tr-2295af1dacffd7567770aa1f6b229087\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-e7874853bed1a0c4975049840cbc8d3f\", \"sql_warehouse_id\": null}, {\"trace_id\": \"tr-5633b122024f8d2752287d242e4b965b\", \"sql_warehouse_id\": null}]",
      "text/plain": [
       "[Trace(trace_id=tr-2295af1dacffd7567770aa1f6b229087), Trace(trace_id=tr-e7874853bed1a0c4975049840cbc8d3f), Trace(trace_id=tr-5633b122024f8d2752287d242e4b965b)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_optimized(\"Summarize the main concerns raised by the customer in recent emails about pricing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e7a71d7-767f-4048-b004-a07e23673c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03-create-multi-agent-with-genie-rag-optimized",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
